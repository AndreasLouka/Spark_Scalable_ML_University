{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>\n",
    "\n",
    "[Logistic regression](http://en.wikipedia.org/wiki/Logistic_regression) is widely used to predict a binary response. We will show how to implement Logistic Regression in Spark using Scala. First, the standard initialization with the relevant imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.evaluation.MulticlassMetrics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.linalg.{Vector, Vectors}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.param.ParamMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Logistic Regression\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\n",
    "\n",
    "// Naive Bayes\n",
    "import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\n",
    "\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/03/16 11:10:49 INFO SparkContext: Running Spark version 2.0.1\n",
      "17/03/16 11:10:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/03/16 11:10:51 INFO SecurityManager: Changing view acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/16 11:10:51 INFO SecurityManager: Changing modify acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/16 11:10:51 INFO SecurityManager: Changing view acls groups to: \n",
      "17/03/16 11:10:51 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/03/16 11:10:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(b97eec96efcb40779e247b002e047f82); groups with view permissions: Set(); users  with modify permissions: Set(b97eec96efcb40779e247b002e047f82); groups with modify permissions: Set()\n",
      "17/03/16 11:10:52 INFO Utils: Successfully started service 'sparkDriver' on port 38330.\n",
      "17/03/16 11:10:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/03/16 11:10:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/03/16 11:10:52 INFO DiskBlockManager: Created local directory at /projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week6/java_temp/blockmgr-8c287f6e-97fb-438d-9aeb-c50bb30a5f41\n",
      "17/03/16 11:10:52 INFO MemoryStore: MemoryStore started with capacity 3.8 GB\n",
      "17/03/16 11:10:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/03/16 11:10:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/03/16 11:10:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "17/03/16 11:10:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "17/03/16 11:10:53 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "17/03/16 11:10:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.0.36:4043\n",
      "17/03/16 11:10:53 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/03/16 11:10:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44428.\n",
      "17/03/16 11:10:53 INFO NettyBlockTransferService: Server created on 10.240.0.36:44428\n",
      "17/03/16 11:10:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.0.36, 44428)\n",
      "17/03/16 11:10:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.240.0.36:44428 with 3.8 GB RAM, BlockManagerId(driver, 10.240.0.36, 44428)\n",
      "17/03/16 11:10:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.0.36, 44428)\n",
      "17/03/16 11:10:54 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n",
      "17/03/16 11:10:54 INFO SharedState: Warehouse path is '/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week6/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@62036112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"Logistic Regression\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in Notebook 2, Logistic Regression (LR) instance is an estimator. A small scale example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n",
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_dbdaf58dcdb8-elasticNetParam: 0.0,\n",
      "\tlogreg_dbdaf58dcdb8-featuresCol: features,\n",
      "\tlogreg_dbdaf58dcdb8-fitIntercept: true,\n",
      "\tlogreg_dbdaf58dcdb8-labelCol: label,\n",
      "\tlogreg_dbdaf58dcdb8-maxIter: 10,\n",
      "\tlogreg_dbdaf58dcdb8-predictionCol: prediction,\n",
      "\tlogreg_dbdaf58dcdb8-probabilityCol: probability,\n",
      "\tlogreg_dbdaf58dcdb8-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_dbdaf58dcdb8-regParam: 0.01,\n",
      "\tlogreg_dbdaf58dcdb8-standardization: true,\n",
      "\tlogreg_dbdaf58dcdb8-threshold: 0.5,\n",
      "\tlogreg_dbdaf58dcdb8-tol: 1.0E-6\n",
      "}\n",
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_dbdaf58dcdb8-elasticNetParam: 0.0,\n",
      "\tlogreg_dbdaf58dcdb8-featuresCol: features,\n",
      "\tlogreg_dbdaf58dcdb8-fitIntercept: true,\n",
      "\tlogreg_dbdaf58dcdb8-labelCol: label,\n",
      "\tlogreg_dbdaf58dcdb8-maxIter: 30,\n",
      "\tlogreg_dbdaf58dcdb8-predictionCol: prediction,\n",
      "\tlogreg_dbdaf58dcdb8-probabilityCol: myProbability,\n",
      "\tlogreg_dbdaf58dcdb8-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_dbdaf58dcdb8-regParam: 0.1,\n",
      "\tlogreg_dbdaf58dcdb8-standardization: true,\n",
      "\tlogreg_dbdaf58dcdb8-threshold: 0.55,\n",
      "\tlogreg_dbdaf58dcdb8-tol: 1.0E-6\n",
      "}\n",
      "([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171033984,0.9429269582896601], prediction=1.0\n",
      "([3.0,2.0,-0.1], 0.0) -> prob=[0.9238522311704088,0.0761477688295912], prediction=0.0\n",
      "([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114779145,0.8902722388522085], prediction=1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtraining\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [label: double, features: vector]\n",
       "\u001b[36mlr\u001b[0m: \u001b[32mLogisticRegression\u001b[0m = logreg_dbdaf58dcdb8\n",
       "\u001b[36mres4_3\u001b[0m: \u001b[32mLogisticRegression\u001b[0m = logreg_dbdaf58dcdb8\n",
       "\u001b[36mmodel1\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mclassification\u001b[0m.\u001b[32mLogisticRegressionModel\u001b[0m = logreg_dbdaf58dcdb8\n",
       "\u001b[36mparamMap\u001b[0m: \u001b[32mParamMap\u001b[0m = {\n",
       "\tlogreg_dbdaf58dcdb8-maxIter: 30,\n",
       "\tlogreg_dbdaf58dcdb8-regParam: 0.1,\n",
       "\tlogreg_dbdaf58dcdb8-threshold: 0.55\n",
       "}\n",
       "\u001b[36mparamMap2\u001b[0m: \u001b[32mParamMap\u001b[0m = {\n",
       "\tlogreg_dbdaf58dcdb8-probabilityCol: myProbability\n",
       "}\n",
       "\u001b[36mparamMapCombined\u001b[0m: \u001b[32mParamMap\u001b[0m = {\n",
       "\tlogreg_dbdaf58dcdb8-maxIter: 30,\n",
       "\tlogreg_dbdaf58dcdb8-probabilityCol: myProbability,\n",
       "\tlogreg_dbdaf58dcdb8-regParam: 0.1,\n",
       "\tlogreg_dbdaf58dcdb8-threshold: 0.55\n",
       "}\n",
       "\u001b[36mmodel2\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mclassification\u001b[0m.\u001b[32mLogisticRegressionModel\u001b[0m = logreg_dbdaf58dcdb8\n",
       "\u001b[36mtest\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Prepare training data from a list of (label, features) tuples.\n",
    "val training = sparkSession.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Create a LogisticRegression instance. This instance is an Estimator.\n",
    "val lr = new LogisticRegression()\n",
    "// Print out the parameters, documentation, and any default values.\n",
    "println(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "// We can set parameters using setter methods. Possible parameters are listed below.\n",
    "lr.setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "\n",
    "// Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "val model1 = lr.fit(training)\n",
    "// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    "// we can view the parameters it used during fit().\n",
    "// This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "// LogisticRegression instance.\n",
    "println(\"Model 1 was fit using parameters: \" + model1.parent.extractParamMap)\n",
    "\n",
    "// We may alternatively specify parameters using a ParamMap,\n",
    "// which supports several methods for specifying parameters.\n",
    "val paramMap = ParamMap(lr.maxIter -> 20)\n",
    "  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.\n",
    "  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.\n",
    "\n",
    "// One can also combine ParamMaps.\n",
    "val paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\")  // Change output column name.\n",
    "val paramMapCombined = paramMap ++ paramMap2\n",
    "\n",
    "// Now learn a new model using the paramMapCombined parameters.\n",
    "// paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "val model2 = lr.fit(training, paramMapCombined)\n",
    "println(\"Model 2 was fit using parameters: \" + model2.parent.extractParamMap)\n",
    "\n",
    "// Prepare test data.\n",
    "val test = sparkSession.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")\n",
    "\n",
    "// Make predictions on test data using the Transformer.transform() method.\n",
    "// LogisticRegression.transform will only use the 'features' column.\n",
    "// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    "// 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "model2.transform(test)\n",
    "  .select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    "  .collect()\n",
    "  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n",
    "\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MLlib](https://spark.apache.org/docs/2.0.2/mllib-linear-methods.html#implementation-developer) implements a simple distributed version of [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). All provided algorithms take as input a regularization parameter (<tt>regParam</tt>) along with various parameters associated with stochastic gradient descent (<tt>stepSize, numIterations, miniBatchFraction</tt>). Three possible regularizations (<tt>L1</tt>, <tt>L2</tt> or their mixture) are supported. Further details are available [here](https://spark.apache.org/docs/2.0.2/mllib-optimization.html#gradient-descent-and-stochastic-gradient-descent) as well as in the output of the cell above. The [settable parameters](https://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent) include:\n",
    "\n",
    "- <tt>stepSize</tt> is a scalar value denoting the initial step size for gradient descent\n",
    "- <tt>numIterations</tt> is the number of iterations to run.\n",
    "- <tt>regParam</tt> is the regularization parameter when using L1 or L2 regularization.\n",
    "- <tt>miniBatchFraction</tt> is the fraction of the total data that is sampled in each iteration, to compute the gradient direction. \n",
    "\n",
    "A [second algorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS) to solve logistic regression, [L-BFGS](https://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS), an extension of the mini-batch gradient descent above is provided within MLlib. We apply this algorithm below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sc = sparkSession.sparkContext\n",
    "\n",
    "object LRwLBFGS {\n",
    "\n",
    "    def LBFGS() : Unit = {\n",
    "\n",
    "        // Load training data in LIBSVM format.\n",
    "        val data = MLUtils.loadLibSVMFile(sc, \"files/sample_libsvm_data.txt\")        \n",
    "\n",
    "        // Split data into training (60%) and test (40%).\n",
    "        val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "        val training = splits(0).cache()\n",
    "        val test = splits(1)\n",
    "\n",
    "        // Run training algorithm to build the model\n",
    "        val model = new LogisticRegressionWithLBFGS()\n",
    "          .setNumClasses(10)\n",
    "          .run(training)\n",
    "\n",
    "        // Compute raw scores on the test set.\n",
    "        val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "          val prediction = model.predict(features)\n",
    "          (prediction, label)\n",
    "        }\n",
    "\n",
    "        // Get evaluation metrics.\n",
    "        val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "        val accuracy = metrics.accuracy\n",
    "        println(s\"Accuracy = $accuracy\")\n",
    "    }\n",
    "}\n",
    "\n",
    "LRwLBFGS.LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exercises</h1>\n",
    "\n",
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Create standalone programs for both methods for solving linear regression to run on the HPC. Run both on the [default of credit card clients](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) data and [occupancy detection](http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#) data. Carry out timing experiments for both methods.\n",
    "\n",
    "<h2>Exercise 2</h2>\n",
    "\n",
    "Also using these datasets, explore the three available different regularization methods <tt>L1</tt> ([lasso](https://en.wikipedia.org/wiki/Lasso_(statistics))), <tt>L2</tt> ([ridge regression](https://en.wikipedia.org/wiki/Ridge_regression)) and <tt>L1/L2</tt> ([elastic net](http://en.wikipedia.org/wiki/Elastic_net_regularization)) by varying the <tt>regParam</tt>. You will need to carry out [cross validation](https://spark.apache.org/docs/2.0.2/ml-tuning.html#example-model-selection-via-cross-validation) using pipeline to find good regularization parameters.\n",
    "\n",
    "<h2>Exercise 3</h2>\n",
    "\n",
    "Use logistic regression to predict the probability of an ad click. There is a nice guide to a Python implementation of this at [https://turi.com/learn/gallery/notebooks/click_through_rate_prediction_intro.html](https://turi.com/learn/gallery/notebooks/click_through_rate_prediction_intro.html) which you can adapt."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
