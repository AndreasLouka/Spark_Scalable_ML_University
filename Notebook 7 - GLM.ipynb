{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generalized Linear Model (GLM)</h1>\n",
    "\n",
    "Unlike linear regression, where the output is assumed to follow a Gaussian distribution, \n",
    "in [generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model) (GLMs) the response variable $Y_i$ follows some distribution from the [exponential family of distributions](https://en.wikipedia.org/wiki/Exponential_family).\n",
    "\n",
    "We carry out the usual initialization and the relevant imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.regression.GeneralizedLinearRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we can view linear regression as a [generalized linear model](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.GeneralizedLinearRegression) using the Gaussian family. The following families (and supported links) are available: \n",
    "\n",
    "<table>\n",
    "<tr><td><b>Family</b></td><td><b>Response Type</b></td><td><b>Supported Links</b></td></tr>\n",
    "<tr><td>Gaussian</td><td>Continuous</td><td>Identity, Log, Inverse</td></tr>\n",
    "<tr><td>Binomial</td><td>Binary</td><td>Logit, Probit, CLogLog</td></tr>\n",
    "<tr><td>Poisson</td><td>Count</td><td>Log, Identity, Sqrt</td></tr>\n",
    "<tr><td>Gamma</td><td>Continuous</td><td>Inverse, Identity, Log</td></tr>\n",
    "</table>\n",
    "\n",
    "And a Gaussian example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "| -7.896274316726144|(10,[0,1,2,3,4,5,...|\n",
      "| -8.464803554195287|(10,[0,1,2,3,4,5,...|\n",
      "| 2.1214592666251364|(10,[0,1,2,3,4,5,...|\n",
      "| 1.0720117616524107|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -5.082010756207233|(10,[0,1,2,3,4,5,...|\n",
      "|  7.887786536531237|(10,[0,1,2,3,4,5,...|\n",
      "| 14.323146365332388|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-0.8995693247765151|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|  5.601801561245534|(10,[0,1,2,3,4,5,...|\n",
      "|-3.2256352187273354|(10,[0,1,2,3,4,5,...|\n",
      "| 1.5299675726687754|(10,[0,1,2,3,4,5,...|\n",
      "| -0.250102447941961|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 38, localhost): java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got -28.571478869743427",
      "\tat scala.Predef$.require(Predef.scala:224)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:543)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:333)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:332)",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "\tat java.lang.Thread.run(Thread.java:745)",
      "",
      "Driver stacktrace: (Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 38, localhost): java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got -28.571478869743427",
      "\tat scala.Predef$.require(Predef.scala:224)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:543)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:333)",
      "\tat org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:332)",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "\tat java.lang.Thread.run(Thread.java:745)",
      "",
      "Driver stacktrace:)",
      "  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)",
      "  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)",
      "  scala.Option.foreach(Option.scala:257)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)",
      "  org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:358)",
      "  org.apache.spark.rdd.RDD.reduce(RDD.scala:984)",
      "  org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:358)",
      "  org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)",
      "  org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:81)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink.initialize(GeneralizedLinearRegression.scala:340)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:275)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:139)",
      "  org.apache.spark.ml.Predictor.fit(Predictor.scala:90)",
      "  cmd9$$user$$anonfun$10.apply(Main.scala:59)",
      "  cmd9$$user$$anonfun$10.apply(Main.scala:58)",
      "java.lang.IllegalArgumentException: requirement failed: The response variable of Gamma family should be positive, but got -28.571478869743427 (requirement failed: The response variable of Gamma family should be positive, but got -28.571478869743427)",
      "  scala.Predef$.require(Predef.scala:224)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression$Gamma$.initialize(GeneralizedLinearRegression.scala:543)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:333)",
      "  org.apache.spark.ml.regression.GeneralizedLinearRegression$FamilyAndLink$$anonfun$4.apply(GeneralizedLinearRegression.scala:332)",
      "  scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "  scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "  scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "  scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)",
      "  scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)",
      "  scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)",
      "  scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)",
      "  org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "  org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1112)",
      "  org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "  org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1113)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)",
      "  org.apache.spark.rdd.RDD.iterator(RDD.scala:283)",
      "  org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "  org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "  java.lang.Thread.run(Thread.java:745)"
     ]
    }
   ],
   "source": [
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"GLM\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Load training data\n",
    "val dataset = sparkSession.read.format(\"libsvm\")\n",
    "  .load(\"files/sample_linear_regression_data.txt\")\n",
    "    \n",
    "dataset.show()\n",
    "val glr = new GeneralizedLinearRegression()\n",
    "  .setFamily(\"gamma\")\n",
    "  .setLink(\"log\")\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = dataset.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Fit the model to the training data\n",
    "val model = glr.fit(trainingData)\n",
    "\n",
    "// Print the coefficients and intercept for generalized linear regression model\n",
    "println(s\"Coefficients: ${model.coefficients}\")\n",
    "println(s\"Intercept: ${model.intercept}\")\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Implement GLM as a standalone program that you can run on the HPC. Make sure that you can pass the family and the link function in as inputs. Using the family Gaussian (i.e. carrying out linear regression), explore different link functions for the datasets from Notebook 6. Are there any link functions that make more sense than others for the datasets?\n",
    "\n",
    "<h2>Exercise 2</h2>\n",
    "\n",
    "Explore different link functions for Gamma regression for the same datasets.\n",
    "\n",
    "<h2>Exercise 3</h2>\n",
    "\n",
    "Compare optimization algorithms: LinearRegression uses L-BFGS or OWL-QN, depending of the type of regularization, and GeneralizedLinearRegression uses IRLS (iterative reweighted least squares). \n",
    "\n",
    "<h2>Exercise 4</h2>\n",
    "\n",
    "Compare linear regression and Gamma regression for the following two datasets: \n",
    "\n",
    "1. [Airline dataset](http://stat-computing.org/dataexpo/2009/the-data.html) Design your program to accept multiple years' worth of input.\n",
    "\n",
    "2. [Allstate claim prediction challenge](https://www.kaggle.com/c/ClaimPredictionChallenge/data)\n",
    "\n",
    "Take a look at the datasets before you apply Gamma regression and consider whether it makes sense to apply (and alternative options)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
