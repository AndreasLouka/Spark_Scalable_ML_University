{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Background</h1>\n",
    "\n",
    "<tt>Spark</tt> allows for parallel operations in a program to be executed on a cluster. The main abstractions are:\n",
    "\n",
    "- A **dataset** which is a collection of elements partitioned across the nodes of the cluster which can be worked on in parallel\n",
    "\n",
    "- **Shared variables** which can be shared across tasks or between tasks and the driver program.\n",
    "\n",
    "This notebook introduces Spark, Scala and running <tt>.scala</tt> programs on a HPC.\n",
    "\n",
    "<h1>Setting up Spark</h1>\n",
    "\n",
    "Spark supports multiple programming languages including **Scala**, **Java**, **R** and **Python**. Throughout these tutorials, we will use **Scala**.\n",
    "\n",
    "To run a piece of code in a Jupyter notebook, you can use the \"Cell\" menu or use the CTRL+Return combination when you've selected the relevant cell. SageMathCloud can only have one notebook running at a time, so to open another, you'll need to close the current one (using the menu \"File\" $\\rightarrow$ \"Close and halt\" option).\n",
    "\n",
    "To start our <tt>Spark</tt> application, we first set (and check) the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to the classpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147 new artifacts in macro\n",
      "147 new artifacts in runtime\n",
      "147 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import some Spark classes (more detail on these below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initializing Spark</h2>\n",
    "\n",
    "Before Spark 2.0, it was necessary to create a SparkContext object and tell Spark how to access a cluster, using:\n",
    "\n",
    "    import org.apache.spark.SparkContext\n",
    "    import org.apache.spark.SparkConf\n",
    "    val conf = new SparkConf().setAppName(appName).setMaster(master)\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "In Spark 2.0, the two were subsumed by SparkSession\n",
    "\n",
    "    import org.apache.spark.sql.SparkSession\n",
    "    val sparkSession = SparkSession.builder\n",
    "      .master(\"local\")\n",
    "      .appName(\"my-spark-app\")\n",
    "      .config(\"spark.some.config.option\", \"config-value\")\n",
    "      .getOrCreate()\n",
    "      \n",
    "(Note we have already carried out the necessary <tt>import</tt> in the previous cell.) The underlying SparkContext, which is created when SparkSession is called (if it doesn't already exist), can still be accessed using the <tt>sparkContext</tt> method of a <tt>sparkSession</tt>. Now we create an instance of a SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/02/28 20:02:20 INFO SparkContext: Running Spark version 2.0.1\n",
      "17/02/28 20:02:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/02/28 20:02:22 INFO SecurityManager: Changing view acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/02/28 20:02:22 INFO SecurityManager: Changing modify acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/02/28 20:02:22 INFO SecurityManager: Changing view acls groups to: \n",
      "17/02/28 20:02:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/02/28 20:02:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(b97eec96efcb40779e247b002e047f82); groups with view permissions: Set(); users  with modify permissions: Set(b97eec96efcb40779e247b002e047f82); groups with modify permissions: Set()\n",
      "17/02/28 20:02:23 INFO Utils: Successfully started service 'sparkDriver' on port 42200.\n",
      "17/02/28 20:02:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/02/28 20:02:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/02/28 20:02:23 INFO DiskBlockManager: Created local directory at /projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week1/java_temp/blockmgr-39366218-e046-4637-b21f-92c407f9d2f4\n",
      "17/02/28 20:02:23 INFO MemoryStore: MemoryStore started with capacity 3.8 GB\n",
      "17/02/28 20:02:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/02/28 20:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/02/28 20:02:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "17/02/28 20:02:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.0.3:4041\n",
      "17/02/28 20:02:24 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/02/28 20:02:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44341.\n",
      "17/02/28 20:02:25 INFO NettyBlockTransferService: Server created on 10.240.0.3:44341\n",
      "17/02/28 20:02:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.0.3, 44341)\n",
      "17/02/28 20:02:25 INFO BlockManagerMasterEndpoint: Registering block manager 10.240.0.3:44341 with 3.8 GB RAM, BlockManagerId(driver, 10.240.0.3, 44341)\n",
      "17/02/28 20:02:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.0.3, 44341)\n",
      "17/02/28 20:02:25 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n",
      "17/02/28 20:02:25 INFO SharedState: Warehouse path is '/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week1/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@2cfd796a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkSession = SparkSession.builder\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"Spark examples\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating standalone programs</h2>\n",
    "\n",
    "Since the amount of memory available to a Jupyter notebook is limited (which is why we have to close one to be able to open another), any larger scale programs have to be run on a 'proper' machine and so need us to create standalone programs. We will continue to demonstrate small scale examples in the notebooks, and you can try out small bits of code in a Jupyter cell, but you should try to create the corresponding <tt>.scala</tt> programs on <tt>iceberg</tt> and elsewhere. When you log into a node on <tt>iceberg</tt>, you may need to ask for more memory than the default <tt>qrsh</tt> gives you:\n",
    "\n",
    "    ssh user@iceberg.shef.ac.uk\n",
    "    qrsh -l mem=8G -l rmem=8G\n",
    "\n",
    "<h3>Passing functions to Spark</h3>\n",
    "\n",
    "We need to pass functions in the driver program to the cluster. One of the ways to do this is using static methods in a global singleton object. **Note** that applications should define a <tt>main()</tt> method. The following, along with the relevant imports, form self contained programs - starting with the usual <tt>Hello World</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mHelloWorld\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object HelloWorld {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "      \n",
    "        val sparkSession = SparkSession.builder\n",
    "          .master(\"local\")\n",
    "          .appName(\"Hello World\")\n",
    "          .getOrCreate()\n",
    "        \n",
    "        println(\"Hello, world!\")\n",
    "    }\n",
    "}\n",
    "\n",
    "HelloWorld.main(Array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a standalone program, this is available <a href=\"files/HelloWorld.scala\">here</a>. You will need to package this using <tt>sbt</tt> (the Scala build tool), using <tt>sbt package</tt>, and then use <tt>spark-submit</tt> to run the resulting package. (More details on this in the <a href=\"files/README\">README</a>.)\n",
    "\n",
    "<h2>Modifying data</h2>\n",
    "\n",
    "There are lots of things we can do with data! For example, we can remove any instances that don't match our requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@76e5eb51\n",
       "\u001b[36minput\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = MapPartitionsRDD[1] at map at Main.scala:28\n",
       "\u001b[36mresult\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = MapPartitionsRDD[2] at filter at Main.scala:31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Extract the spark context\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "\n",
    "val input = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000)).map(_.toDouble)\n",
    "val result = input.filter(x => x <= 10)\n",
    "println(result.collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing something!</h2>\n",
    "\n",
    "Our next example program reads in data, starts a <tt>sparkSession</tt> and finds out the number of occurrences of individual letters within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 6, Lines with b: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mLetterCountingApp\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object LetterCountingApp {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        val inputFile = \"files/TaleOfTwoCities.txt\" // Should be some file on your system\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Letter counting app\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        val inputData = sc.textFile(inputFile, 2).cache()\n",
    "        val numAs = inputData.filter(line => line.contains(\"a\")).count()\n",
    "        val numBs = inputData.filter(line => line.contains(\"b\")).count()\n",
    "        println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n",
    "    }\n",
    "}\n",
    "\n",
    "LetterCountingApp.main(Array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Statistics</h2>\n",
    "\n",
    "[Basic statistics](https://spark.apache.org/docs/2.0.2/mllib-statistics.html) functions are implemented within Spark, which can be useful for computing the mean of data or other stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0,20.0,200.0]\n",
      "[1.0,100.0,10000.0]\n",
      "[3.0,3.0,3.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\u001b[0m\n",
       "\u001b[36mobservations\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mmllib\u001b[0m.\u001b[32mlinalg\u001b[0m.\u001b[32mVector\u001b[0m] = ParallelCollectionRDD[7] at parallelize at Main.scala:25\n",
       "\u001b[36msummary\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mmllib\u001b[0m.\u001b[32mstat\u001b[0m.\u001b[32mMultivariateStatisticalSummary\u001b[0m = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@3acc93cf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import for vectors\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Import for statistics\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "\n",
    "val observations = sc.parallelize(\n",
    "  Seq(\n",
    "    Vectors.dense(1.0, 10.0, 100.0),\n",
    "    Vectors.dense(2.0, 20.0, 200.0),\n",
    "    Vectors.dense(3.0, 30.0, 300.0)\n",
    "  )\n",
    ")\n",
    "\n",
    "// Compute column summary statistics.\n",
    "val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n",
    "println(summary.mean)  // a dense vector containing the mean value for each column\n",
    "println(summary.variance)  // column-wise variance\n",
    "println(summary.numNonzeros)  // number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "There are many more examples in the Scala / Spark documentation than are shown in these notebooks - you will find it beneficial to consult these.\n",
    "\n",
    "<h3>Exercise 1</h3>\n",
    "\n",
    "Use the <tt>List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000)</tt> data introduced above along with the available <tt>statistics</tt> package to filter out any values that are more than 3 standard deviations away from the mean. You should be able to carry out this exercise in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The values that are more than 3 std away from the mean are: ,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0)\n",
      "(The values that are more than 3 std away from the mean are: ,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.{Statistics}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mThree_std\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Hint: standard deviation = sqrt ( variance )\n",
    "\n",
    "\n",
    "// Import for statistics\n",
    "import org.apache.spark.mllib.stat.{Statistics}\n",
    "\n",
    "object Three_std {\n",
    "    \n",
    "    def main (args: Array[String]){\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Letter counting app\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        val the_list = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1000)).map(_.toDouble)\n",
    "\n",
    "            \n",
    "        // Compute column summary statistics.\n",
    "        val mean = the_list.mean\n",
    "        val variance = the_list.variance\n",
    "        val std = math.sqrt(variance)\n",
    "        \n",
    "        val max = mean + 3*std \n",
    "        val min = mean - 3*std\n",
    "        \n",
    "        val result = the_list.filter(x => x<= max) //works because all values are positive\n",
    "        println(\"The values that are more than 3 std away from the mean are: \", result.collect().mkString(\",\"))\n",
    "        \n",
    "        //better way to do it\n",
    "        val result_2 = the_list.filter(x => math.abs(mean - x) <= 3*std)\n",
    "        println(\"The values that are more than 3 std away from the mean are: \", result_2.collect().mkString(\",\"))\n",
    "        \n",
    "   }\n",
    "}\n",
    "\n",
    "Three_std.main(Array())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 2</h3>\n",
    "\n",
    "Now we'll use real data: the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris). The webpage provides a [dataset description](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names) as well as the data for download. The [iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) file is available for the notebook to use at \n",
    "\n",
    "    files/iris.data\n",
    "\n",
    "and the column data at\n",
    "    \n",
    "    files/sepal_length.data\n",
    "    files/sepal_width.data\n",
    "    files/petal_length.data\n",
    "    files/petal_width.data\n",
    "    \n",
    "The column arrays are also provided for you in the cell below. Can you compute correlations between the various pairs of features? (You may find that a correlation function that you can use already exists.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of sepal is: -0.10936924995062468\n",
      "Correlation of petal is: 0.9627570970509658\n",
      "Correlation of lengths is: 0.8717541573048866\n",
      "Correlation of widths is: -0.35654408961379946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msepalLength\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = ParallelCollectionRDD[15] at parallelize at Main.scala:26\n",
       "\u001b[36msepalWidth\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = ParallelCollectionRDD[16] at parallelize at Main.scala:29\n",
       "\u001b[36mpetalLength\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = ParallelCollectionRDD[17] at parallelize at Main.scala:32\n",
       "\u001b[36mpetalWidth\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mDouble\u001b[0m] = ParallelCollectionRDD[18] at parallelize at Main.scala:35\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.Statistics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[36mcorrelation_1\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m-0.10936924995062468\u001b[0m\n",
       "\u001b[36mcorrelation_2\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.9627570970509658\u001b[0m\n",
       "\u001b[36mcorrelation_3\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.8717541573048866\u001b[0m\n",
       "\u001b[36mcorrelation_4\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m-0.35654408961379946\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sepalLength = sc.parallelize(Array(5.1,4.9,4.7,4.6,5.0,5.4,4.6,5.0,4.4,4.9,5.4,4.8,4.8,4.3,5.8,5.7,5.4,5.1,5.7,5.1,5.4,5.1,4.6,5.1,4.8,5.0,5.0,5.2,5.2,4.7,4.8,5.4,5.2,5.5,4.9,5.0,5.5,4.9,4.4,5.1,5.0,4.5,4.4,5.0,5.1,4.8,5.1,4.6,5.3,5.0,7.0,6.4,6.9,5.5,6.5,5.7,6.3,4.9,6.6,5.2,5.0,5.9,6.0,6.1,5.6,6.7,5.6,5.8,6.2,5.6,5.9,6.1,6.3,6.1,6.4,6.6,6.8,6.7,6.0,5.7,5.5,5.5,5.8,6.0,5.4,6.0,6.7,6.3,5.6,5.5,5.5,6.1,5.8,5.0,5.6,5.7,5.7,6.2,5.1,5.7,6.3,5.8,7.1,6.3,6.5,7.6,4.9,7.3,6.7,7.2,6.5,6.4,6.8,5.7,5.8,6.4,6.5,7.7,7.7,6.0,6.9,5.6,7.7,6.3,6.7,7.2,6.2,6.1,6.4,7.2,7.4,7.9,6.4,6.3,6.1,7.7,6.3,6.4,6.0,6.9,6.7,6.9,5.8,6.8,6.7,6.7,6.3,6.5,6.2,5.9))\n",
    "val sepalWidth = sc.parallelize(Array(3.5,3.0,3.2,3.1,3.6,3.9,3.4,3.4,2.9,3.1,3.7,3.4,3.0,3.0,4.0,4.4,3.9,3.5,3.8,3.8,3.4,3.7,3.6,3.3,3.4,3.0,3.4,3.5,3.4,3.2,3.1,3.4,4.1,4.2,3.1,3.2,3.5,3.1,3.0,3.4,3.5,2.3,3.2,3.5,3.8,3.0,3.8,3.2,3.7,3.3,3.2,3.2,3.1,2.3,2.8,2.8,3.3,2.4,2.9,2.7,2.0,3.0,2.2,2.9,2.9,3.1,3.0,2.7,2.2,2.5,3.2,2.8,2.5,2.8,2.9,3.0,2.8,3.0,2.9,2.6,2.4,2.4,2.7,2.7,3.0,3.4,3.1,2.3,3.0,2.5,2.6,3.0,2.6,2.3,2.7,3.0,2.9,2.9,2.5,2.8,3.3,2.7,3.0,2.9,3.0,3.0,2.5,2.9,2.5,3.6,3.2,2.7,3.0,2.5,2.8,3.2,3.0,3.8,2.6,2.2,3.2,2.8,2.8,2.7,3.3,3.2,2.8,3.0,2.8,3.0,2.8,3.8,2.8,2.8,2.6,3.0,3.4,3.1,3.0,3.1,3.1,3.1,2.7,3.2,3.3,3.0,2.5,3.0,3.4,3.0))\n",
    "val petalLength = sc.parallelize(Array(1.4,1.4,1.3,1.5,1.4,1.7,1.4,1.5,1.4,1.5,1.5,1.6,1.4,1.1,1.2,1.5,1.3,1.4,1.7,1.5,1.7,1.5,1.0,1.7,1.9,1.6,1.6,1.5,1.4,1.6,1.6,1.5,1.5,1.4,1.5,1.2,1.3,1.5,1.3,1.5,1.3,1.3,1.3,1.6,1.9,1.4,1.6,1.4,1.5,1.4,4.7,4.5,4.9,4.0,4.6,4.5,4.7,3.3,4.6,3.9,3.5,4.2,4.0,4.7,3.6,4.4,4.5,4.1,4.5,3.9,4.8,4.0,4.9,4.7,4.3,4.4,4.8,5.0,4.5,3.5,3.8,3.7,3.9,5.1,4.5,4.5,4.7,4.4,4.1,4.0,4.4,4.6,4.0,3.3,4.2,4.2,4.2,4.3,3.0,4.1,6.0,5.1,5.9,5.6,5.8,6.6,4.5,6.3,5.8,6.1,5.1,5.3,5.5,5.0,5.1,5.3,5.5,6.7,6.9,5.0,5.7,4.9,6.7,4.9,5.7,6.0,4.8,4.9,5.6,5.8,6.1,6.4,5.6,5.1,5.6,6.1,5.6,5.5,4.8,5.4,5.6,5.1,5.1,5.9,5.7,5.2,5.0,5.2,5.4,5.1))\n",
    "val petalWidth = sc.parallelize(Array(0.2,0.2,0.2,0.2,0.2,0.4,0.3,0.2,0.2,0.1,0.2,0.2,0.1,0.1,0.2,0.4,0.4,0.3,0.3,0.3,0.2,0.4,0.2,0.5,0.2,0.2,0.4,0.2,0.2,0.2,0.2,0.4,0.1,0.2,0.1,0.2,0.2,0.1,0.2,0.2,0.3,0.3,0.2,0.6,0.4,0.3,0.2,0.2,0.2,0.2,1.4,1.5,1.5,1.3,1.5,1.3,1.6,1.0,1.3,1.4,1.0,1.5,1.0,1.4,1.3,1.4,1.5,1.0,1.5,1.1,1.8,1.3,1.5,1.2,1.3,1.4,1.4,1.7,1.5,1.0,1.1,1.0,1.2,1.6,1.5,1.6,1.5,1.3,1.3,1.3,1.2,1.4,1.2,1.0,1.3,1.2,1.3,1.3,1.1,1.3,2.5,1.9,2.1,1.8,2.2,2.1,1.7,1.8,1.8,2.5,2.0,1.9,2.1,2.0,2.4,2.3,1.8,2.2,2.3,1.5,2.3,2.0,2.0,1.8,2.1,1.8,1.8,1.8,2.1,1.6,1.9,2.0,2.2,1.5,1.4,2.3,2.4,1.8,1.8,2.1,2.4,2.3,1.9,2.3,2.5,2.3,1.9,2.0,2.3,1.8))\n",
    "// must have the same number of partitions and cardinality\n",
    "\n",
    "\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "\n",
    "\n",
    "// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a\n",
    "// method is not specified, Pearson's method will be used by default.\n",
    "val correlation_1: Double = Statistics.corr(sepalLength, sepalWidth, \"pearson\")\n",
    "println(s\"Correlation of sepal is: $correlation_1\")\n",
    "\n",
    "val correlation_2: Double = Statistics.corr(petalLength, petalWidth, \"pearson\")\n",
    "println(s\"Correlation of petal is: $correlation_2\")\n",
    "\n",
    "val correlation_3: Double = Statistics.corr(sepalLength, petalLength, \"pearson\")\n",
    "println(s\"Correlation of lengths is: $correlation_3\")\n",
    "\n",
    "val correlation_4: Double = Statistics.corr(sepalWidth, petalWidth, \"pearson\")\n",
    "println(s\"Correlation of widths is: $correlation_4\")\n",
    "\n",
    "// val data: RDD[Vector] = sc.parallelize(\n",
    "//   Seq(\n",
    "//     Vectors.dense(1.0, 10.0, 100.0),\n",
    "//     Vectors.dense(2.0, 20.0, 200.0),\n",
    "//     Vectors.dense(5.0, 33.0, 366.0))\n",
    "// )  // note that each Vector is a row and not a column\n",
    "\n",
    "// // calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n",
    "// // If a method is not specified, Pearson's method will be used by default.\n",
    "// val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n",
    "// println(correlMatrix.toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 3</h3>\n",
    "\n",
    "Use the downloadable <tt>HelloWorld.scala</tt> program to create a standalone version on the HPC. Run this and check the output generates what you expect. (Note that this does not need to be submitted as a job, but can be run interactively.)\n",
    "\n",
    "<h3>Exercise 4</h3>\n",
    "\n",
    "Make the <tt>LetterCountingApp</tt> a standalone program executable on the HPC. Don't forget to correctly set the <tt>inputFile</tt> path. Can you make the program take the file path as an argument? (Again, this can be run interactively.)\n",
    "\n",
    "<h3>Exercise 5 (optional)</h3>\n",
    "\n",
    "The data description of the Iris dataset in Exercise 2 contains some statistics for this data. Can you reproduce these in the cell below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
