{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## COM6012 - 2017:  Practical Quiz 2 \n",
    "\n",
    "Use logistic regression to perform classification in the [spam](./files/spambase.data) dataset. To test your algorithm split the samples in two datasets, one for training (with 70% of the samples), and one for testing (with 30% of the samples). Use a regularisation parameter of 0.01 and an elastic net parameter of 0.1.\n",
    "\n",
    "The marks will be assigned like this:\n",
    "1. Loading the data: 1 Mark\n",
    "2. Performing the training stage: 2 Marks\n",
    "3. Performing the testing stage: 2 Marks\n",
    "\n",
    "Provide your solution in the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.param.ParamMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// The usual imports\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/03/23 12:18:16 INFO SparkContext: Running Spark version 2.0.1\n",
      "17/03/23 12:18:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/03/23 12:18:17 INFO SecurityManager: Changing view acls to: 5e62ac24ea96411ca3b6bebb2b072296\n",
      "17/03/23 12:18:17 INFO SecurityManager: Changing modify acls to: 5e62ac24ea96411ca3b6bebb2b072296\n",
      "17/03/23 12:18:17 INFO SecurityManager: Changing view acls groups to: \n",
      "17/03/23 12:18:17 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/03/23 12:18:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(5e62ac24ea96411ca3b6bebb2b072296); groups with view permissions: Set(); users  with modify permissions: Set(5e62ac24ea96411ca3b6bebb2b072296); groups with modify permissions: Set()\n",
      "17/03/23 12:18:19 INFO Utils: Successfully started service 'sparkDriver' on port 43913.\n",
      "17/03/23 12:18:19 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/03/23 12:18:19 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/03/23 12:18:19 INFO DiskBlockManager: Created local directory at /projects/5e62ac24-ea96-411c-a3b6-bebb2b072296/PracticalQuizzes/PracticalQuiz2Solution/java_temp/blockmgr-7b4e83a8-ff8b-4ea9-953b-3ed9a3ea8e26\n",
      "17/03/23 12:18:19 INFO MemoryStore: MemoryStore started with capacity 3.8 GB\n",
      "17/03/23 12:18:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/03/23 12:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "17/03/23 12:18:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "17/03/23 12:18:21 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "17/03/23 12:18:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.0.18:4042\n",
      "17/03/23 12:18:21 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/03/23 12:18:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42216.\n",
      "17/03/23 12:18:21 INFO NettyBlockTransferService: Server created on 10.240.0.18:42216\n",
      "17/03/23 12:18:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.0.18, 42216)\n",
      "17/03/23 12:18:21 INFO BlockManagerMasterEndpoint: Registering block manager 10.240.0.18:42216 with 3.8 GB RAM, BlockManagerId(driver, 10.240.0.18, 42216)\n",
      "17/03/23 12:18:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.0.18, 42216)\n",
      "17/03/23 12:18:22 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n",
      "17/03/23 12:18:22 INFO SharedState: Warehouse path is '/projects/5e62ac24-ea96-411c-a3b6-bebb2b072296/PracticalQuizzes/PracticalQuiz2Solution/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@1673c6bf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// \"Open the bridge\"\n",
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"Logistic Regression\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtext\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = files/spambase.data MapPartitionsRDD[1] at textFile at Main.scala:25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Load the data\n",
    "val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mArray\u001b[0m[\u001b[32mDouble\u001b[0m]] = MapPartitionsRDD[2] at map at Main.scala:25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Separate into array\n",
    "val data = text.map(line => line.split(',').map(_.toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataLP\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mDouble\u001b[0m, \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mlinalg\u001b[0m.\u001b[32mVector\u001b[0m)] = MapPartitionsRDD[3] at map at Main.scala:26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Organise data into feastures and labels\n",
    "val dataLP = data.map(t => (t(57), Vectors.dense(t.take(57))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataDF\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Convert to a dataframe\n",
    "val dataDF = sparkSession.createDataFrame(dataLP).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabelIndexer\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mfeature\u001b[0m.\u001b[32mStringIndexerModel\u001b[0m = strIdx_1f1edaa15a0a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Index the labels\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfeatureIndexer\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mfeature\u001b[0m.\u001b[32mVectorIndexerModel\u001b[0m = vecIdx_c07e8bf653ef"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Index the feature vector\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4)\n",
    "  .fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msplits\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mRow\u001b[0m]] = \u001b[33mArray\u001b[0m([label: double, features: vector], [label: double, features: vector])\n",
       "\u001b[36mtrainingData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mRow\u001b[0m] = [label: double, features: vector]\n",
       "\u001b[36mtestData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mRow\u001b[0m] = [label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Split into training and testing data\n",
    "val splits = dataDF.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlr\u001b[0m: \u001b[32mLogisticRegression\u001b[0m = logreg_bdec8c8de0ee"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Create a LogisticRegression instance. \n",
    "val lr = new LogisticRegression()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[0m: \u001b[32mLogisticRegression\u001b[0m = logreg_bdec8c8de0ee"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// We can set requested parameters for the regulariser\n",
    "lr.setMaxIter(10).setRegParam(0.01).setElasticNetParam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabelConverter\u001b[0m: \u001b[32mIndexToString\u001b[0m = idxToStr_991ff2c3e856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Convert predictions to labels\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpipeline\u001b[0m: \u001b[32mPipeline\u001b[0m = pipeline_11e78e5674ac"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Set the Pipeline and the Stages\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, featureIndexer, lr, labelConverter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodel\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mml\u001b[0m.\u001b[32mPipelineModel\u001b[0m = pipeline_11e78e5674ac"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Fir the model\n",
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictions\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [label: double, features: vector ... 6 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Predictions over test data\n",
    "val predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mevaluator\u001b[0m: \u001b[32mMulticlassClassificationEvaluator\u001b[0m = mcEval_edfe2f6f1038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Create an evaluator using the indexed label and the prediction\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.08605341246290799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maccuracy\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.913946587537092\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
