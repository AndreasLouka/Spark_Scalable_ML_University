{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Collaborative filtering</h1>\n",
    "\n",
    "[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering) is often used for recommender systems.\n",
    "\n",
    "<tt>spark.mllib</tt> uses the alternating least squares (ALS) algorithm to fill in the missing entries of a user-item association matrix. The following parameters are available:\n",
    "\n",
    "- *numBlocks* is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "- *rank* is the number of latent factors in the model.\n",
    "- *iterations* is the number of iterations of ALS to run.\n",
    "- *lambda* specifies the regularization parameter in ALS.\n",
    "- *implicitPrefs* specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\n",
    "- *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\n",
    "\n",
    "We start up in the usual way and carry out the relevant imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we present a small example of [collaborative filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering) with the data taken from the [MovieLens](http://grouplens.org/datasets/movielens/) project. In this notebook, we use the old 100k dataset (available to Jupyter at <tt>files/ml.data</tt>).\n",
    "\n",
    "The dataset looks like this:\n",
    "\n",
    "    196     242     3       881250949\n",
    "    186     302     3       891717742\n",
    "    22      377     1       878887116\n",
    "    244     51      2       880606923\n",
    "    ...\n",
    "\n",
    "This is a tab separated list of \n",
    "    \n",
    "    user id | item id | rating | timestamp \n",
    "\n",
    "We use the default [ALS](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS).train() method which assumes ratings are explicit. The Mean Squared Error of rating prediction is used to evaluate the recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkSession = SparkSession.builder\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"Collaborative filtering\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "\n",
    "// Load and parse the data\n",
    "val data = sc.textFile(\"files/ml.data\")\n",
    "val ratings = data.map(_.split(\"\\t\") match { case Array(user, item, rate, timestamp) =>\n",
    "    Rating(user.toInt, item.toInt, rate.toDouble)\n",
    "})\n",
    "\n",
    "// Build the recommendation model using ALS\n",
    "val rank = 10\n",
    "val numIterations = 10\n",
    "val model = ALS.train(ratings, rank, numIterations, 0.01)\n",
    "\n",
    "// Evaluate the model on rating data\n",
    "val usersProducts = ratings.map { case Rating(user, product, rate) =>\n",
    "    (user, product)\n",
    "}\n",
    "\n",
    "val predictions =\n",
    "    model.predict(usersProducts).map { case Rating(user, product, rate) =>\n",
    "        ((user, product), rate)\n",
    "}\n",
    "\n",
    "val ratesAndPreds = ratings.map { case Rating(user, product, rate) =>\n",
    "    ((user, product), rate)\n",
    "}.join(predictions)\n",
    "\n",
    "val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =>\n",
    "    val err = (r1 - r2)\n",
    "    err * err\n",
    "}.mean()\n",
    "\n",
    "println(\"Mean Squared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exercises</h1>\n",
    "\n",
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Create a standalone program that carries out collaborative filtering. Run this on the 10Mb [MovieLens](http://grouplens.org/datasets/movielens/) data (available from the link provided).\n",
    "\n",
    "<h2>Exercise 2</h2>\n",
    "\n",
    "Use 10-fold cross validation (with an 80% training and 20% testing split) to find an average mean average (or squared) error on your test data. Keep your program as parallel as possible. You can create your splits randomly (or any other way you choose!), and don't forget who has access to various variables and who doesn't..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
