{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COM6012 - 2017: Practical Quiz 2\n",
    "\n",
    "Use logistic regression to perform classification in the [spam](./files/spambase.data) dataset. To test your algorithm split the samples in two datasets, one for training (with 70% of the samples), and one for testing (with 30% of the samples). Use a regularisation parameter of 0.01 and an elastic net parameter of 0.1.\n",
    "\n",
    "The marks will be assigned like this:\n",
    "1. Loading the data: 1 Mark\n",
    "2. Performing the training stage: 2 Marks\n",
    "3. Performing the testing stage: 2 Marks\n",
    "\n",
    "Provide your solution in the Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.913235294117647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassificationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassifier\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.DoubleType\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.linalg.Vector\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mLR_Class\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\n",
    "\n",
    "object LR_Class {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"LR_Class\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"files/spambase.data\").toDF\n",
    "  \n",
    "        val toDouble = udf[Double, Double]( _.toDouble)\n",
    "   \n",
    "val dfnew = df\n",
    ".withColumn(\"a\", toDouble(df(\"_c0\")))\n",
    ".withColumn(\"b\", toDouble(df(\"_c1\")))\n",
    ".withColumn(\"c\", toDouble(df(\"_c2\")))\n",
    ".withColumn(\"d\", toDouble(df(\"_c3\")))\n",
    ".withColumn(\"e\", toDouble(df(\"_c4\")))\n",
    ".withColumn(\"f\", toDouble(df(\"_c5\")))\n",
    ".withColumn(\"g\", toDouble(df(\"_c6\")))\n",
    ".withColumn(\"h\", toDouble(df(\"_c7\")))\n",
    ".withColumn(\"i\", toDouble(df(\"_c8\")))\n",
    ".withColumn(\"j\", toDouble(df(\"_c9\")))\n",
    ".withColumn(\"k\", toDouble(df(\"_c10\")))\n",
    ".withColumn(\"l\", toDouble(df(\"_c11\")))\n",
    ".withColumn(\"m\", toDouble(df(\"_c12\")))\n",
    ".withColumn(\"n\", toDouble(df(\"_c13\")))\n",
    ".withColumn(\"o\", toDouble(df(\"_c14\")))\n",
    ".withColumn(\"p\", toDouble(df(\"_c15\")))\n",
    ".withColumn(\"q\", toDouble(df(\"_c16\")))\n",
    ".withColumn(\"r\", toDouble(df(\"_c17\")))\n",
    ".withColumn(\"s\", toDouble(df(\"_c18\")))\n",
    ".withColumn(\"t\", toDouble(df(\"_c19\")))\n",
    ".withColumn(\"u\", toDouble(df(\"_c20\")))\n",
    ".withColumn(\"v\", toDouble(df(\"_c21\")))\n",
    ".withColumn(\"w\", toDouble(df(\"_c22\")))\n",
    ".withColumn(\"x\", toDouble(df(\"_c23\")))\n",
    ".withColumn(\"y\", toDouble(df(\"_c24\")))\n",
    ".withColumn(\"z\", toDouble(df(\"_c25\")))\n",
    ".withColumn(\"aa\", toDouble(df(\"_c26\")))\n",
    ".withColumn(\"bb\", toDouble(df(\"_c27\")))\n",
    ".withColumn(\"cc\", toDouble(df(\"_c28\")))\n",
    ".withColumn(\"dd\", toDouble(df(\"_c29\")))\n",
    ".withColumn(\"ee\", toDouble(df(\"_c30\")))\n",
    ".withColumn(\"ff\", toDouble(df(\"_c31\")))\n",
    ".withColumn(\"gg\", toDouble(df(\"_c32\")))\n",
    ".withColumn(\"hh\", toDouble(df(\"_c33\")))\n",
    ".withColumn(\"ii\", toDouble(df(\"_c34\")))\n",
    ".withColumn(\"jj\", toDouble(df(\"_c35\")))\n",
    ".withColumn(\"kk\", toDouble(df(\"_c36\")))\n",
    ".withColumn(\"ll\", toDouble(df(\"_c37\")))\n",
    ".withColumn(\"mm\", toDouble(df(\"_c38\")))\n",
    ".withColumn(\"nn\", toDouble(df(\"_c39\")))\n",
    ".withColumn(\"oo\", toDouble(df(\"_c40\")))\n",
    ".withColumn(\"pp\", toDouble(df(\"_c41\")))\n",
    ".withColumn(\"qq\", toDouble(df(\"_c42\")))\n",
    ".withColumn(\"rr\", toDouble(df(\"_c43\")))\n",
    ".withColumn(\"ss\", toDouble(df(\"_c44\")))\n",
    ".withColumn(\"tt\", toDouble(df(\"_c45\")))\n",
    ".withColumn(\"uu\", toDouble(df(\"_c46\")))\n",
    ".withColumn(\"vv\", toDouble(df(\"_c47\")))\n",
    ".withColumn(\"ww\", toDouble(df(\"_c48\")))\n",
    ".withColumn(\"xx\", toDouble(df(\"_c49\")))\n",
    ".withColumn(\"yy\", toDouble(df(\"_c50\")))\n",
    ".withColumn(\"zz\", toDouble(df(\"_c51\")))\n",
    ".withColumn(\"aaa\", toDouble(df(\"_c52\")))\n",
    ".withColumn(\"bbb\", toDouble(df(\"_c53\")))\n",
    ".withColumn(\"ccc\", toDouble(df(\"_c54\")))\n",
    ".withColumn(\"ddd\", toDouble(df(\"_c55\")))\n",
    ".withColumn(\"eee\", toDouble(df(\"_c56\")))\n",
    ".withColumn(\"label\", toDouble(df(\"_c57\")))\n",
    ".select(\"a\",\"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"aa\",\"bb\", \"cc\", \"dd\",\n",
    "        \"ee\", \"ff\", \"gg\", \"hh\", \"ii\", \"jj\",\"kk\",\"ll\",\"mm\",\"nn\",\"oo\",\"pp\",\"qq\",\"rr\",\"ss\",\"tt\",\"uu\",\"vv\",\"ww\",\"xx\",\"y\",\"zz\",\"aaa\",\"bbb\",\"ccc\",\"ddd\",\"eee\",\"label\")\n",
    "\n",
    "        //dfnew.show()\n",
    "        \n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"a\",\"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"aa\",\"bb\", \"cc\", \"dd\",\n",
    "        \"ee\", \"ff\", \"gg\", \"hh\", \"ii\", \"jj\",\"kk\",\"ll\",\"mm\",\"nn\",\"oo\",\"pp\",\"qq\",\"rr\",\"ss\",\"tt\",\"uu\",\"vv\",\"ww\",\"xx\",\"y\",\"zz\",\"aaa\",\"bbb\",\"ccc\",\"ddd\",\"eee\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setElasticNetParam(0.1)\n",
    "  .setRegParam(0.01)\n",
    "        \n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = dfnew.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, lr))\n",
    "\n",
    "// Train model.\n",
    "val model = pipeline.fit(trainingData)\n",
    " \n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Accuracy: \"  + (accuracy))\n",
    "\n",
    "\n",
    "}\n",
    "}\n",
    "LR_Class.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
