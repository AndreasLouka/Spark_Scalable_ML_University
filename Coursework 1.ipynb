{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 - 2017: Courserwork 1\n",
    "\n",
    "Deadline: 11:59PM on Thursday 23 Feb 2017\n",
    "\n",
    "Submission: via SageMatchCloud (We will collect from you automatically.)\n",
    "\n",
    "## Exercise 1 [1 mark]\n",
    "\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) contains information extracted from the US 1994 Census database. \n",
    "\n",
    "Compute the correlation between the column [age](./files/age.txt) and the column [salary](./files/salary.txt) extracted from the dataset. Provide the .sbt file and the .scala file necessary to run the code on the HPC. The variable salary has two values: 0 denotes a salary below or equal to 50K, 1 denotes a salary higher than 50K.\n",
    "\n",
    "## Answer to Exercise 1\n",
    "Correlation is 0.2303\n",
    "\n",
    "## Exercise 2 [3 marks]\n",
    "Use the complete dataset [Census Income Data](./files/CensusIncomeData.csv), to answer the following questions (provide the corresponding .scala files used to answer the questions)\n",
    "\n",
    "Question 1: How many Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool are there in the dataset?\n",
    "\n",
    "Question 2: How many Bachelors earn more that 50K? How many earn less or equal to 50K? Hint: filter the dataset by Bachelor, and count how many Bachelors are there per class. \n",
    "\n",
    "## Answer to Exercise 2\n",
    "\n",
    "Prof-school: 444, 10th: 753, 7th-8th: 508, 5th-6th: 252, Assoc-acdm: 837, Assoc-voc: 1090, Masters: 1341, 12th: 334, Preschool: 40, 9th: 411, Bachelors: 4261, Doctorate: 326, HS-grad: 8352, 11th: 933, Some-college: 5771, 1st-4th:  122\n",
    "\n",
    "The total number of Bachelors is: 4261.\n",
    "The number of Bachelors that earn less than 50k is: 2507.\n",
    "The number of Bachelors that earn more than 50k is: 1754.\n",
    "\n",
    "\n",
    "## Exercise 3 [3 marks]\n",
    "\n",
    "The file [TV shows and channels](./files/tv_shows_channels.txt) contains a list of different TV shows in different TV channels. On the other hand, the file [TV shows and viewers](./files/tv_shows_viewers.txt) contains the amount of viewers that watched any show during a particular week. Which ABC's TV show had the highest number of viewers? Provide the .sbt file and the .scala code that can be run on the HPC. \n",
    "\n",
    "## Answer to Exercise 3\n",
    "\n",
    "Hourly_Talking,19704.0\n",
    "\n",
    "\n",
    "## Exercise 4 [6 marks]\n",
    "\n",
    "In this exercise, you will build a pipeline to analyze a textfile. Please do the following:\n",
    "\n",
    "Step 1. Find a text file of your favourite (English) novel.\n",
    "\n",
    "Step 2. Build a pipeline that takes the text file in Step 1, tokenize it, remove stop words, work out the n-grams with n=2 and n=3 (i.e., 2-grams and 3-grams). Please refer to https://spark.apache.org/docs/2.1.0/ml-features.html#n-gram\n",
    "\n",
    "Step 3. Find the top 10 most frequently used 2-grams and 3-grams (20 in total).\n",
    "\n",
    "Step 4. Find the top 10 most frequently used 2-grams and 3-grams using the pipeline in Step 2 without removing stop words.\n",
    "\n",
    "Step 5. Briefly describe 3-5 interesting findings.\n",
    "\n",
    "Note: If the file is too big for you to process, keep the first few chapters. Briefly explain how much you've kept and why.\n",
    "\n",
    "\n",
    "## Answer to Exercise 4\n",
    "\n",
    "1) N-grams have many non letter characters, due to the fact that only stopword removal has been applied to the given text file.\n",
    "2) The most frequent 2-gram and also 3-gram is 'space' character, again because tokenization is performed on white space.\n",
    "3) If stopword removal is not applied, then the most frequent n-grams include mostly stop-words.\n",
    "4) A lot of the most frequent 2-grams and 3-grams (both with and without the stopword removal) include 'space'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.Statistics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation is: 0.23030365950430015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.Statistics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mAge_and_Salary_Correlation\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Exercise 1:\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "object Age_and_Salary_Correlation {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Correlation\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        //Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant\n",
    "        //collection of elements that can be operated on in parallel. There are two ways to create RDDs:\n",
    "        //parallelizing an existing collection in your driver program, or referencing a dataset in an external storage\n",
    "        //system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "        \n",
    "        \n",
    "        val age = sc.textFile(\"files/age.txt\").map(_.toDouble)\n",
    "        val salary = sc.textFile(\"files/salary.txt\").map(_.toDouble)\n",
    "\n",
    "        val correlation: Double = Statistics.corr(age, salary)\n",
    "        println(s\"Correlation is: $correlation\")\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "Age_and_Salary_Correlation.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+-------------+---+--------------------+------------------+--------------+-------------------+-------+-----+----+----+--------------+------+\n",
      "|_c0|              _c1|   _c2|          _c3|_c4|                 _c5|               _c6|           _c7|                _c8|    _c9| _c10|_c11|_c12|          _c13|  _c14|\n",
      "+---+-----------------+------+-------------+---+--------------------+------------------+--------------+-------------------+-------+-----+----+----+--------------+------+\n",
      "| 39|        State-gov| 77516|    Bachelors| 13|       Never-married|      Adm-clerical| Not-in-family|              White|   Male| 2174|   0|  40| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc| 83311|    Bachelors| 13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|    0|   0|  13| United-States| <=50K|\n",
      "| 38|          Private|215646|      HS-grad|  9|            Divorced| Handlers-cleaners| Not-in-family|              White|   Male|    0|   0|  40| United-States| <=50K|\n",
      "| 53|          Private|234721|         11th|  7|  Married-civ-spouse| Handlers-cleaners|       Husband|              Black|   Male|    0|   0|  40| United-States| <=50K|\n",
      "| 28|          Private|338409|    Bachelors| 13|  Married-civ-spouse|    Prof-specialty|          Wife|              Black| Female|    0|   0|  40|          Cuba| <=50K|\n",
      "| 37|          Private|284582|      Masters| 14|  Married-civ-spouse|   Exec-managerial|          Wife|              White| Female|    0|   0|  40| United-States| <=50K|\n",
      "| 49|          Private|160187|          9th|  5| Married-spouse-a...|     Other-service| Not-in-family|              Black| Female|    0|   0|  16|       Jamaica| <=50K|\n",
      "| 52| Self-emp-not-inc|209642|      HS-grad|  9|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male|    0|   0|  45| United-States|  >50K|\n",
      "| 31|          Private| 45781|      Masters| 14|       Never-married|    Prof-specialty| Not-in-family|              White| Female|14084|   0|  50| United-States|  >50K|\n",
      "| 42|          Private|159449|    Bachelors| 13|  Married-civ-spouse|   Exec-managerial|       Husband|              White|   Male| 5178|   0|  40| United-States|  >50K|\n",
      "| 37|          Private|280464| Some-college| 10|  Married-civ-spouse|   Exec-managerial|       Husband|              Black|   Male|    0|   0|  80| United-States|  >50K|\n",
      "| 30|        State-gov|141297|    Bachelors| 13|  Married-civ-spouse|    Prof-specialty|       Husband| Asian-Pac-Islander|   Male|    0|   0|  40|         India|  >50K|\n",
      "| 23|          Private|122272|    Bachelors| 13|       Never-married|      Adm-clerical|     Own-child|              White| Female|    0|   0|  30| United-States| <=50K|\n",
      "| 32|          Private|205019|   Assoc-acdm| 12|       Never-married|             Sales| Not-in-family|              Black|   Male|    0|   0|  50| United-States| <=50K|\n",
      "| 40|          Private|121772|    Assoc-voc| 11|  Married-civ-spouse|      Craft-repair|       Husband| Asian-Pac-Islander|   Male|    0|   0|  40|             ?|  >50K|\n",
      "| 34|          Private|245487|      7th-8th|  4|  Married-civ-spouse|  Transport-moving|       Husband| Amer-Indian-Eskimo|   Male|    0|   0|  45|        Mexico| <=50K|\n",
      "| 25| Self-emp-not-inc|176756|      HS-grad|  9|       Never-married|   Farming-fishing|     Own-child|              White|   Male|    0|   0|  35| United-States| <=50K|\n",
      "| 32|          Private|186824|      HS-grad|  9|       Never-married| Machine-op-inspct|     Unmarried|              White|   Male|    0|   0|  40| United-States| <=50K|\n",
      "| 38|          Private| 28887|         11th|  7|  Married-civ-spouse|             Sales|       Husband|              White|   Male|    0|   0|  50| United-States| <=50K|\n",
      "| 43| Self-emp-not-inc|292175|      Masters| 14|            Divorced|   Exec-managerial|     Unmarried|              White| Female|    0|   0|  45| United-States|  >50K|\n",
      "+---+-----------------+------+-------------+---+--------------------+------------------+--------------+-------------------+-------+-----+----+----+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|          _c3|count|\n",
      "+-------------+-----+\n",
      "|  Prof-school|  444|\n",
      "|         10th|  753|\n",
      "|      7th-8th|  508|\n",
      "|      5th-6th|  252|\n",
      "|   Assoc-acdm|  837|\n",
      "|    Assoc-voc| 1090|\n",
      "|         null|    1|\n",
      "|      Masters| 1341|\n",
      "|         12th|  334|\n",
      "|    Preschool|   40|\n",
      "|          9th|  411|\n",
      "|    Bachelors| 4261|\n",
      "|    Doctorate|  326|\n",
      "|      HS-grad| 8352|\n",
      "|         11th|  933|\n",
      "| Some-college| 5771|\n",
      "|      1st-4th|  122|\n",
      "+-------------+-----+\n",
      "\n",
      "The total number of Bachelors is: 4261\n",
      "The number of Bachelors that earn less than 50k is: 2507\n",
      "The number of Bachelors that earn more than 50k is: 1754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions.col\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mGet_Words\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Exercise 2\n",
    "import scala.io.Source\n",
    "\n",
    "\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.functions.col\n",
    "object Get_Words {\n",
    "  def main(args: Array[String]) {\n",
    "      val sparkSession = SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Word Counting\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    val sc = sparkSession.sparkContext\n",
    "\n",
    "    val df = sparkSession.read\n",
    "            .format(\"com.databricks.spark.csv\")\n",
    "            .option(\"header\", \"false\") //reading the headers\n",
    "            .option(\"mode\", \"DROPMALFORMED\")\n",
    "            .load(\"files/CensusIncomeData.csv\");\n",
    "            \n",
    "      val selection = df.select(\"_c3\")\n",
    "      \n",
    "      df.show()\n",
    "      //or use: val income_data = sparkSession.read.option(\"header\",\"false\").csv(\"files/CensusIncomeData.csv\")\n",
    "      val counting_occurances = df.groupBy(\"_c3\").count().show()\n",
    "      \n",
    "      \n",
    "     val bachelors = df.filter(col(\"_c3\").like(\"%Bachelors\"))\n",
    "     val bachelors_less = bachelors.filter(col(\"_c14\") === \" <=50K\").count()\n",
    "     val bachelors_more = bachelors.filter(col(\"_c14\") === \" >50K\").count()\n",
    "      \n",
    "     val total_bachelors = bachelors.count()\n",
    "     println(s\"The total number of Bachelors is: $total_bachelors\")\n",
    "     println(s\"The number of Bachelors that earn less than 50k is: $bachelors_less\")\n",
    "     println(s\"The number of Bachelors that earn more than 50k is: $bachelors_more\")\n",
    "    }\n",
    "}\n",
    "Get_Words.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+\n",
      "|               _c0|_c1|\n",
      "+------------------+---+\n",
      "|     Hourly_Sports|DEF|\n",
      "|        Baked_News|BAT|\n",
      "|PostModern_Talking|XYZ|\n",
      "|         Loud_News|CNO|\n",
      "|       Almost_Show|ABC|\n",
      "|       Hot_Talking|DEF|\n",
      "|         Dumb_Show|BAT|\n",
      "|      Surreal_Show|XYZ|\n",
      "|      Cold_Talking|CNO|\n",
      "|    Hourly_Cooking|ABC|\n",
      "|     Baked_Cooking|DEF|\n",
      "|   PostModern_News|BAT|\n",
      "|      Loud_Cooking|XYZ|\n",
      "|     Almost_Sports|CNO|\n",
      "|          Hot_Show|ABC|\n",
      "|       Dumb_Sports|DEF|\n",
      "|    Surreal_Sports|BAT|\n",
      "|         Cold_Show|XYZ|\n",
      "|      Hourly_Games|CNO|\n",
      "|       Baked_Games|ABC|\n",
      "+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+---+\n",
      "|               _c0|_c1|\n",
      "+------------------+---+\n",
      "|     Hourly_Sports| 21|\n",
      "|   PostModern_Show| 38|\n",
      "|      Surreal_News| 73|\n",
      "|      Dumb_Cooking|144|\n",
      "|      Cold_Talking|287|\n",
      "|    Almost_Talking|574|\n",
      "|         Loud_News|113|\n",
      "|       Hot_Talking|228|\n",
      "|       Baked_Games|459|\n",
      "|    Hourly_Talking|922|\n",
      "| PostModern_Sports|813|\n",
      "|   Surreal_Cooking|596|\n",
      "|        Dumb_Games|163|\n",
      "|         Cold_Show|334|\n",
      "|       Almost_Show|677|\n",
      "|      Loud_Cooking|328|\n",
      "|          Hot_News|667|\n",
      "|     Baked_Talking|310|\n",
      "|       Hourly_Show|633|\n",
      "|PostModern_Talking|244|\n",
      "+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-------+\n",
      "|         tv-show|    sum|\n",
      "+----------------+-------+\n",
      "|  Hourly_Talking|19704.0|\n",
      "|    Dumb_Talking|18704.0|\n",
      "|      Loud_Games|10304.0|\n",
      "|       Dumb_Show| 9956.0|\n",
      "| PostModern_News| 9736.0|\n",
      "|     Baked_Games| 9692.0|\n",
      "|     Cold_Sports| 9636.0|\n",
      "|       Hot_Games| 8716.0|\n",
      "|     Almost_Show| 8532.0|\n",
      "|  Hourly_Cooking| 8452.0|\n",
      "|PostModern_Games| 8244.0|\n",
      "|    Almost_Games| 8149.0|\n",
      "|  Surreal_Sports| 8144.0|\n",
      "|        Hot_Show| 8036.0|\n",
      "|    Surreal_News| 8024.0|\n",
      "|     Hourly_Show| 7992.0|\n",
      "|      Baked_News| 7824.0|\n",
      "|       Loud_Show| 7804.0|\n",
      "|     Almost_News| 7596.0|\n",
      "|       Cold_News| 7500.0|\n",
      "+----------------+-------+\n",
      "\n",
      "[Hourly_Talking,19704.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.Statistics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mhighest_view\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Exercise 3\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "object highest_view {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Correlation\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "     \n",
    "        val df_tv_channels= sparkSession.read\n",
    "            .format(\"com.databricks.spark.csv\")\n",
    "            .option(\"header\", \"false\") //reading the headers\n",
    "            .option(\"mode\", \"DROPMALFORMED\")\n",
    "            .load(\"files/tv_shows_channels.txt\");\n",
    "        \n",
    "        val df_tv_reviews= sparkSession.read\n",
    "            .format(\"com.databricks.spark.csv\")\n",
    "            .option(\"header\", \"false\") //reading the headers\n",
    "            .option(\"mode\", \"DROPMALFORMED\")\n",
    "            .load(\"files/tv_shows_viewers.txt\");\n",
    "   \n",
    "        \n",
    "        df_tv_channels.show()\n",
    "        df_tv_reviews.show()\n",
    "        \n",
    "        val col_1 = Seq(\"tv-show\", \"tv-channel\")\n",
    "        val col_2 = Seq(\"tv-show\", \"tv-viewers\")\n",
    "        \n",
    "        \n",
    "        val df_tv_shows_channels = df_tv_channels.toDF(col_1:_*)\n",
    "        val df_tv_shows_reviews = df_tv_reviews.toDF(col_2:_*)\n",
    "        \n",
    "        \n",
    "        val the_show_is = df_tv_shows_channels.filter(col(\"tv-channel\") === \"ABC\").join(df_tv_shows_reviews, \"tv-show\")\n",
    "                                                                       .select(df_tv_shows_channels(\"tv-show\"), df_tv_shows_reviews(\"tv-viewers\"))\n",
    "                                                                       .groupBy(col(\"tv-show\"))\n",
    "                                                                       .agg(sum(col(\"tv-viewers\")).as(\"sum\"))\n",
    "                                                                       .orderBy(col(\"sum\").desc)\n",
    "        \n",
    "        the_show_is.show()\n",
    "        println(the_show_is.first)\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "highest_view.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|2grams           |2grams_count|\n",
      "+-----------------+------------+\n",
      "|                 |203         |\n",
      "|* *              |6           |\n",
      "| chris           |5           |\n",
      "| *               |3           |\n",
      "| \"i              |3           |\n",
      "|# #              |2           |\n",
      "| didn't          |2           |\n",
      "|physical training|2           |\n",
      "| \"have           |2           |\n",
      "|rick arnold      |2           |\n",
      "+-----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------+------------+\n",
      "|3grams                |3grams_count|\n",
      "+----------------------+------------+\n",
      "|                      |168         |\n",
      "|  chris               |5           |\n",
      "|  \"i                  |3           |\n",
      "|* * *                 |3           |\n",
      "| * *                  |3           |\n",
      "|  *                   |3           |\n",
      "|  \"have               |2           |\n",
      "|  \"you                |2           |\n",
      "|thinking getting out.\"|1           |\n",
      "|  female              |1           |\n",
      "+----------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+------------+\n",
      "|2grams    |2grams_count|\n",
      "+----------+------------+\n",
      "|          |203         |\n",
      "|* *       |6           |\n",
      "| chris    |5           |\n",
      "|the doctor|4           |\n",
      "| he       |4           |\n",
      "| *        |3           |\n",
      "|have to   |3           |\n",
      "|of the    |3           |\n",
      "|you have  |3           |\n",
      "|do you    |3           |\n",
      "+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------------+\n",
      "|3grams  |3grams_count|\n",
      "+--------+------------+\n",
      "|        |168         |\n",
      "|  chris |5           |\n",
      "|* * *   |3           |\n",
      "| * *    |3           |\n",
      "|  *     |3           |\n",
      "|  \"i    |3           |\n",
      "|  \"have |2           |\n",
      "| \"i just|2           |\n",
      "|  the   |2           |\n",
      "|  \"you  |2           |\n",
      "+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StopWordsRemover\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.NGram\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.Statistics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mnovel\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.NGram\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "\n",
    "object novel {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Correlation\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    " \n",
    "        val df_ = sparkSession.read\n",
    "            .format(\"com.databricks.spark.csv\")\n",
    "            .option(\"header\", \"false\")\n",
    "            .option(\"mode\", \"DROPMALFORMED\")\n",
    "            .load(\"files/novel.txt\");\n",
    "  \n",
    "        \n",
    "        val df = df_.toDF(\"lines\")\n",
    "        \n",
    "        val tokenisation = new Tokenizer().setInputCol(\"lines\").setOutputCol(\"tokens\")\n",
    "        val stopword = new StopWordsRemover().setInputCol(\"tokens\").setOutputCol(\"stopwordfree\")\n",
    "        val ngrams = new NGram().setN(2).setInputCol(\"stopwordfree\").setOutputCol(\"ngrams\")\n",
    "        val ngrams3 = new NGram().setN(3).setInputCol(\"stopwordfree\").setOutputCol(\"ngrams3\")\n",
    "        val pipeline = new Pipeline().setStages(Array(tokenisation, stopword, ngrams, ngrams3))\n",
    "        val fit_pipe = pipeline.fit(df)\n",
    "        val fitted = fit_pipe.transform(df)\n",
    "        \n",
    "        val results = fitted.withColumn(\"2grams\", explode(col(\"ngrams\")))\n",
    "        results.groupBy(\"2grams\").agg(count(\"*\") as \"2grams_count\").orderBy(desc(\"2grams_count\")).show(10, false)\n",
    "        \n",
    "        val results_3 = fitted.withColumn(\"3grams\", explode(col(\"ngrams3\")))\n",
    "        results_3.groupBy(\"3grams\").agg(count(\"*\") as \"3grams_count\").orderBy(desc(\"3grams_count\")).show(10, false)\n",
    "\n",
    "        \n",
    "        \n",
    "        val tokenisation_2 = new Tokenizer().setInputCol(\"lines\").setOutputCol(\"tokens\")        \n",
    "        val ngrams_2 = new NGram().setN(2).setInputCol(\"tokens\").setOutputCol(\"ngrams\")\n",
    "        val ngrams_3 = new NGram().setN(3).setInputCol(\"tokens\").setOutputCol(\"ngrams3\")        \n",
    "        val pipeline2 = new Pipeline().setStages(Array(tokenisation_2, ngrams_2, ngrams_3))\n",
    "        val fit_pipe_2 = pipeline2.fit(df)\n",
    "        val fitted_2 = fit_pipe_2.transform(df)\n",
    "        \n",
    "        val results_2 = fitted_2.withColumn(\"2grams\", explode(col(\"ngrams\")))\n",
    "        results_2.groupBy(\"2grams\").agg(count(\"*\") as \"2grams_count\").orderBy(desc(\"2grams_count\")).show(10, false)\n",
    "        \n",
    "        val results_3_2 = fitted_2.withColumn(\"3grams\", explode(col(\"ngrams3\")))\n",
    "        results_3_2.groupBy(\"3grams\").agg(count(\"*\") as \"3grams_count\").orderBy(desc(\"3grams_count\")).show(10, false)\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "novel.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
