{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 - 2017: Courserwork 2\n",
    "\n",
    "Deadline: 11:59PM on Thursday 9 March 2017\n",
    "\n",
    "Submission: via SageMatchCloud (We will collect from you automatically.)\n",
    "\n",
    "## Exercise 1 [8 marks]\n",
    "\n",
    "In this exercise, you will study parallelization in detail by analyzing a text file and examining the time cost.\n",
    "\n",
    "Please do the following:\n",
    "\n",
    "Step 1. Download [Enron data](https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.enron.txt.gz) and understand the file format described at [Bag of Words Data Set](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).\n",
    "\n",
    "Step 2. Read the <tt>.gz</tt> file in DIRECTLY.\n",
    "\n",
    "Step 3. Get the counts for each word using a number transformations and action(s). Note that the triple has a format of \"docID wordID count\" and you need to get the overall counts for each wordID. This format is different from Exercise 4 of Coursework 1.\n",
    "\n",
    "Step 4. Set your job to be executed using only one core (thread).\n",
    "\n",
    "Step 5. Set the number of partitions to be 20, 40, and 80. Time the execution and report the time for each of the three cases.\n",
    "\n",
    "Step 6. Set your job to be executed using more than one cores (threads) and explain your choice. Repeat Step 5.\n",
    "\n",
    "Step 7. Briefly describe 3-5 interesting findings.\n",
    "\n",
    "\n",
    "## Solution Exercise 1\n",
    "\n",
    "1 core, 1 partition = 5 seconds\n",
    "\n",
    "1 core, 20 partitions = 16 seconds\n",
    "\n",
    "1 core, 40 partitions = 15 seconds\n",
    "\n",
    "1 core, 80 partitions = 15 seconds\n",
    "\n",
    "7 cores, 20 partitions = 4 seconds\n",
    "\n",
    "14 cores, 40 partitions = 3 seconds\n",
    "\n",
    "27 cores, 80 partitions = 4 seconds\n",
    "\n",
    "\n",
    "> The faster execution time is when every 1 core uses 2-4 partitions (average 3). IE. for 7 cores - 21 partitions.\n",
    "> By increasing the number of partitions alone, the execution time increases instead of decreasing.\n",
    "> Also, by increasing the number of cores alone, does not decrease the time. Additionally, its a waste to use a lot of cores and have few number of partitions because you are not utilizing the cores well.\n",
    "\n",
    "## Exercise 2 [5 marks]\n",
    "\n",
    "In this exercise, you will perform sentiment analysis using a decision tree. Each row in the file [Sentiment Analysis](./files/sentiment_data.txt) contains a review obtained from Amazon, Yelp or IMDB, and a score, where a score of one indicates a positive comment, and a score of zero indicates a negative comment. Provide a pipeline for feature extraction, and classification using decision trees, and report a table with the different performances that you obtain depending on the parameters used in the pipeline. Which configuration in the pipeline provides the best performance?\n",
    "\n",
    "\n",
    "## Solution Exercise 2\n",
    "\n",
    "\n",
    "| MaxDepth | Impurity | MaxBins | ACCURACY |\n",
    "|----------|----------|---------|----------|\n",
    "|   10     |  gini    |   10    |   0.641  |\n",
    "|   10     |  gini    |   20    |   0.626  |\n",
    "|   10     |  gini    |   30    |   0.630  |\n",
    "|   10     |  entropy |   10    |   0.651  |\n",
    "|   10     |  entropy |   20    |   0.652  |\n",
    "|   10     |  entropy |   30    |   0.658  |\n",
    "|   15     |  gini    |   10    |   0.658  |\n",
    "|   15     |  gini    |   20    |   0.667  |\n",
    "|   15     |  gini    |   30    |   0.656  |\n",
    "|   15     |  entropy |   10    |   0.634  |\n",
    "|   15     |  entropy |   20    |   0.641  |\n",
    "|   15     |  entropy |   30    |   0.664  |\n",
    "|   20     |  gini    |   10    |   0.664  |\n",
    "|   20     |  gini    |   20    |   0.655  |\n",
    "|   20     |  gini    |   30    |   0.666  |\n",
    "|   20     |  entropy |   10    |   0.671  |\n",
    "|   20     |  entropy |   20    |   0.656  |\n",
    "|   20     |  entropy |   30    |   0.660  |\n",
    "\n",
    "\n",
    "The highest accuracy (0.671) is obtained with MaxDepth = 20, Impurity = Entropy, MaxBins = 10. Therefore, by using these parameters training and testing was altered to see if it affects the accuracy:\n",
    "\n",
    "| Training| Testing | ACCYRACY |\n",
    "|---------|---------|----------|\n",
    "|  0.8    |   0.2   |   0.669  |\n",
    "|  0.7    |   0.3   |   0.671  |\n",
    "|  0.6    |   0.4   |   0.670  |\n",
    "|  0.5    |   0.5   |   0.660  |\n",
    "|  0.4    |   0.6   |   0.659  |\n",
    "\n",
    "\n",
    "Therefore, the pipeline configuration that provides the highest accuracy is:\n",
    "\n",
    "| Training | Testing | MaxDepth | Impurity | MaxBins | ACCURACY |\n",
    "|----------|---------|----------|----------|---------|----------|\n",
    "|    0.7   |   0.3   |   20     | Entropy  |   10    |  0.671   |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|wordID| count|\n",
      "+------+------+\n",
      "| 18130|1989.0|\n",
      "| 14204|1075.0|\n",
      "| 26005|  74.0|\n",
      "| 23843|2179.0|\n",
      "| 17686|6009.0|\n",
      "| 23097|  74.0|\n",
      "| 12394| 183.0|\n",
      "| 11888| 612.0|\n",
      "| 24269| 621.0|\n",
      "| 14369|  23.0|\n",
      "| 20569| 904.0|\n",
      "| 21452| 122.0|\n",
      "| 14157| 281.0|\n",
      "| 25555|1069.0|\n",
      "| 14887|1162.0|\n",
      "| 23459|  84.0|\n",
      "|  1159| 559.0|\n",
      "| 25032| 532.0|\n",
      "|   467| 278.0|\n",
      "| 18726| 170.0|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "8.390005701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions.col\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mE_1\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Exercise 1:\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "object E_1 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        \n",
    "        //read file DIRECTLY:\n",
    "        val enronDF = sparkSession.sparkContext.textFile(\"files/docword.enron.txt.gz\")\n",
    "       \n",
    "       \n",
    "        val df = enronDF.map(line => line.split(\" \")).\n",
    "                      filter(lines => lines.length == 3). \n",
    "                      map(row => (row(0), row(1), row(2))).\n",
    "                      toDF(\"docID\", \"wordID\", \"count1\") \n",
    "        //df.show()\n",
    "        \n",
    "        val timing = System.nanoTime\n",
    "        \n",
    "        val df_partitions = df.repartition(20)\n",
    "        val get_counts = df_partitions.select($\"wordID\", $\"count1\").groupBy(\"wordID\").agg(sum($\"count1\").as(\"count\")).show()\n",
    "        \n",
    "        println((System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "E_1.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|Wow... Loved this...|    1|\n",
      "|  Crust is not good.|    0|\n",
      "|Not tasty and the...|    0|\n",
      "|Stopped by during...|    1|\n",
      "|The selection on ...|    1|\n",
      "|Now I am getting ...|    0|\n",
      "|Honeslty it didn'...|    0|\n",
      "|The potatoes were...|    0|\n",
      "|The fries were gr...|    1|\n",
      "|      A great touch.|    1|\n",
      "|Service was very ...|    1|\n",
      "|  Would not go back.|    0|\n",
      "|The cashier had n...|    0|\n",
      "|I tried the Cape ...|    1|\n",
      "|I was disgusted b...|    0|\n",
      "|I was shocked bec...|    0|\n",
      "| Highly recommended.|    1|\n",
      "|Waitress was a li...|    0|\n",
      "|This place is not...|    0|\n",
      "|did not like at all.|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy: 0.6446886446886447\n",
      "+--------------------+-----+------------+--------------------+--------------------+-------------+--------------------+----------+--------------+\n",
      "|            Features|label|indexedLabel|              tokens|        stopwordfree|rawPrediction|         probability|prediction|predictedLabel|\n",
      "+--------------------+-----+------------+--------------------+--------------------+-------------+--------------------+----------+--------------+\n",
      "|(5665,[0,1,2,3,4,...|    0|         1.0|[\", the, structur...|[\", structure, fi...|   [92.0,0.0]|           [1.0,0.0]|       0.0|             1|\n",
      "|(5665,[0,2,3,4,5,...|    1|         0.0|[\"you'll, love, i...|[\"you'll, love, i...|   [92.0,0.0]|           [1.0,0.0]|       0.0|             1|\n",
      "|(5665,[116,1606,3...|    0|         1.0|[(it, wasn't, bus...|[(it, wasn't, bus...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|        (5665,[],[])|    1|         0.0|      [(it, works!)]|      [(it, works!)]|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[17,66,106,...|    0|         1.0|[), don't, waste,...|[), don't, waste,...|   [0.0,16.0]|           [0.0,1.0]|       1.0|             0|\n",
      "|(5665,[1,184,731,...|    1|         0.0|[), a, happy,, wo...|[), happy,, wonde...|    [0.0,1.0]|           [0.0,1.0]|       1.0|             0|\n",
      "|(5665,[92,133,190...|    1|         0.0|[*, both, the, ho...|[*, hot, &, sour,...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[16,55,140,...|    1|         0.0|[*, comes, with, ...|[*, comes, strong...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[127,678],[...|    1|         0.0|[10, out, of, 10,...|    [10, 10, stars.]|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[45,256,463...|    1|         0.0|[:-)oh,, the, cha...|[:-)oh,, charger,...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|   (5665,[42],[1.0])|    1|         0.0|[;), recommend, w...|[;), recommend, c...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|    (5665,[4],[1.0])|    0|         1.0|[a, lassie, movie...|[lassie, movie, \"...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[4,48,115,2...|    0|         1.0|[a, cheap, and, c...|[cheap, cheerless...|   [0.0,13.0]|           [0.0,1.0]|       1.0|             0|\n",
      "|(5665,[258,376,98...|    1|         0.0|[a, couple, of, m...|[couple, months, ...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[295,3585,4...|    1|         0.0|[a, fantastic, ne...|[fantastic, neigh...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[2,10,4091]...|    1|         0.0|[a, great, film, ...|[great, film, gre...|   [92.0,0.0]|           [1.0,0.0]|       0.0|             1|\n",
      "|(5665,[1,7,90,152...|    1|         0.0|[a, lot, of, webs...|[lot, websites, r...|   [84.0,2.0]|[0.97674418604651...|       0.0|             1|\n",
      "|(5665,[10,119,165...|    1|         0.0|[a, very, charmin...|[charming, film, ...|   [11.0,0.0]|           [1.0,0.0]|       0.0|             1|\n",
      "|(5665,[162,364,60...|    0|         1.0|[a, week, later, ...|[week, later, act...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "|(5665,[46,68,185,...|    0|         1.0|[after, the, firs...|[first, charge, k...|[585.0,792.0]|[0.42483660130718...|       1.0|             0|\n",
      "+--------------------+-----+------------+--------------------+--------------------+-------------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassificationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassifier\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StopWordsRemover\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mE_2\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "// decision tree imports\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "\n",
    "// importing CSV data into the expected format\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "object E_2 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E2\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "\n",
    "\n",
    "\n",
    "val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .load(\"files/sentiment_data.txt\").toDF\n",
    "\n",
    "//rename columns:\n",
    "val col_1 = Seq(\"features\", \"label\")\n",
    "               \n",
    "val df_new = df.toDF(col_1:_*)\n",
    "\n",
    "        df_new.show()\n",
    "        \n",
    "\n",
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(df_new)\n",
    "        \n",
    "        \n",
    "val tokenisation = new Tokenizer().setInputCol(\"features\").setOutputCol(\"tokens\")\n",
    "        \n",
    "val stopword = new StopWordsRemover().setInputCol(\"tokens\").setOutputCol(\"stopwordfree\")  \n",
    "\n",
    "   \n",
    "// fit a CountVectorizerModel from the corpus\n",
    "val CountVectorizerModel = new CountVectorizer()\n",
    "  .setInputCol(\"stopwordfree\")\n",
    "  .setOutputCol(\"Features\")  \n",
    "\n",
    "        \n",
    "// Train a DecisionTree model.\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"Features\")\n",
    "  .setMaxDepth(20)\n",
    "  .setImpurity(\"entropy\")\n",
    "  .setMaxBins(10)\n",
    "        \n",
    "// Convert indexed labels back to original labels.\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = df_new.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, tokenisation, stopword, CountVectorizerModel, dt, labelConverter))\n",
    "        \n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "        \n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Accuracy: \"  + (accuracy))\n",
    "\n",
    "predictions.show()\n",
    "\n",
    "        \n",
    "}\n",
    "}\n",
    "E_2.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
