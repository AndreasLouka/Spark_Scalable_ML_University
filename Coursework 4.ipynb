{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 - 2017: Coursework 4\n",
    "\n",
    "Deadline: 11:59PM on Thursday 27 April 2017\n",
    "\n",
    "Submission: via SageMatchCloud (We will collect from you automatically). \n",
    "Please upload everything in a zip file to SageMathCloud as instructed in Coursework 1 **EXCEPT the downloaded data because they have large size**. <br />\n",
    "Check the filesize of the zip file uploaded to make sure it is not too big.<br />\n",
    "Please start early since some part could be challenging and time-consuming on the server side.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 1 (7 Marks)\n",
    "\n",
    "Compare linear regression and Gamma regression for the following dataset\n",
    "\n",
    "[Allstate claim prediction challenge](https://www.kaggle.com/c/ClaimPredictionChallenge/data)\n",
    "\n",
    "Use different link functions, and report the different performances. Use a metric performance that makes sense to you. Discuss which link functions make more or less sense to use.\n",
    "\n",
    "## Exercise 2 (6 Marks)\n",
    "\n",
    "Note 1: Please at least **read through** excercises 1 and 2 in notebook 8 before working on this coursework.<br />\n",
    "Note 2: You have worked on a similar dataset in excercise 1 of coursework 2 already so reviewing that excercise could be useful.<br />\n",
    "Note 3: If you indeed cannot handle the NYTIMES data, remove some of the data to a size that you can handle and provide the details and an explanation.\n",
    "\n",
    "Step 1: Compute the top 5 principal components (PCs) on the NIPS data from the [Bag of Words Data Set](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words) by two methods: <tt>computePrincipalComponents</tt> and then <tt>computeSVD</tt>. Show the two sets of (5) PCs and the corresponding eigenvalues. \n",
    "\n",
    "Step 2: Compute the top 5 PCs on the NYTIMES data from the [Bag of Words Data Set](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words) by <tt>computeSVD</tt> (You are encouraged to try <tt>computePrincipalComponents</tt> and see what will happen). Show the 5 PCs and the corresponding eigenvalues. \n",
    "\n",
    "Step 3: Report the total running time and the number of cores used for each case above and briefly describe 3-5 interesting findings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ANSWERS Exercise 1\n",
    "\n",
    "\n",
    "### Files: Exercise 1 Linear regression: exercise1.1.scala , Exercise 1 Gamma regression: exercise1.2.scala .\n",
    "\n",
    "\n",
    "## 1. Introduction:\n",
    "\n",
    "> The aim of this exercise was to predict insurance claim payments. This was done implementing Linear and Gamma regression models and comparing the results. \n",
    "\n",
    "> The Allstate claim prediction challenge dataset was used to implement the models.\n",
    "\n",
    "> Only the training dataset was used, because the testing dataset does not have target variables. (The training set was divided 70% for trianing purposes and 30% for testing the models).\n",
    "\n",
    ">##### Note1:  \n",
    "> The majority part of the Allstate dataset (about 99%) contains cases were the insurance claim payments were 0 (ie. the label = 0). This created a challenge when trying to implement the Gamma regression model, and this will be explained in further detail down below in section 2.4 Gamma Regression. \n",
    "\n",
    ">##### Note2:\n",
    "> The null values in the dataset were denoted with the symbol \"?\". In order to reduce noise and for more accurate predictions, these null values have been ignored. \n",
    "\n",
    ">##### Note3:\n",
    "> On SageMathCloud smaller versions of datasets were used (on HPC the original size datasets were used). \n",
    "\n",
    "\n",
    "## 2. Models:\n",
    "\n",
    ">### 2.1 Link Functions:\n",
    "\n",
    ">The link functions provide the relationship between the linear predictor and the mean of the distribution function. Both linear and gamma regression support the same link functions, which are: Identity, Log, Inverse.\n",
    "\n",
    ">### 2.2 Preprocessing:\n",
    "\n",
    ">For both the linear and gamma model, the functions StringIndexer and VectorAssembler were used to create the required format of data that can be applied to the models.\n",
    "\n",
    ">### 2.3 Linear Regression:\n",
    "\n",
    ">Linear regression was implemented using the GeneralizedLinearRegression() model with gaussian family.  \n",
    "\n",
    ">>#### Discuss link functions for linear regression:\n",
    ">In linear regression we try to model the relationship between a scalar dependent variable y (in this case the expected mean value of continuous response variable) and one or more explanatory variables. It makes more sense to use the identity link function in this case, since we are directly modeling the mean( Also, the canonical link function for the gaussian family is the identity link function).\n",
    "\n",
    ">### 2.3 Gamma Regression:\n",
    "\n",
    ">Gamma regression was implemented using the GeneralizedLinearRegression() model with gamma family.  \n",
    "\n",
    ">Gamma regressions is mostly appropriate for data that are continuous, positive, right-skew and where variance is near-constant on the log-scale. As explained in note1 the majority of the Allstate dataset contains cases were the insurance claim payments were 0. Therefore, gamma regression could not have been applied to such data. \n",
    "The most accurate approach to overcome this problem would have been to first use classification on the data, and then use the gamma regression. On this exercise a more simpler approach was performed, which was to replace data which were equal to 0.0 with a very small value of 0.01. \n",
    "\n",
    ">>#### Discuss link functions for gamma regression:\n",
    "> When using gamma regression, the canonical link function is the inverse link function. In this case, the domain of the mean of the canonical link function is not the same as the range allowed by the mean. The linear regression may be have negative mean or very large values. Therefore it makes more sense to use the inverse link function for the gamma family.\n",
    "\n",
    "\n",
    "## 3. Results:\n",
    "\n",
    ">Two metric performances were used to evaluate the effectiveness of the models. The first one was the execution time and the second one was the Root-Mean-Square-Error (RMSE). The RMSE is a frequently used measure of the differences between values predicted by a model and the values actually observed. It is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent. \n",
    "\n",
    ">The table below contains the results obtained from both linear and gamma regression models:\n",
    "\n",
    "|  MODEL   |  LINK FUNCTION  | EXECUTION TIME |    RMSE   |   OUTPUT FILE\n",
    "|----------|-----------------|----------------|-----------|----------------------\n",
    "|  Linear  |   identity      |   1899.94      |   38.23   |acp16al_Coursework4.sh.o389308  \n",
    "|  Gamma   |   identity      |   4843.96      |   35.53   |acp16al_Coursework4.sh.o389317\n",
    "|  Gamma   |    inverse      |   4420.12      |   39.35   |acp16al_Coursework4.sh.o389577\n",
    "  \n",
    "\n",
    ">As it can be shown from the results, the gamma and linear regression models provide approximately the same RMSE. On the other hand, the linear model executes much faster than the gamma regression model. Therefore, it can be said that for this particular dataset the linear regression model with identity link function is the most appropriate one to use. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ANSWERS Exercise 2\n",
    "\n",
    "### Files: Exercise 2 nips dataset: exercise 2.1.scala, Exercise 2 nytimes dataset: exercise 2.2.scala\n",
    "\n",
    "## 1. Introduction:\n",
    "\n",
    ">The aim of this exercise was to compute the top 5 principal components (PCs) on two datasets (NIPS and NYTIMES), and show the 5 PCs and the corresponding eigenvalues. This was done with dimensionality reduction.\n",
    "\n",
    ">Dimensionality reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables. The technique used in this lab for dimensionality reduction is Principal Component Analysis (PCA). PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. \n",
    "\n",
    "## 2. Datasets:\n",
    "\n",
    ">### 2.1 NIPS dataset:\n",
    "> The top 5 principal components and the corresponding eigenvalues on the NIPS dataset were computed using two methods: computePrincipalComponentsAndExplainedVariance and then computeSVD. \n",
    "\n",
    "> ##### Note:\n",
    "> computePrincipalComponentsAndExplainedVariance returns the eigenvalues divided by their sum (i.e., normalised eigenvalues), which is a more meaningful representation. On the other hand, the computeSVD does not normalise eigenvales. \n",
    "\n",
    "> The aim of the exercise is to confirm that the two methods are equivalent in terms of PCA. \n",
    "\n",
    ">>## Results:\n",
    "\n",
    "|                     METHOD                        | EXECUTION TIME | NUMBER OF CORES|        OUTPUT FILE           |\n",
    "|---------------------------------------------------|----------------|-----------------------------------------------|\n",
    "|                  computeSVD                       |      68.71     |        7       |acp16al_Coursework4.sh.o389332| \n",
    "|  computePrincipalComponentsAndExplainedVariance   |     5481.38    |        7       |acp16al_Coursework4.sh.o389434| \n",
    "\n",
    ">>>### TOP 5 PCs and CORRESPONING EIGENVALUES for computeSVD:\n",
    "\n",
    "|                                  CORRESPONDING EIGENVALUES                                  | \n",
    "|---------------------------------------------------------------------------------------------|\n",
    "|782462.2912189161, 579322.5708832599, 454138.8057138115, 369230.9128998486, 301412.5136995867|\n",
    "\n",
    "|           PC1         |          PC2           |            PC3         |           PC4          |           PC5          |\n",
    "|-----------------------|------------------------|------------------------|------------------------|------------------------|\n",
    "|4.1593563066249435E-5  | -3.3757028856980356E-4 | -1.5603778464809556E-4 | -4.1925324717908055E-5 | -1.3470262149657035E-4 |  \n",
    "|-3.4978234275155346E-4 | -2.7192153997421806E-4 | 6.13467594646716E-5    |  2.0237748993596203E-4 |  1.9144628539687834E-4 |\n",
    "|6.39413482566738E-4    | -0.0017282264999178775 | 6.103055479356823E-4   | 0.0013894023675135134  | 0.0013225187897918     | \n",
    "|4.341531175635357E-4   | -8.662945828302467E-5  | -8.104437563094699E-5  | -2.868548508403671E-4  | -1.5410753948440952E-4 | \n",
    "|1.2793545974311063E-4  | -2.669030792064854E-4  | -4.542139588767831E-5  | -1.5050166389473448E-5 | 1.482989608551199E-4   | \n",
    "|-1.3318080382014312E-4 | 1.1072824391853996E-4  |  -1.9355836538308612E-4|  -6.024451474837771E-4 |  -8.80023996669587E-5  |  \n",
    "|-1.6019075219364268E-4 | 1.4722159225685432E-4  | -1.447730844907519E-4  | 1.2609715851321523E-4  | -1.4163951937157604E-4 | \n",
    "|0.0016528466656541931  | 0.0035804852322326376  | -6.838548411447098E-4  | 0.0035569154771340986  | -0.0023997639513194087 | \n",
    "|1.2302891607538023E-4  | -7.927009760265055E-5  | -2.7274920466130874E-5 | 2.5829014007941792E-5  | -4.098440458721691E-4  | \n",
    "|-2.2368783781552486E-4 | -1.0782268438764112E-4 | 8.378821144788038E-4   | 4.570956580783604E-4   | 9.002105422787173E-5   | \n",
    "|9.571876585319955E-6   | -4.275307568030417E-6  | 4.965107911529186E-5   | 2.1124739979853439E-4  | 5.844884374809703E-4   | \n",
    "|9.442573083784481E-4   | 0.002117575544562773   | -6.706476228225057E-4  | 4.352391815785365E-4   | -0.0012363547442328705 | \n",
    "|3.177544612221666E-4   | 6.912435508520543E-4   | 8.909253454313397E-5   | 4.2553796555780036E-4  | -2.831716979010427E-4  | \n",
    "|1.4389865113810318E-4  | -2.0509631968574008E-4 | 2.061139076887754E-4   | -1.4247221512728379E-4 | 7.443585439624015E-7   | \n",
    "\n",
    "  \n",
    ">>>### TOP 5 PCs and CORRESPONING EIGENVALUES for computePrincipalComponentsAndExplainedVariance :\n",
    "\n",
    "|                                         CORRESPONDING EIGENVALUES                                         | \n",
    "|-----------------------------------------------------------------------------------------------------------|\n",
    "|0.006322919644804959, 0.004681388617987523, 0.004415994397011628, 0.00434764726056331, 0.004213446608677534|\n",
    "\n",
    "|          PC1         |          PC2           |           PC3          |           PC4          |           PC5          |\n",
    "|----------------------|------------------------|------------------------|------------------------|------------------------|\n",
    "|-4.159356306628059E-5 |  -3.3757028856845617E-4|  -1.0847396240127433E-8|  2.7931345380315114E-8 |  -9.687777506801106E-9 |                \n",
    "|3.4978234275212516E-4 |  -2.719215399753505E-4 |  0.001188019244312965  |  -0.0020622969027469947|  0.005329735572645177  |            \n",
    "|-6.394134825657982E-4 |  -0.00172822649991837  |  -0.0014779407893779242|  0.005010539740057661  |  -0.0013985092915906362|            \n",
    "|-4.3415311756334063E-4|  -8.662945828341595E-5 |  -0.0017395453654010773|  -2.7127822224284914E-4|  -0.0016231380975585965|  \n",
    "|-1.2793545974312814E-4|  -2.6690307920632095E-4|  9.255867371161345E-5  |  0.0010649861712624198 |  -9.772336682288205E-4 |  \n",
    "|1.3318080382013903E-4 |  1.107282439185564E-4  |  0.008618749644251874  |  0.0020765833121956425 |  -0.0066302642745165   |  \n",
    "|1.6019075219364455E-4 |  1.472215922568576E-4  |  0.0034597708712838147 |  0.001289230248358063  |  -7.8416440627532E-4   |  \n",
    "|-0.0016528466656541925|  0.003580485232232626  |  0.0027977065260352983 |  0.005189676057327596  |  -0.003984810267961188 |  \n",
    "|-1.2302891607537868E-4|  -7.927009760264959E-5 |  -4.779586038611031E-4 |  0.0012768923578476892 |  -9.350614049776897E-4 |  \n",
    "|2.2368783781552017E-4 |  -1.0782268438764123E-4|  0.0029394090103613186 |  0.0027291408266569613 |  -3.374489973738392E-4 |  \n",
    "|-9.571876585316822E-6 |  -4.275307568027443E-6 |  -9.403191238563428E-4 |  -2.400522294157571E-4 |  -8.246868811776369E-4 |  \n",
    "|-9.442573083784489E-4 |  0.002117575544562767  |  -0.013663746292546857 |  -0.0016208065113978327|  0.004405990071599086  |  \n",
    "|-3.1775446122216913E-4|  6.912435508520492E-4  |  -0.0024287343261939283|  -0.010833902126949931 |  0.002897437656730946  |  \n",
    "|-1.438986511381068E-4 |  -2.050963196857437E-4 |  3.7392135127771335E-5 |  9.763668468852213E-4  |  -0.0021529129175106372|            \n",
    "\n",
    ">### 2.2 NYTIMES dataset:\n",
    "\n",
    "> The top 5 principal components and the corresponding eigenvalues on the Nytimes dataset were computed using computeSVD. \n",
    "\n",
    "\n",
    ">>## Results:\n",
    "\n",
    "|                     METHOD                        | EXECUTION TIME | NUMBER OF CORES|        OUTPUT FILE           |\n",
    "|---------------------------------------------------|----------------|-----------------------------------------------|\n",
    "|                  computeSVD                       |    5604.18     |        7       |acp16al_Coursework4.sh.o389554| \n",
    "       \n",
    "             \n",
    "\n",
    ">>>### TOP 5 PCs and CORRESPONING EIGENVALUES for computeSVD :\n",
    "\n",
    "|                                         CORRESPONDING EIGENVALUES                                         | \n",
    "|-----------------------------------------------------------------------------------------------------------|\n",
    "| 4128028.139692928, 3124782.8450389816, 2994492.5694080144, 2487382.662142984, 2226039.114806705           |\n",
    "\n",
    "|         PC1          |          PC2           |            PC3         |            PC4         |             PC5         |\n",
    "|----------------------|------------------------|------------------------|------------------------|-------------------------|\n",
    "|-2.2235045541695042E-5|  1.4786453447158338E-5 |  1.3625186438847961E-5 |  4.1881746013000575E-5 |  -2.175283493334278E-5  | \n",
    "|-7.038440789413052E-6 |  8.378435775374976E-6  |  -1.232034939175615E-6 |  2.3689425749706696E-5 |  -2.1529484375630673E-7 | \n",
    "|4.5239770069534525E-6 |  2.0469728171160254E-5 |  -1.9794827584984223E-5|  1.0333128108542262E-5 |  -1.3345810648235769E-5 | \n",
    "|6.000298495324464E-5  |  -5.837620391285558E-5 |  -2.0276840565713048E-4|  1.1551464953560142E-4 |  -5.996285485368897E-5  | \n",
    "|-6.5909124460874565E-6|  4.758920041776419E-6  |  -2.140243391029512E-6 |  8.363126497376831E-6  |  -9.703700207350121E-6  | \n",
    "|-2.792542747440713E-5 |  -3.909813070954295E-5 |  6.7372802052902E-5    |  -3.2741112397468005E-5|  1.582174599513575E-5   | \n",
    "|1.856993591379314E-5  |  2.3130122941916224E-5 |  3.530865923252643E-5  |  1.2410843996700066E-4 |  -7.0539686253177125E-6 | \n",
    "|0.001780308006958957  |  -3.945735151946203E-4 |  -0.0015533223079234621|  4.466983984613343E-4  |  -4.653557170710789E-4  | \n",
    "|0.002435060674252606  |  -9.502848856985154E-4 |  -0.0023619986095714486|  0.0028420102218789036 |  -0.0024770154366808967 | \n",
    "|9.105145508185111E-4  |  -1.426934682655799E-4 |  -7.983317655234302E-4 |  1.7034679371038283E-4 |  -1.3075229635504059E-5 | \n",
    "|1.7982523873740145E-4 |  -1.5505464324028258E-4|  -1.871560540818478E-4 |  2.227107547611311E-4  |  -1.6278854179772097E-4 | \n",
    "|-1.791289502678731E-5 |  -2.2238164665387136E-5|  4.1325426811010534E-5 |  -1.856925951348664E-5 |  4.760134121689068E-6   | \n",
    "|1.8168032101950526E-5 |  6.03029062294834E-6   | -3.1617922900678886E-6 | 2.530321577860571E-5   | -2.507995183367641E-5   |\n",
    "|-4.391916472266485E-6 |  -2.2264885821766398E-5|  -1.5907991865909564E-5|  5.7985059124658424E-5 |  -3.505882541933396E-5  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. INTERESTING THINGS:\n",
    "\n",
    "> .computePrincipalComponentsAndExplainedVariance and SVD produce approximately the same results. \n",
    "\n",
    "> .computePrincipalComponentsAndExplainedVariance and SVD might produce the same results, but the results are not exactly identical (minor differences). In theory, the results should be exactly the same, but in practice these different solvers might have different precision limitations which cause the results to be slightly different.\n",
    "\n",
    "> .computePrincipalComponents could not be applied to nytimes dataset because of column size. The RowMatrix supports columns of number 65535 or less, and the nytimes dataset has 102660 columns. \n",
    "\n",
    "> .SVD executes faster than computePrincipalComponentsAndExplainedVariance.\n",
    "\n",
    "> .SVD scales better than computePrincipalComponentsAndExplainedVariance.\n",
    "\n",
    "> .computePrincipalComponentsAndExplainedVariance automatically performs data centering. When using SVD, in order to center data StandardScaler is required. \n",
    "\n",
    "> .computePrincipalComponentsAndExplainedVariance returns normalised eigenvalues. On the other hand, the computeSVD does not normalise eigenvales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.02402206085516662,-0.015553832586614353,0.0034926765011519577,0.21162740035016786,0.2731892121841241,-0.10550549210662875,-0.2814382206154455,-0.9933645150255717,0.39399381914863274,-0.032543294433910935,0.3749756002725115,-0.2967211407278946,-0.11460283856510294,0.1908689599715838,0.030460993332628184,-0.021860767880625156,0.21450631586038363,0.22691630667522128,-0.04506803239249403,-0.09286352597605775,0.43991300392668364,0.5328993246816548,-0.22142411342059004,0.514128839956174,-0.5910159654688021,-0.24185999790290427,-0.09538983755507065,0.24643625274694242,0.16373528484668848,-0.05005487439900302,-0.013063634268582923,-0.03858799464477046]\n",
      "Intercept: -364.4625984352555\n",
      "+------------------+------------+--------------------+\n",
      "|        prediction|Claim_Amount|            features|\n",
      "+------------------+------------+--------------------+\n",
      "|0.7245905036029399|         0.0|(32,[0,1,2,4,16,1...|\n",
      "| 0.951506810278147|         0.0|(32,[0,1,2,4,16,1...|\n",
      "|0.7832733999266566|         0.0|(32,[0,1,2,4,16,1...|\n",
      "|0.9977797157870327|         0.0|(32,[0,1,2,4,16,1...|\n",
      "|2.0499384469326856|         0.0|(32,[0,1,2,4,16,1...|\n",
      "+------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 34.662459140202294\n",
      "Time: 50.121438761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.regression.GeneralizedLinearRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StringIndexer\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions.{concat, lit}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mlinear_R\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Exercise 1.1: Linear Regression\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "object linear_R {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"linear_R\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        //Read file:\n",
    "        val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"true\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .option(\"nullValue\", \"?\")  // Use \"?\" as null character\n",
    "        .load(\"files/train_set__.csv\")\n",
    "        .na.drop()\n",
    "        .toDF\n",
    "         \n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime        \n",
    "\n",
    "        \n",
    "        //PREPROCESSING:\n",
    "        val DtoDouble = udf[Double, Double]( _.toDouble)\n",
    "              \n",
    "        val dfnew = df\n",
    "        .withColumn(\"Vehicle\", DtoDouble(df(\"Vehicle\")))\n",
    "        .withColumn(\"Calendar_Year\", DtoDouble(df(\"Calendar_Year\")))\n",
    "        .withColumn(\"Model_Year\", DtoDouble(df(\"Model_Year\")))\n",
    "        .withColumn(\"OrdCat\", DtoDouble(df(\"OrdCat\")))\n",
    "        .withColumn(\"Var1\", DtoDouble(df(\"Var1\")))\n",
    "        .withColumn(\"Var2\", DtoDouble(df(\"Var2\")))\n",
    "        .withColumn(\"Var3\", DtoDouble(df(\"Var3\")))\n",
    "        .withColumn(\"Var4\", DtoDouble(df(\"Var4\")))\n",
    "        .withColumn(\"Var5\", DtoDouble(df(\"Var5\")))\n",
    "        .withColumn(\"Var6\", DtoDouble(df(\"Var6\")))\n",
    "        .withColumn(\"Var7\", DtoDouble(df(\"Var7\")))\n",
    "        .withColumn(\"Var8\", DtoDouble(df(\"Var8\")))\n",
    "        .withColumn(\"NVVar1\", DtoDouble(df(\"NVVar1\")))\n",
    "        .withColumn(\"NVVar2\", DtoDouble(df(\"NVVar2\")))\n",
    "        .withColumn(\"NVVar3\", DtoDouble(df(\"NVVar3\")))\n",
    "        .withColumn(\"NVVar4\", DtoDouble(df(\"NVVar4\")))\n",
    "        .withColumn(\"Claim_Amount\", DtoDouble(df(\"Claim_Amount\")))\n",
    "        .select(\"Vehicle\", \"Calendar_Year\", \"Model_Year\", \"Blind_Make\", \"Blind_Model\", \"Blind_Submodel\", \"Cat1\", \"Cat2\",\"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\",\n",
    "        \"Cat8\", \"Cat9\", \"Cat10\", \"Cat11\", \"Cat12\", \"OrdCat\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \"Var6\", \"Var7\", \"Var8\", \"NVCat\", \"NVVar1\", \"NVVar2\", \"NVVar3\", \"NVVar4\",\n",
    "                \"Claim_Amount\")\n",
    "        \n",
    "        //VectorAssembler for non-strings.\n",
    "        val assembler_Doubles = new VectorAssembler()\n",
    "          .setInputCols(Array(\"Vehicle\", \"Calendar_Year\", \"Model_Year\", \"OrdCat\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \"Var6\", \"Var7\", \"Var8\", \"NVVar1\", \"NVVar2\", \"NVVar3\",\n",
    "                              \"NVVar4\"))\n",
    "          .setOutputCol(\"features_D\")\n",
    "        val dfnewD = assembler_Doubles.transform(dfnew)\n",
    "\n",
    "        //Use StringIndexer for all string columns:\n",
    "        val indexer1 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Make\")\n",
    "          .setOutputCol(\"Blind_Make_\")\n",
    "          .fit(dfnewD)\n",
    "          .transform(dfnewD)\n",
    "\n",
    "        val indexer2 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Model\")\n",
    "          .setOutputCol(\"Blind_Model_\")\n",
    "          .fit(indexer1)\n",
    "          .transform(indexer1)\n",
    "\n",
    "        val indexer3 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Submodel\")\n",
    "          .setOutputCol(\"Blind_Submodel_\")\n",
    "          .fit(indexer2)\n",
    "          .transform(indexer2)\n",
    "\n",
    "        val indexer4 = new StringIndexer()\n",
    "          .setInputCol(\"Cat1\")\n",
    "          .setOutputCol(\"Cat1_\")\n",
    "          .fit(indexer3)\n",
    "          .transform(indexer3)\n",
    "\n",
    "        val indexer5 = new StringIndexer()\n",
    "          .setInputCol(\"Cat2\")\n",
    "          .setOutputCol(\"Cat2_\")\n",
    "          .fit(indexer4)\n",
    "          .transform(indexer4)\n",
    "\n",
    "        val indexer6 = new StringIndexer()\n",
    "          .setInputCol(\"Cat3\")\n",
    "          .setOutputCol(\"Cat3_\")\n",
    "          .fit(indexer5)\n",
    "          .transform(indexer5)\n",
    "\n",
    "        val indexer7 = new StringIndexer()\n",
    "          .setInputCol(\"Cat4\")\n",
    "          .setOutputCol(\"Cat4_\")\n",
    "          .fit(indexer6)\n",
    "          .transform(indexer6)\n",
    "\n",
    "        val indexer8 = new StringIndexer()\n",
    "          .setInputCol(\"Cat5\")\n",
    "          .setOutputCol(\"Cat5_\")\n",
    "          .fit(indexer7)\n",
    "          .transform(indexer7)\n",
    "\n",
    "        val indexer9 = new StringIndexer()\n",
    "          .setInputCol(\"Cat6\")\n",
    "          .setOutputCol(\"Cat6_\") \n",
    "          .fit(indexer8)\n",
    "          .transform(indexer8)\n",
    "\n",
    "        val indexer10 = new StringIndexer()\n",
    "          .setInputCol(\"Cat7\")\n",
    "          .setOutputCol(\"Cat7_\")\n",
    "          .fit(indexer9)\n",
    "          .transform(indexer9)\n",
    "\n",
    "        val indexer11 = new StringIndexer()\n",
    "          .setInputCol(\"Cat8\")\n",
    "          .setOutputCol(\"Cat8_\")\n",
    "          .fit(indexer10)\n",
    "          .transform(indexer10)\n",
    "\n",
    "        val indexer12 = new StringIndexer()\n",
    "          .setInputCol(\"Cat9\")\n",
    "          .setOutputCol(\"Cat9_\")\n",
    "          .fit(indexer11)\n",
    "          .transform(indexer11)\n",
    "\n",
    "        val indexer13 = new StringIndexer()\n",
    "          .setInputCol(\"Cat10\")\n",
    "          .setOutputCol(\"Cat10_\")\n",
    "          .fit(indexer12)\n",
    "          .transform(indexer12)\n",
    "\n",
    "        val indexer14 = new StringIndexer()\n",
    "          .setInputCol(\"Cat11\")\n",
    "          .setOutputCol(\"Cat11_\")\n",
    "          .fit(indexer13)\n",
    "          .transform(indexer13)   \n",
    "\n",
    "        val indexer15 = new StringIndexer()\n",
    "          .setInputCol(\"Cat12\")\n",
    "          .setOutputCol(\"Cat12_\")\n",
    "          .fit(indexer14)\n",
    "          .transform(indexer14)\n",
    "\n",
    "        val indexer16 = new StringIndexer()\n",
    "          .setInputCol(\"NVCat\")\n",
    "          .setOutputCol(\"NVCat_\")\n",
    "          .fit(indexer15)\n",
    "          .transform(indexer15)\n",
    "\n",
    "        //VectorAssembler for all strings.\n",
    "        val assembler_Strings = new VectorAssembler()\n",
    "          .setInputCols(Array(\"Blind_Make_\", \"Blind_Model_\", \"Blind_Submodel_\", \"Cat1_\", \"Cat2_\",\"Cat3_\", \"Cat4_\", \"Cat5_\", \"Cat6_\", \"Cat7_\",\"Cat8_\", \"Cat9_\", \"Cat10_\",\n",
    "                              \"Cat11_\", \"Cat12_\", \"NVCat_\"))\n",
    "          .setOutputCol(\"features_S\")\n",
    "        val dfnewS = assembler_Strings.transform(indexer16)\n",
    "    \n",
    "        //Final VectorAssembler to combine non-strings and strings features:\n",
    "        val assembler = new VectorAssembler()\n",
    "            .setInputCols(Array(\"features_S\", \"features_D\"))\n",
    "            .setOutputCol(\"features\")\n",
    "        val dfNew = assembler.transform(dfnewS)\n",
    "        \n",
    "        //Split dataset for training and testing:\n",
    "        val Array(trainingData, testData) = dfNew.select(\"Claim_Amount\", \"features\").randomSplit(Array(0.7, 0.3))\n",
    "        \n",
    "        //Linear Regression Model:\n",
    "        val glr = new GeneralizedLinearRegression()\n",
    "          .setLabelCol(\"Claim_Amount\")\n",
    "          .setFeaturesCol(\"features\")\n",
    "          .setFamily(\"gaussian\")\n",
    "          .setLink(\"identity\")\n",
    "          .setMaxIter(10)\n",
    "          .setRegParam(0.9)\n",
    "          .setSolver(\"l-bfgs\")\n",
    "\n",
    "        // Fit the model to the training data\n",
    "        val model = glr.fit(trainingData)\n",
    "\n",
    "        // Print the coefficients and intercept for generalized linear regression model\n",
    "        println(s\"Coefficients: ${model.coefficients}\")\n",
    "        println(s\"Intercept: ${model.intercept}\")\n",
    "\n",
    "        // Make predictions.\n",
    "        val predictions = model.transform(testData)\n",
    "\n",
    "        // Select example rows to display.\n",
    "        predictions.select(\"prediction\", \"Claim_Amount\", \"features\").show(5)\n",
    "\n",
    "        // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new RegressionEvaluator()\n",
    "          .setLabelCol(\"Claim_Amount\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\")\n",
    "        val rmse = evaluator.evaluate(predictions)\n",
    "        println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n",
    "        \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "\n",
    "\n",
    "}\n",
    "}\n",
    "linear_R.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [3.2418029324792426E30,1.0249124713285857E31,-3.954737247653854E30,-1.4287483329988732E30,-4.856018548241099E29,8.322725016362226E30,-4.8362142593765465E28,7.427813196125703E30,-3.5160042241101284E31,1.201613323354362E31,-5.377525737838193E30,3.937135862100095E30,7.781914075439464E30,-4.796687312677471E31,1.0349864698241355E31,-2.6111053881371117E31,-2.588379425787721E31,6.881257276611046E30,-1.5859102653937417E30,8.60426257423508E30,5.076048336814862E30,-1.6327131083156813E31,1.9326021480730597E31,-1.4454949627341318E31,1.1216254200811382E31,1.345128984924997E31,1.2129282396622552E31,-3.352625919955259E31,-3.712200785382125E31,1.8792850916247868E31,7.935384448749434E30,3.5925178633715283E31]\n",
      "Intercept: -1.124114090145658E34\n",
      "+----------+-------------+--------------------+\n",
      "|prediction|Claim_Amount_|            features|\n",
      "+----------+-------------+--------------------+\n",
      "|   1.0E-16|         0.01|(32,[0,1,2,4,16,1...|\n",
      "|   1.0E-16|         0.01|(32,[0,1,2,4,16,1...|\n",
      "|   1.0E-16|         0.01|(32,[0,1,2,4,16,1...|\n",
      "|   1.0E-16|         0.01|(32,[0,1,2,4,16,1...|\n",
      "|   1.0E-16|         0.01|(32,[0,1,2,4,16,1...|\n",
      "+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 15.24990365659914\n",
      "Time: 69.094169307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.regression.GeneralizedLinearRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StringIndexer\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions.{concat, lit}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mgamma_R\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Exercise 1.2: Gamma Regression\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "object gamma_R {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"linear_R\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        //Read file:\n",
    "        val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"true\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .option(\"nullValue\", \"?\")  // Use \"?\" as null character\n",
    "        .load(\"files/train_set__.csv\")\n",
    "        .na.drop()\n",
    "        .toDF\n",
    "     \n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime \n",
    "    \n",
    "        val DtoDouble = udf[Double, Double]( _.toDouble)\n",
    "\n",
    "        \n",
    "        //PREPROCESSING:\n",
    "        val dfnew = df\n",
    "        .withColumn(\"Vehicle\", DtoDouble(df(\"Vehicle\")))\n",
    "        .withColumn(\"Calendar_Year\", DtoDouble(df(\"Calendar_Year\")))\n",
    "        .withColumn(\"Model_Year\", DtoDouble(df(\"Model_Year\")))\n",
    "        .withColumn(\"OrdCat\", DtoDouble(df(\"OrdCat\")))\n",
    "        .withColumn(\"Var1\", DtoDouble(df(\"Var1\")))\n",
    "        .withColumn(\"Var2\", DtoDouble(df(\"Var2\")))\n",
    "        .withColumn(\"Var3\", DtoDouble(df(\"Var3\")))\n",
    "        .withColumn(\"Var4\", DtoDouble(df(\"Var4\")))\n",
    "        .withColumn(\"Var5\", DtoDouble(df(\"Var5\")))\n",
    "        .withColumn(\"Var6\", DtoDouble(df(\"Var6\")))\n",
    "        .withColumn(\"Var7\", DtoDouble(df(\"Var7\")))\n",
    "        .withColumn(\"Var8\", DtoDouble(df(\"Var8\")))\n",
    "        .withColumn(\"NVVar1\", DtoDouble(df(\"NVVar1\")))\n",
    "        .withColumn(\"NVVar2\", DtoDouble(df(\"NVVar2\")))\n",
    "        .withColumn(\"NVVar3\", DtoDouble(df(\"NVVar3\")))\n",
    "        .withColumn(\"NVVar4\", DtoDouble(df(\"NVVar4\")))\n",
    "        .withColumn(\"Claim_Amount\", DtoDouble(df(\"Claim_Amount\")))\n",
    "        .select(\"Vehicle\", \"Calendar_Year\", \"Model_Year\", \"Blind_Make\", \"Blind_Model\", \"Blind_Submodel\", \"Cat1\", \"Cat2\",\"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\",\n",
    "        \"Cat8\", \"Cat9\", \"Cat10\", \"Cat11\", \"Cat12\", \"OrdCat\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \"Var6\", \"Var7\", \"Var8\", \"NVCat\", \"NVVar1\", \"NVVar2\", \"NVVar3\", \"NVVar4\",\n",
    "                \"Claim_Amount\")\n",
    "\n",
    "        //VectorAssembler for non-strings.\n",
    "        val assembler_Doubles = new VectorAssembler()\n",
    "          .setInputCols(Array(\"Vehicle\", \"Calendar_Year\", \"Model_Year\", \"OrdCat\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \"Var6\", \"Var7\", \"Var8\", \"NVVar1\", \"NVVar2\", \n",
    "                              \"NVVar3\", \"NVVar4\"))\n",
    "          .setOutputCol(\"features_D\")\n",
    "        val dfnewD = assembler_Doubles.transform(dfnew)\n",
    "        \n",
    "        \n",
    "        //Use StringIndexer for all string columns:\n",
    "        val indexer1 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Make\")\n",
    "          .setOutputCol(\"Blind_Make_\")\n",
    "          .fit(dfnewD)\n",
    "          .transform(dfnewD)\n",
    "\n",
    "        val indexer2 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Model\")\n",
    "          .setOutputCol(\"Blind_Model_\")\n",
    "          .fit(indexer1)\n",
    "          .transform(indexer1)\n",
    "\n",
    "        val indexer3 = new StringIndexer()\n",
    "          .setInputCol(\"Blind_Submodel\")\n",
    "          .setOutputCol(\"Blind_Submodel_\")\n",
    "          .fit(indexer2)\n",
    "          .transform(indexer2)\n",
    "\n",
    "        val indexer4 = new StringIndexer()\n",
    "          .setInputCol(\"Cat1\")\n",
    "          .setOutputCol(\"Cat1_\")\n",
    "          .fit(indexer3)\n",
    "          .transform(indexer3)\n",
    "\n",
    "        val indexer5 = new StringIndexer()\n",
    "          .setInputCol(\"Cat2\")\n",
    "          .setOutputCol(\"Cat2_\")\n",
    "          .fit(indexer4)\n",
    "          .transform(indexer4)\n",
    "\n",
    "        val indexer6 = new StringIndexer()\n",
    "          .setInputCol(\"Cat3\")\n",
    "          .setOutputCol(\"Cat3_\")\n",
    "          .fit(indexer5)\n",
    "          .transform(indexer5)\n",
    "\n",
    "        val indexer7 = new StringIndexer()\n",
    "         .setInputCol(\"Cat4\")\n",
    "         .setOutputCol(\"Cat4_\")\n",
    "         .fit(indexer6)\n",
    "         .transform(indexer6)\n",
    "\n",
    "        val indexer8 = new StringIndexer()\n",
    "          .setInputCol(\"Cat5\")\n",
    "          .setOutputCol(\"Cat5_\")\n",
    "          .fit(indexer7)\n",
    "          .transform(indexer7)\n",
    "\n",
    "        val indexer9 = new StringIndexer()\n",
    "          .setInputCol(\"Cat6\")\n",
    "          .setOutputCol(\"Cat6_\") \n",
    "          .fit(indexer8)\n",
    "          .transform(indexer8)\n",
    "\n",
    "        val indexer10 = new StringIndexer()\n",
    "          .setInputCol(\"Cat7\")\n",
    "          .setOutputCol(\"Cat7_\")\n",
    "          .fit(indexer9)\n",
    "          .transform(indexer9)\n",
    "\n",
    "        val indexer11 = new StringIndexer()\n",
    "          .setInputCol(\"Cat8\")\n",
    "          .setOutputCol(\"Cat8_\")\n",
    "          .fit(indexer10)\n",
    "          .transform(indexer10)\n",
    "\n",
    "        val indexer12 = new StringIndexer()\n",
    "          .setInputCol(\"Cat9\")\n",
    "          .setOutputCol(\"Cat9_\")\n",
    "          .fit(indexer11)\n",
    "          .transform(indexer11)\n",
    "\n",
    "        val indexer13 = new StringIndexer()\n",
    "          .setInputCol(\"Cat10\")\n",
    "          .setOutputCol(\"Cat10_\")\n",
    "          .fit(indexer12)\n",
    "          .transform(indexer12)\n",
    "\n",
    "        val indexer14 = new StringIndexer()\n",
    "          .setInputCol(\"Cat11\")\n",
    "          .setOutputCol(\"Cat11_\")\n",
    "          .fit(indexer13)\n",
    "          .transform(indexer13)   \n",
    "\n",
    "        val indexer15 = new StringIndexer()\n",
    "          .setInputCol(\"Cat12\")\n",
    "          .setOutputCol(\"Cat12_\")\n",
    "          .fit(indexer14)\n",
    "          .transform(indexer14)\n",
    "\n",
    "        val indexer16 = new StringIndexer()\n",
    "          .setInputCol(\"NVCat\")\n",
    "          .setOutputCol(\"NVCat_\")\n",
    "          .fit(indexer15)\n",
    "          .transform(indexer15)\n",
    "\n",
    "        \n",
    "        //VectorAssembler for all strings.\n",
    "        val assembler_Strings = new VectorAssembler()\n",
    "          .setInputCols(Array(\"Blind_Make_\", \"Blind_Model_\", \"Blind_Submodel_\", \"Cat1_\", \"Cat2_\",\"Cat3_\", \"Cat4_\", \"Cat5_\", \"Cat6_\", \"Cat7_\",\"Cat8_\", \"Cat9_\", \"Cat10_\", \n",
    "                              \"Cat11_\", \"Cat12_\", \"NVCat_\"))\n",
    "          .setOutputCol(\"features_S\")\n",
    "        val dfnewS = assembler_Strings.transform(indexer16)\n",
    "\n",
    "        //Final VectorAssembler to combine non-strings and strings features:\n",
    "        val assembler = new VectorAssembler()\n",
    "            .setInputCols(Array(\"features_S\", \"features_D\"))\n",
    "            .setOutputCol(\"features\")\n",
    "        val dfNew = assembler.transform(dfnewS)\n",
    "\n",
    "        \n",
    "        //Change 0.0 values to 0.01:\n",
    "        val dfNEW = dfNew.withColumn(\"Claim_Amount\", when(col(\"Claim_Amount\") === 0.0, 0.01).otherwise(col(\"Claim_Amount\")))\n",
    "                         .withColumnRenamed(\"Claim_Amount\", \"Claim_Amount_\").select(\"Claim_Amount_\", \"features\")\n",
    "        \n",
    "        //Split dataset for training and testing:\n",
    "        val Array(trainingData, testData) = dfNEW.select(\"Claim_Amount_\",\"features\").randomSplit(Array(0.7, 0.3))\n",
    "        \n",
    "        //Gamma Regression Model:\n",
    "        val glr = new GeneralizedLinearRegression()\n",
    "          .setLabelCol(\"Claim_Amount_\")\n",
    "          .setFeaturesCol(\"features\")\n",
    "          .setFamily(\"gamma\")\n",
    "          .setLink(\"inverse\")\n",
    "          .setMaxIter(10)\n",
    "          .setRegParam(0.9)\n",
    "          .setSolver(\"l-bfgs\")\n",
    "\n",
    "        // Fit the model to the training data\n",
    "        val model = glr.fit(trainingData)\n",
    "\n",
    "        // Print the coefficients and intercept for generalized linear regression model\n",
    "        println(s\"Coefficients: ${model.coefficients}\")\n",
    "        println(s\"Intercept: ${model.intercept}\")\n",
    "\n",
    "        // Make predictions.\n",
    "        val predictions = model.transform(testData)\n",
    "\n",
    "        // Select example rows to display.\n",
    "        predictions.select(\"prediction\", \"Claim_Amount_\", \"features\").show(5)\n",
    "\n",
    "        // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new RegressionEvaluator()\n",
    "          .setLabelCol(\"Claim_Amount_\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\")\n",
    "        val rmse = evaluator.evaluate(predictions)\n",
    "        println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n",
    "        \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "\n",
    "}\n",
    "}\n",
    "gamma_R.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 394.0 failed 1 times, most recent failure: Lost task 0.0 in stage 394.0 (TID 393, localhost): java.lang.IllegalArgumentException: Do not support vector type class org.apache.spark.mllib.linalg.SparseVector",
      "\tat org.apache.spark.mllib.feature.StandardScalerModel.transform(StandardScaler.scala:160)",
      "\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:393)",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "\tat java.lang.Thread.run(Thread.java:745)",
      "",
      "Driver stacktrace: (Job aborted due to stage failure: Task 0 in stage 394.0 failed 1 times, most recent failure: Lost task 0.0 in stage 394.0 (TID 393, localhost): java.lang.IllegalArgumentException: Do not support vector type class org.apache.spark.mllib.linalg.SparseVector",
      "\tat org.apache.spark.mllib.feature.StandardScalerModel.transform(StandardScaler.scala:160)",
      "\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "\tat org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:393)",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "\tat java.lang.Thread.run(Thread.java:745)",
      "",
      "Driver stacktrace:)",
      "  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)",
      "  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)",
      "  scala.Option.foreach(Option.scala:257)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)",
      "  org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1324)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:358)",
      "  org.apache.spark.rdd.RDD.take(RDD.scala:1298)",
      "  org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1338)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:358)",
      "  org.apache.spark.rdd.RDD.first(RDD.scala:1337)",
      "  org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)",
      "  org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:217)",
      "  org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:193)",
      "  cmd13$$user$E_2_1$.main(Main.scala:58)",
      "  cmd13$$user$$anonfun$2.apply$mcV$sp(Main.scala:70)",
      "java.lang.IllegalArgumentException: Do not support vector type class org.apache.spark.mllib.linalg.SparseVector (Do not support vector type class org.apache.spark.mllib.linalg.SparseVector)",
      "  org.apache.spark.mllib.feature.StandardScalerModel.transform(StandardScaler.scala:160)",
      "  org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "  org.apache.spark.mllib.feature.VectorTransformer$$anonfun$transform$1.apply(VectorTransformer.scala:52)",
      "  scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",
      "  scala.collection.Iterator$$anon$10.next(Iterator.scala:393)",
      "  scala.collection.Iterator$class.foreach(Iterator.scala:893)",
      "  scala.collection.AbstractIterator.foreach(Iterator.scala:1336)",
      "  scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)",
      "  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)",
      "  scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)",
      "  scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)",
      "  scala.collection.AbstractIterator.to(Iterator.scala:1336)",
      "  scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)",
      "  scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)",
      "  scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)",
      "  scala.collection.AbstractIterator.toArray(Iterator.scala:1336)",
      "  org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "  org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1324)",
      "  org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "  org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1916)",
      "  org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)",
      "  org.apache.spark.scheduler.Task.run(Task.scala:86)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
      "  java.lang.Thread.run(Thread.java:745)"
     ]
    }
   ],
   "source": [
    "// Exercise 2.1: nips dataset\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.linalg.SingularValueDecomposition\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "\n",
    "object E_2_1 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        val timing = System.nanoTime        \n",
    "        \n",
    "        val number_of_documents = 12419 // NYTIMES: 102660, NIPS: 12419\n",
    "        val file = sc.textFile(\"files/docword.nips.txt.gz\").flatMap(element => element.split(\" \") match {\n",
    "            case Array(id, w, c) => Some((id.toInt, (w.toInt-1, c.toDouble)))\n",
    "            case _ => None\n",
    "        }).groupByKey().mapValues(final_row => Vectors.sparse(number_of_documents, final_row.toSeq))\n",
    "        \n",
    "        //METHOD: computePrincipalComponentsAndExplainedVariance:\n",
    "        /*\n",
    "        val mat: RowMatrix = new RowMatrix(file.values)\n",
    "        val (pc, eigen) = mat.computePrincipalComponentsAndExplainedVariance(5)\n",
    "        println(eigen)\n",
    "        println(pc.toString(15, Int.MaxValue))\n",
    "        */\n",
    "        \n",
    "        \n",
    "        //METHOD: computeSVD\n",
    "        \n",
    "        val scaler = new StandardScaler(withMean = true, withStd = false).fit(file.values).transform(file.values)\n",
    "        val mat2: RowMatrix = new RowMatrix(scaler)\n",
    "        val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat2.computeSVD(5, computeU = true)\n",
    "        val singular: Vector = svd.s\n",
    "        val pca: Matrix = svd.V\n",
    "        val normalise = Vectors.dense(singular.toArray.map(a => a*a))\n",
    "        println(normalise)\n",
    "        println(pca.toString(15,Int.MaxValue))\n",
    "    \n",
    "        \n",
    "    \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "E_2_1.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Exercise 2.2: nytimes dataset\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.linalg.SingularValueDecomposition\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "\n",
    "object E_2_2 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        val timing = System.nanoTime        \n",
    "        \n",
    "        val number_of_documents = 102660 // NYTIMES: 102660, NIPS: 12419\n",
    "        val file = sc.textFile(\"files/docword.nytimes.txt.gz\").flatMap(element => element.split(\" \") match {\n",
    "            case Array(id, w, c) => Some((id.toInt, (w.toInt-1, c.toDouble)))\n",
    "            case _ => None\n",
    "        }).groupByKey().mapValues(final_row => Vectors.sparse(number_of_documents, final_row.toSeq))\n",
    "        \n",
    "        \n",
    "        /*\n",
    "        //METHOD: computePrincipalComponentsAndExplainedVariance:\n",
    "        val mat: RowMatrix = new RowMatrix(file.values)\n",
    "        val (pc, eigen) = mat.computePrincipalComponentsAndExplainedVariance(5)\n",
    "        println(pc)\n",
    "        println(eigen)\n",
    "        */\n",
    "        \n",
    "        //METHOD: computeSVD\n",
    "        val scaler = new StandardScaler(withMean = true, withStd = false).fit(file.values).transform(file.values)\n",
    "        val mat2: RowMatrix = new RowMatrix(scaler)\n",
    "        val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat2.computeSVD(5, computeU = true)\n",
    "        val singular: Vector = svd.s\n",
    "        val pca: Matrix = svd.V\n",
    "        val normalise = Vectors.dense(singular.toArray.map(a => a*a))\n",
    "        println(normalise)\n",
    "        println(pca.toString(15,Int.MaxValue))\n",
    "    \n",
    "        \n",
    "    \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "E_2_2.main(Array())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
