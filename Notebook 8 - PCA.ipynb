{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principal component analysis (PCA)</h1>\n",
    "\n",
    "[Principal component analysis](http://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\n",
    "\n",
    "Usual initializations and the relevant imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Matrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.RowMatrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>spark.mllib</tt> supports PCA for tall-and-skinny matrices stored in row-oriented format and any Vectors. We demonstrate how to compute principal components on a [<tt>RowMatrix</tt>](http://spark.apache.org/docs/latest/mllib-data-types.html#rowmatrix) and use them to project the vectors into a low-dimensional space in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,[1,3],[1.0,7.0])\n",
      "[2.0,0.0,3.0,4.0,5.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@78d55545\n",
       "\u001b[36mdata\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mVector\u001b[0m] = \u001b[33mArray\u001b[0m((5,[1,3],[1.0,7.0]), [2.0,0.0,3.0,4.0,5.0], [4.0,0.0,0.0,6.0,7.0])\n",
       "\u001b[36mdataRDD\u001b[0m: \u001b[32mRDD\u001b[0m[\u001b[32mVector\u001b[0m] = ParallelCollectionRDD[8] at parallelize at Main.scala:50\n",
       "\u001b[36mmat\u001b[0m: \u001b[32mRowMatrix\u001b[0m = org.apache.spark.mllib.linalg.distributed.RowMatrix@36f0611a\n",
       "\u001b[36mpc\u001b[0m: \u001b[32mMatrix\u001b[0m = -0.44859172075072673  -0.28423808214073987  0.08344545257592471   0.8364102009456849    \n",
       "0.13301985745398526   -0.05621155904253121  0.044239792581370035  0.17224337841622106   \n",
       "-0.1252315635978212   0.7636264774662965    -0.578071228563837    0.2554154886635869    \n",
       "0.21650756651919933   -0.5652958773533949   -0.7955405062786798   4.858121429822393E-5  \n",
       "-0.8476512931126826   -0.11560340501314653  -0.1550117891430013   -0.4533355491646027   \n",
       "\u001b[36mprojected\u001b[0m: \u001b[32mRowMatrix\u001b[0m = org.apache.spark.mllib.linalg.distributed.RowMatrix@75cb21a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"PCA\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n",
    "  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))\n",
    "\n",
    "val dataRDD = sparkSession.sparkContext.parallelize(data, 2)\n",
    "\n",
    "\n",
    "val mat: RowMatrix = new RowMatrix(dataRDD)\n",
    "\n",
    "// Compute the top 4 principal components.\n",
    "// Principal components are stored in a local dense matrix.\n",
    "val pc: Matrix = mat.computePrincipalComponents(4)\n",
    "\n",
    "// Project the rows to the linear space spanned by the top 4 principal components.\n",
    "val projected: RowMatrix = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In HPC experiments, we will use the [NIPS 2014](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words) paper along with the <b>bag of words</b> data mentioned within, but these datasets are too large for this notebook. We therefore create a small dataset from the documents:\n",
    "\n",
    "    D1: the cat sat on the mat\n",
    "    D2: the cat sat on the cat\n",
    "    D3: the cat sat\n",
    "    D4: the mat sat\n",
    "\n",
    "Numbering the words\n",
    "\n",
    "    0 the\n",
    "    1 cat\n",
    "    2 sat\n",
    "    3 on\n",
    "    4 the\n",
    "    5 mat\n",
    "    \n",
    "the documents can be represented using the following sparse vectors\n",
    "\n",
    "    Vectors.sparse(5, Seq((0, 2.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 2.0), (1, 2.0), (2, 1.0), (3, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 1.0), (1, 1.0), (2, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 1.0), (2, 1.0), (4, 1.0))))\n",
    "\n",
    "Equally, it could be represented by triples of <tt>document_id word_id freq</tt> as follows\n",
    "\n",
    "    1 0 2.0\n",
    "    1 1 1.0\n",
    "    1 2 1.0\n",
    "    1 3 1.0\n",
    "    1 4 1.0\n",
    "    2 0 2.0\n",
    "    2 1 2.0\n",
    "    2 2 1.0\n",
    "    2 3 1.0\n",
    "    3 0 1.0\n",
    "    3 1 1.0\n",
    "    3 2 1.0\n",
    "    4 0 1.0\n",
    "    4 2 1.0\n",
    "    4 4 1.0\n",
    "\n",
    "(This conversion will come in useful for processing the bag of words data.) We now generate the principal component vectors for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sc = sparkSession.sparkContext\n",
    "\n",
    "// The data\n",
    "\n",
    "val data = Array(\n",
    "    Vectors.sparse(5, Seq((0, 2.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 2.0), (1, 2.0), (2, 1.0), (3, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 1.0), (1, 1.0), (2, 1.0))),\n",
    "    Vectors.sparse(5, Seq((0, 1.0), (2, 1.0), (4, 1.0))))\n",
    "\n",
    "val dataRDD = sc.parallelize(data)\n",
    "\n",
    "val mat: RowMatrix = new RowMatrix(dataRDD)\n",
    "\n",
    "// Compute the top 4 principal components.\n",
    "// Principal components are stored in a local dense matrix.\n",
    "val pc: Matrix = mat.computePrincipalComponents(4)\n",
    "\n",
    "// Project the rows to the linear space spanned by the top 4 principal components.\n",
    "val projected: RowMatrix = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "<h3>Exercise 1</h3>\n",
    "\n",
    "Take a look at the <b>bagofwords</b> NIPS data. The format of this data is\n",
    "\n",
    "    Number of documents\n",
    "    Number of words in the vocabulary\n",
    "    Total number of words in the collection\n",
    "    docID wordID count\n",
    "    docID wordID count\n",
    "    ...\n",
    "    docID wordID count\n",
    "    \n",
    "Initially, we need to read this data in: the steps in this would be roughly:\n",
    "\n",
    "1) extract the number of documents, size of the vocabulary and strip off the first 3 lines\n",
    "2) combine the words per document\n",
    "3) create sparse vectors\n",
    "\n",
    "Create this as a standalone program. You can use <tt>.partitions.size</tt> to check the number of partitions your data is divided into and you should keep everything as parallel as possible. You will benefit from creating a very small example to test your work, and then checking that your work scales up to the <tt>NYTIMES</tt> bagofwords data.\n",
    "\n",
    "<h3>Exercise 2</h3>\n",
    "\n",
    "If you try to run the PCA program from the notebook on a large dataset, it is likely to run out of memory as it attempts to construct the covariance matrix locally on the driver (and then uses [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) to generate the principal components). For this exercise, you are to implement an alternative method of computing principal components. For this, you need to [center your matrix](https://en.wikipedia.org/wiki/Centering_matrix) (for each word vector, this involves subtracting the mean) and then use [SVD](https://spark.apache.org/docs/2.0.2/mllib-dimensionality-reduction.html#svd-example). Implementations of this exist and you can follow this link to a Matlab implementation of [disPCA](http://www.cs.princeton.edu/~yingyul/DistributedCoresetAndPCA.zip).\n",
    "\n",
    "You should be able to use the sparse vectors you generated in exercise 1, center these and then use SVD. A small scale example will allow you to check that your first principal components match those generated by <tt>computePrincipalComponents</tt>.\n",
    "\n",
    "Run PCA on the full NIPS data and the NYTIMES data, varying the number of principal components generated.\n",
    "\n",
    "<h3>Exercise 3</h3>\n",
    "\n",
    "Run $k$-means with the generated vectors for both datasets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
