{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# COM6012 - 2017: Coursework 3\n",
    "\n",
    "Deadline: 11:59PM on Thursday 23 March 2017\n",
    "\n",
    "Submission: via SageMatchCloud (We will collect from you automatically).\n",
    "\n",
    "\n",
    "In this coursework, you will apply Decision Trees for Classification, Decision Trees for Regression and Logistic Regression over the [HIGGS dataset](http://archive.ics.uci.edu/ml/datasets/HIGGS). For each algorithm:\n",
    "\n",
    "1. Use pipelines and cross-validation to find the best configuration of parameters and their performance. Use the same splits of training and test data when comparing performances between the algorithms (8 marks).\n",
    "2. Find which features are more relevant for classification or regression (4 marks). \n",
    "3. Provide training times in the cluster when using different cores (1 mark).\n",
    "\n",
    "Do not try to upload the dataset to SageMathCloud when returnig your work. It is 2.6Gb. \n",
    "\n",
    "\n",
    "\n",
    "# Solutions:\n",
    "\n",
    "> A text file 'README.txt\" is provided in the zip for the exercises, that explains which output and error files represent the number of cores used. Eg: 'acp16al_Coursework3.sh.o229510' === Output file for 10 cores, 'acp16al_Coursework3.sh.e229508' === Error file for 5 cores.\n",
    "\n",
    "\n",
    "# DECISION TREES CLASSIFICATION:\n",
    "\n",
    "# 1) & 2)\n",
    "\n",
    "The best configurations of parameters and their performance for different types of features (all, only low-level, only high-level) found from the cross validator are shown in the table:\n",
    "\n",
    "| Features |  MaxBins  | MaxDepth| Impurity | ACCURACY | TIME |\n",
    "|----------|-----------|---------|----------|----------|------|\n",
    "|   ALL    |    20     |   10    |  entropy |   0.688  |740.68|\n",
    "|   LOW    |    20     |   10    |  entropy |   0.610  |619.13|\n",
    "|   HIGH   |    20     |   10    |   gini   |   0.679  |517.00|\n",
    "\n",
    "As shown from the table the best accuracy is obtained when using all the features. Additionally, high-level features provide better accuracy than low-level features. Therefore, for classification high-level features are more relevant (see logistic regression results below that confirm this).\n",
    "\n",
    "# 3)\n",
    "\n",
    "Using ALL features, different number of cores were used:\n",
    "\n",
    "| Cores | TIME |\n",
    "|-------|------|\n",
    "|    1  |994.13|\n",
    "|    5  |734.25|\n",
    "|   10  |682.49|\n",
    "\n",
    "\n",
    "\n",
    "# DECISION TREES REGRESSION:\n",
    "\n",
    "# 1) & 2)\n",
    "\n",
    "The best configurations of parameters and their performance for different types of features (all, only low-level, only high-level) found from the cross validator are shown in the table:\n",
    "\n",
    "| Features |  MaxBins  | MaxDepth| Impurity |   RMSE   | TIME |\n",
    "|----------|-----------|---------|----------|----------|------|\n",
    "|   ALL    |    20     |   10    | variance |  0.449   |393.80|\n",
    "|   LOW    |    30     |   10    | variance |  0.484   |337.43|\n",
    "|   HIGH   |    10     |   10    | variance |  0.449   |313.54|\n",
    "\n",
    "As shown from the table the best RMSE is obtained when using only the low-level features (even better RMSE than when using all the features). Therefore, for regression low-level features are more relevant.\n",
    "\n",
    "# 3)\n",
    "\n",
    "Using ALL features, different number of cores were used:\n",
    "\n",
    "| Cores | TIME |\n",
    "|-------|------|\n",
    "|    1  |521.22|\n",
    "|    5  |373.99|\n",
    "|   10  |379.97|\n",
    "\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION:\n",
    "\n",
    "# 1) & 2)\n",
    "\n",
    "The best configurations of parameters and their performance for different types of features (all, only low-level, only high-level) found from the cross validator are shown in the table:\n",
    "\n",
    "| Features |elasticnet |   Reg   |  MaxIter | ACCURACY | TIME |\n",
    "|----------|-----------|---------|----------|----------|------|\n",
    "|   ALL    |    0      |    0    |     10   |  0.639   |773.56|\n",
    "|   LOW    |    0      |    0    |     5    |  0.565   |701.38|\n",
    "|   HIGH   |    0      |    0    |     10   |  0.617   |517.53|\n",
    "\n",
    "As shown from the table the best accuracy is obtained when using all the features. Additionally, high-level features provide better accuracy than low-level features, confirming the result of decision tree classification above: that high-level features are more relevant for classification. \n",
    "# 3)\n",
    "\n",
    "Using ALL features, different number of cores were used:\n",
    "\n",
    "| Cores | TIME  |\n",
    "|-------|-------|\n",
    "|    1  |1007.93|\n",
    "|    5  | 735.44|\n",
    "|   10  | 694.67|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
      ]
     },
     "execution_count": 14,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6935890480901171\n",
      "maxBins = 30\n",
      "maxDepth = 10\n",
      "impurity = entropy\n",
      "Time (seconds)748.040070298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassificationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassifier\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.DoubleType\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{HashingTF, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.linalg.Vector\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mDT_Class\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "object DT_Class {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"DT_Class\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "\n",
    "val timing = System.nanoTime\n",
    "        \n",
    "val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"files/HIGGS_r.csv.gz\").toDF\n",
    "\n",
    "\n",
    "val toDouble = udf[Double, Double]( _.toDouble)\n",
    "   \n",
    "val dfnew = df\n",
    ".withColumn(\"label\", toDouble(df(\"_c0\")))\n",
    ".withColumn(\"lepton_pT\", toDouble(df(\"_c1\")))\n",
    ".withColumn(\"lepton_eta\", toDouble(df(\"_c2\")))\n",
    ".withColumn(\"lepton_phi\", toDouble(df(\"_c3\")))\n",
    ".withColumn(\"missing_energy_magnitude\", toDouble(df(\"_c4\")))\n",
    ".withColumn(\"missing_energy_phi\", toDouble(df(\"_c5\")))\n",
    ".withColumn(\"jet_1_pt\", toDouble(df(\"_c6\")))\n",
    ".withColumn(\"jet_1_eta\", toDouble(df(\"_c7\")))\n",
    ".withColumn(\"jet_1_phi\", toDouble(df(\"_c8\")))\n",
    ".withColumn(\"jet_1_b-tag\", toDouble(df(\"_c9\")))\n",
    ".withColumn(\"jet_2_pt\", toDouble(df(\"_c10\")))\n",
    ".withColumn(\"jet_2_eta\", toDouble(df(\"_c11\")))\n",
    ".withColumn(\"jet_2_phi\", toDouble(df(\"_c12\")))\n",
    ".withColumn(\"jet_2_b-tag\", toDouble(df(\"_c13\")))\n",
    ".withColumn(\"jet_3_pt\", toDouble(df(\"_c14\")))\n",
    ".withColumn(\"jet_3_eta\", toDouble(df(\"_c15\")))\n",
    ".withColumn(\"jet_3_phi\", toDouble(df(\"_c16\")))\n",
    ".withColumn(\"jet_3_b-tag\", toDouble(df(\"_c17\")))\n",
    ".withColumn(\"jet_4_pt\", toDouble(df(\"_c18\")))\n",
    ".withColumn(\"jet_4_eta\", toDouble(df(\"_c19\")))\n",
    ".withColumn(\"jet_4_phi\", toDouble(df(\"_c20\")))\n",
    ".withColumn(\"jet_4_b-tag\", toDouble(df(\"_c21\")))\n",
    ".withColumn(\"m_jj\", toDouble(df(\"_c22\")))\n",
    ".withColumn(\"m_jjj\", toDouble(df(\"_c23\")))\n",
    ".withColumn(\"m_lv\", toDouble(df(\"_c24\")))\n",
    ".withColumn(\"m_jlv\", toDouble(df(\"_c25\")))\n",
    ".withColumn(\"m_bb\", toDouble(df(\"_c26\")))\n",
    ".withColumn(\"m_wbb\", toDouble(df(\"_c27\")))\n",
    ".withColumn(\"m_wwbb\", toDouble(df(\"_c28\")))\n",
    ".select(\"label\",\"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\",\n",
    "\"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\",\n",
    "\"jet_4_b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\")\n",
    "    \n",
    "        //dfnew.show()\n",
    "     \n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\",\n",
    "\"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\",\n",
    "\"jet_4_b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = dfnew.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "        \n",
    "// Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, dt))\n",
    "        \n",
    "        \n",
    "val paramGrid = new ParamGridBuilder()\n",
    ".addGrid(dt.maxBins, Array(10, 20, 30))\n",
    ".addGrid(dt.maxDepth, Array(10, 15, 20))\n",
    ".addGrid(dt.impurity, Array(\"entropy\", \"gini\"))\n",
    ".build()\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new MulticlassClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5)         \n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModel = cv.fit(trainingData)\n",
    "// Make predictions.\n",
    "val predictions = cvModel.transform(testData)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Accuracy: \"  + (accuracy))\n",
    "\n",
    "val bestparameters = cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestparameters.stages\n",
    "\n",
    "val dtstage = stages(1).asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(\"maxBins = \" + dtstage.getMaxBins)\n",
    "println(\"maxDepth = \" + dtstage.getMaxDepth)\n",
    "println(\"impurity = \" + dtstage.getImpurity)\n",
    "        \n",
    "println(\"Time (seconds)\" + ((System.nanoTime-timing) / 1e9d))\n",
    "\n",
    "       \n",
    "}\n",
    "}\n",
    "DT_Class.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.45087236357115557\n",
      "maxBins = 20\n",
      "maxDepth = 10\n",
      "impurity = variance\n",
      "Time (seconds)312.495914529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StopWordsRemover\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.DoubleType\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.regression.DecisionTreeRegressor\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.regression.DecisionTreeRegressionModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mDT_Reg\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressionModel\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "object DT_Reg {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"DT_Reg\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "\n",
    "val timing = System.nanoTime\n",
    "\n",
    "val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"files/HIGGS_r.csv.gz\").toDF\n",
    "\n",
    "\n",
    "val toDouble = udf[Double, Double]( _.toDouble)\n",
    "   \n",
    "val dfnew = df\n",
    ".withColumn(\"label\", toDouble(df(\"_c0\")))\n",
    ".withColumn(\"lepton_pT\", toDouble(df(\"_c1\")))\n",
    ".withColumn(\"lepton_eta\", toDouble(df(\"_c2\")))\n",
    ".withColumn(\"lepton_phi\", toDouble(df(\"_c3\")))\n",
    ".withColumn(\"missing_energy_magnitude\", toDouble(df(\"_c4\")))\n",
    ".withColumn(\"missing_energy_phi\", toDouble(df(\"_c5\")))\n",
    ".withColumn(\"jet_1_pt\", toDouble(df(\"_c6\")))\n",
    ".withColumn(\"jet_1_eta\", toDouble(df(\"_c7\")))\n",
    ".withColumn(\"jet_1_phi\", toDouble(df(\"_c8\")))\n",
    ".withColumn(\"jet_1_b-tag\", toDouble(df(\"_c9\")))\n",
    ".withColumn(\"jet_2_pt\", toDouble(df(\"_c10\")))\n",
    ".withColumn(\"jet_2_eta\", toDouble(df(\"_c11\")))\n",
    ".withColumn(\"jet_2_phi\", toDouble(df(\"_c12\")))\n",
    ".withColumn(\"jet_2_b-tag\", toDouble(df(\"_c13\")))\n",
    ".withColumn(\"jet_3_pt\", toDouble(df(\"_c14\")))\n",
    ".withColumn(\"jet_3_eta\", toDouble(df(\"_c15\")))\n",
    ".withColumn(\"jet_3_phi\", toDouble(df(\"_c16\")))\n",
    ".withColumn(\"jet_3_b-tag\", toDouble(df(\"_c17\")))\n",
    ".withColumn(\"jet_4_pt\", toDouble(df(\"_c18\")))\n",
    ".withColumn(\"jet_4_eta\", toDouble(df(\"_c19\")))\n",
    ".withColumn(\"jet_4_phi\", toDouble(df(\"_c20\")))\n",
    ".withColumn(\"jet_4_b-tag\", toDouble(df(\"_c21\")))\n",
    ".withColumn(\"m_jj\", toDouble(df(\"_c22\")))\n",
    ".withColumn(\"m_jjj\", toDouble(df(\"_c23\")))\n",
    ".withColumn(\"m_lv\", toDouble(df(\"_c24\")))\n",
    ".withColumn(\"m_jlv\", toDouble(df(\"_c25\")))\n",
    ".withColumn(\"m_bb\", toDouble(df(\"_c26\")))\n",
    ".withColumn(\"m_wbb\", toDouble(df(\"_c27\")))\n",
    ".withColumn(\"m_wwbb\", toDouble(df(\"_c28\")))\n",
    ".select(\"label\",\"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\",\n",
    "\"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\",\n",
    "\"jet_4_b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\")\n",
    "    \n",
    "        //dfnew.show()\n",
    "\n",
    "     \n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "        \n",
    "val dt = new DecisionTreeRegressor()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  \n",
    " \n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = dfnew.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, dt))\n",
    "        \n",
    "        \n",
    "val paramGrid = new ParamGridBuilder()\n",
    ".addGrid(dt.maxBins, Array(10, 20, 30))\n",
    ".addGrid(dt.maxDepth, Array(10, 15, 20))\n",
    ".addGrid(dt.impurity, Array(\"variance\"))\n",
    ".build()\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new RegressionEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5)         \n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModel = cv.fit(trainingData)\n",
    "// Make predictions.\n",
    "val predictions = cvModel.transform(testData)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse)\n",
    "        \n",
    "val bestparameters = cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestparameters.stages\n",
    "\n",
    "val dtstage = stages(1).asInstanceOf[DecisionTreeRegressionModel]\n",
    "println(\"maxBins = \" + dtstage.getMaxBins)\n",
    "println(\"maxDepth = \" + dtstage.getMaxDepth)\n",
    "println(\"impurity = \" + dtstage.getImpurity)\n",
    "\n",
    "println(\"Time (seconds)\" + ((System.nanoTime-timing) / 1e9d))\n",
    "\n",
    "}\n",
    "}\n",
    "DT_Reg.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6416240562937792\n",
      "elasticnetparam = 0.0\n",
      "regparam = 0.0\n",
      "maxiter = 10\n",
      "Time (seconds)773.832084513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassificationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.DecisionTreeClassifier\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StopWordsRemover\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.DoubleType\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mLogistic_Reg\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "\n",
    "object Logistic_Reg {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"Logistic_Reg\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "\n",
    "val timing = System.nanoTime\n",
    "\n",
    "val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"files/HIGGS_r.csv.gz\").toDF\n",
    "\n",
    "\n",
    "val toDouble = udf[Double, Double]( _.toDouble)\n",
    "   \n",
    "val dfnew = df\n",
    ".withColumn(\"label\", toDouble(df(\"_c0\")))\n",
    ".withColumn(\"lepton_pT\", toDouble(df(\"_c1\")))\n",
    ".withColumn(\"lepton_eta\", toDouble(df(\"_c2\")))\n",
    ".withColumn(\"lepton_phi\", toDouble(df(\"_c3\")))\n",
    ".withColumn(\"missing_energy_magnitude\", toDouble(df(\"_c4\")))\n",
    ".withColumn(\"missing_energy_phi\", toDouble(df(\"_c5\")))\n",
    ".withColumn(\"jet_1_pt\", toDouble(df(\"_c6\")))\n",
    ".withColumn(\"jet_1_eta\", toDouble(df(\"_c7\")))\n",
    ".withColumn(\"jet_1_phi\", toDouble(df(\"_c8\")))\n",
    ".withColumn(\"jet_1_b-tag\", toDouble(df(\"_c9\")))\n",
    ".withColumn(\"jet_2_pt\", toDouble(df(\"_c10\")))\n",
    ".withColumn(\"jet_2_eta\", toDouble(df(\"_c11\")))\n",
    ".withColumn(\"jet_2_phi\", toDouble(df(\"_c12\")))\n",
    ".withColumn(\"jet_2_b-tag\", toDouble(df(\"_c13\")))\n",
    ".withColumn(\"jet_3_pt\", toDouble(df(\"_c14\")))\n",
    ".withColumn(\"jet_3_eta\", toDouble(df(\"_c15\")))\n",
    ".withColumn(\"jet_3_phi\", toDouble(df(\"_c16\")))\n",
    ".withColumn(\"jet_3_b-tag\", toDouble(df(\"_c17\")))\n",
    ".withColumn(\"jet_4_pt\", toDouble(df(\"_c18\")))\n",
    ".withColumn(\"jet_4_eta\", toDouble(df(\"_c19\")))\n",
    ".withColumn(\"jet_4_phi\", toDouble(df(\"_c20\")))\n",
    ".withColumn(\"jet_4_b-tag\", toDouble(df(\"_c21\")))\n",
    ".withColumn(\"m_jj\", toDouble(df(\"_c22\")))\n",
    ".withColumn(\"m_jjj\", toDouble(df(\"_c23\")))\n",
    ".withColumn(\"m_lv\", toDouble(df(\"_c24\")))\n",
    ".withColumn(\"m_jlv\", toDouble(df(\"_c25\")))\n",
    ".withColumn(\"m_bb\", toDouble(df(\"_c26\")))\n",
    ".withColumn(\"m_wbb\", toDouble(df(\"_c27\")))\n",
    ".withColumn(\"m_wwbb\", toDouble(df(\"_c28\")))\n",
    ".select(\"label\",\"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\",\n",
    "\"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\",\n",
    "\"jet_4_b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\")\n",
    "    \n",
    "        //dfnew.show()\n",
    "\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b-tag\",\n",
    "\"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b-tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b-tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\",\n",
    "\"jet_4_b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"))\n",
    "  .setOutputCol(\"features\")\n",
    "                \n",
    "val lr = new LogisticRegression()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  \n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = dfnew.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, lr))\n",
    "        \n",
    "val paramGrid = new ParamGridBuilder()\n",
    ".addGrid(lr.elasticNetParam, Array(0, 0.2, 0.4, 0.6, 0.8, 1))\n",
    ".addGrid(lr.regParam, Array(0.0, 0.5, 1.0))\n",
    ".addGrid(lr.maxIter, Array(3,4,5,8,10))\n",
    ".build()\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new MulticlassClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5)         \n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModel = cv.fit(trainingData)\n",
    "// Make predictions.\n",
    "val predictions = cvModel.transform(testData)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Accuracy: \"  + (accuracy))\n",
    "\n",
    "val bestparameters = cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestparameters.stages\n",
    "\n",
    "val lrstage = stages(1).asInstanceOf[LogisticRegressionModel]\n",
    "println(\"elasticnetparam = \" + lrstage.getElasticNetParam)\n",
    "println(\"regparam = \" + lrstage.getRegParam)\n",
    "println(\"maxiter = \" + lrstage.getMaxIter)\n",
    "\n",
    "println(\"Time (seconds)\" + ((System.nanoTime-timing) / 1e9d))\n",
    "\n",
    "}\n",
    "}\n",
    "Logistic_Reg.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}