{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bigger decision trees</h1>\n",
    "\n",
    "This notebook presents bigger examples of decision trees and introduces regression using a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// decision tree imports\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "\n",
    "// importing CSV data into the expected format\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regression</h2>\n",
    "\n",
    "Using the <tt>spambase</tt> dataset from the previous notebook, we perform regression using a decision tree with variance as an impurity measure and a maximum tree depth of 5. The Mean Squared Error is computed at the end to evaluate goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.0851854980284374\n",
      "Learned regression tree model:\n",
      "DecisionTreeModel regressor of depth 5 with 45 nodes\n",
      "  If (feature 52 <= 0.055)\n",
      "   If (feature 6 <= 0.05)\n",
      "    If (feature 51 <= 0.378)\n",
      "     If (feature 15 <= 0.19)\n",
      "      If (feature 22 <= 0.29)\n",
      "       Predict: 0.061284619917501476\n",
      "      Else (feature 22 > 0.29)\n",
      "       Predict: 0.8125\n",
      "     Else (feature 15 > 0.19)\n",
      "      If (feature 36 <= 0.19)\n",
      "       Predict: 0.525\n",
      "      Else (feature 36 > 0.19)\n",
      "       Predict: 0.06060606060606061\n",
      "    Else (feature 51 > 0.378)\n",
      "     If (feature 55 <= 10.0)\n",
      "      If (feature 16 <= 0.0)\n",
      "       Predict: 0.2072072072072072\n",
      "      Else (feature 16 > 0.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 55 > 10.0)\n",
      "      If (feature 38 <= 0.27)\n",
      "       Predict: 0.9074074074074074\n",
      "      Else (feature 38 > 0.27)\n",
      "       Predict: 0.0\n",
      "   Else (feature 6 > 0.05)\n",
      "    If (feature 26 <= 0.0)\n",
      "     If (feature 45 <= 0.0)\n",
      "      If (feature 29 <= 0.18)\n",
      "       Predict: 0.9674418604651163\n",
      "      Else (feature 29 > 0.18)\n",
      "       Predict: 0.0\n",
      "     Else (feature 45 > 0.0)\n",
      "      If (feature 1 <= 0.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 0.0)\n",
      "       Predict: 0.0\n",
      "    Else (feature 26 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 52 > 0.055)\n",
      "   If (feature 24 <= 0.21)\n",
      "    If (feature 45 <= 0.16)\n",
      "     If (feature 41 <= 0.13)\n",
      "      If (feature 55 <= 9.0)\n",
      "       Predict: 0.6666666666666666\n",
      "      Else (feature 55 > 9.0)\n",
      "       Predict: 0.9780058651026393\n",
      "     Else (feature 41 > 0.13)\n",
      "      If (feature 5 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 5 > 0.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 45 > 0.16)\n",
      "     If (feature 8 <= 0.11)\n",
      "      Predict: 0.0\n",
      "     Else (feature 8 > 0.11)\n",
      "      Predict: 1.0\n",
      "   Else (feature 24 > 0.21)\n",
      "    If (feature 4 <= 0.85)\n",
      "     If (feature 17 <= 0.06)\n",
      "      Predict: 0.0\n",
      "     Else (feature 17 > 0.06)\n",
      "      Predict: 1.0\n",
      "    Else (feature 4 > 0.85)\n",
      "     If (feature 7 <= 0.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 7 > 0.0)\n",
      "      Predict: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@71fb6a24\n",
       "\u001b[36mtext\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = files/spambase.data MapPartitionsRDD[34] at textFile at Main.scala:49\n",
       "\u001b[36mdata\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[36] at map at Main.scala:52\n",
       "\u001b[36msplits\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m]] = \u001b[33mArray\u001b[0m(\n",
       "  MapPartitionsRDD[37] at randomSplit at Main.scala:55,\n",
       "  MapPartitionsRDD[38] at randomSplit at Main.scala:55\n",
       ")\n",
       "\u001b[36mtrainingData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[37] at randomSplit at Main.scala:55\n",
       "\u001b[36mtestData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[38] at randomSplit at Main.scala:55\n",
       "\u001b[36mcategoricalFeaturesInfo\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mimpurity\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"variance\"\u001b[0m\n",
       "\u001b[36mmaxDepth\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m5\u001b[0m\n",
       "\u001b[36mmaxBins\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m32\u001b[0m\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32mDecisionTreeModel\u001b[0m = DecisionTreeModel regressor of depth 5 with 45 nodes\n",
       "\u001b[36mlabelsAndPredictions\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mDouble\u001b[0m, \u001b[32mDouble\u001b[0m)] = MapPartitionsRDD[63] at map at Main.scala:74\n",
       "\u001b[36mtestMSE\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.0851854980284374\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Create Spark session\n",
    "val sparkSession = SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"Decision Tree example\")\n",
    "    .getOrCreate()\n",
    "\n",
    "// Load the data\n",
    "val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")\n",
    "\n",
    "// Separate into array\n",
    "val data = text.map(line => line.split(',').map(_.toDouble)).map(t => LabeledPoint(t(57), Vectors.dense(t.take(57))))\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"variance\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,\n",
    "  maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelsAndPredictions = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testMSE = labelsAndPredictions.map{ case (v, p) => math.pow(v - p, 2) }.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression tree model:\\n\" + model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exercises</h1>\n",
    "\n",
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Run your standalone decision tree program on the [Physical Activity Monitoring](http://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring) and the [Physics](http://archive.ics.uci.edu/ml/datasets/HIGGS) datasets, methodically experimenting with the <tt>maxDepth</tt> and <tt>maxBins</tt> values. Obtain timings for each experiment. Note that the <tt>physical activity monitoring</tt> dataset contains <tt>NaN</tt> (not a number) values when values are missing - you should try dealing with this in two ways\n",
    "\n",
    "1. Drop lines containing <tt>NaN</tt>\n",
    "2. Replace <tt>NaN</tt> with the average value from that column\n",
    "\n",
    "Run experiments with both options.\n",
    "\n",
    "<h3>Exercise 2</h3>\n",
    "\n",
    "Determine which features are the most important for classification (start by fixing your <tt>maxDepth</tt> and <tt>maxBins</tt> values). Restrict the decision tree program to only these features and compare preformance against the full feature set.\n",
    "\n",
    "<h2>Exercise 3</h2>\n",
    "\n",
    "Carry out both exercises also with the regression using decision tree program above."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
