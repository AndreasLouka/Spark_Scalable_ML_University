{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$k$-means</h1>\n",
    "\n",
    "[$k$-means](http://en.wikipedia.org/wiki/K-means_clustering) is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The following parameters are available:\n",
    "\n",
    "- <tt>k</tt> is the number of desired clusters.\n",
    "- <tt>maxIterations</tt> is the maximum number of iterations to run.\n",
    "- <tt>initializationMode</tt> specifies either random initialization or initialization via $k$-means.\n",
    "- <tt>runs</tt> is the number of times to run the $k$-means algorithm ($k$-means is not guaranteed to find a globally optimal solution, and when run multiple times on a given dataset, the algorithm returns the best clustering result).\n",
    "- <tt>initializationSteps</tt> determines the number of steps in the $k$-means algorithm.\n",
    "- <tt>epsilon</tt> determines the distance threshold within which we consider $k$-means to have converged.\n",
    "- <tt>initialModel</tt> is an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.\n",
    "\n",
    "First, the usual initializations with the relevant imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will start our small scale $k$-means clustering experiment. We will use the [Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase) dataset within the Jupyter notebook (you can download the dataset from the link given, but for the notebook, it's available at <tt>files/spambase.data</tt>). \n",
    "\n",
    "First step involves loading and parsing the data. Note that the data has known format, and it includes a label, classifying each instance as spam or not spam in the final column. This will need to be taken off for the purposes of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkSession = SparkSession\n",
    "  .builder()\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"k-means\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "\n",
    "// Load the data\n",
    "val data = sc.textFile(\"files/spambase.data\")\n",
    "\n",
    "// Split off the labels and parse the remaining data (known format)\n",
    "val labels = data.map(s => Vectors.dense(s.split(',').takeRight(1).map(_.toDouble)))\n",
    "val parsedData = data.map(s => Vectors.dense(s.split(',').take(57).map(_.toDouble))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the <tt>KMeans</tt> object to cluster the data into two clusters. We pass the number of clusters to the algorithm. To evaluate the clustering, we compute the <tt>Within Set Sum of Squared Error</tt> (WSSSE). This can be reduced by increasing $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Cluster the data into two classes using KMeans\n",
    "val numClusters = 2\n",
    "val numIterations = 200\n",
    "val clusters = KMeans.train(parsedData, numClusters, numIterations)\n",
    "\n",
    "// Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(parsedData)\n",
    "println(\"Within Set Sum of Squared Errors = \" + WSSSE)\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "clusters.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The MLlib implementation includes a parallelized variant of the [k-means++](http://en.wikipedia.org/wiki/K-means%2B%2B) method called [k-means||](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf). Both these techniques are ways to initialize the centroids rather than picking at random. In <tt>k-means++</tt>, the decision regarding each centroid is dependent on the previous decision, so it cannot be parallelised. <tt>k-means||</tt> is parallelizable.\n",
    "\n",
    "<h1>Exercises</h1>\n",
    "\n",
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Make the $k$-means implementation a standalone program. You should run your HPC application on the KDDCUP1999 data with 4M points. The dataset is available [here](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+Data).\n",
    "\n",
    "<h2>Exercise 2</h2>\n",
    "\n",
    "The details of <tt>k-means||</tt> are available in the paper linked above. The algorithm details are reproduced for you here:\n",
    "\n",
    "<img src=\"images/kmeans_alg2.png\" />\n",
    "\n",
    "For this week's exercise, you should implement the centroid initialization directly. Some of the functions you will need to implement are indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Sampling a point with a probability\n",
    "\n",
    "// Calculating / updating phi_X(C)\n",
    "\n",
    "// Main algorithm"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
