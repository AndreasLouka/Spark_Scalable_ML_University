{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COM6012 - 2017: Practical Quiz 3\n",
    "\n",
    "This quiz is built upon your previous quiz 2 so you can reuse the PracticalQuiz2Solution. You will still use logistic regression to perform classification in the [spam](./files/spambase.data) dataset. You will split the samples in two datasets, one for training (with 70% of the samples), and one for testing (with 30% of the samples). You will use a regularisation parameter of 0.01 and an elastic net parameter of 0.1.\n",
    "\n",
    "Step 1 [2 marks]: Use PCA to reduce the features to keep (approximately) 95% of the total variance using computePrincipalComponentsAndExplainedVariance. Report the reduced feature dimension and variance kept (in percentage), as well as the classification accuracy.\n",
    "\n",
    "Step 2 [1 mark]: Use PCA to reduce the features to keep (approximately) 85% of the total variance using computeSVD. Report the reduced feature dimension and variance kept (in percentage), as well as the classification accuracy.\n",
    "\n",
    "Step 3 [2 marks]: Please tell which original attribute (feature, i.e., raw feature before applying PCA) explains the most variance in the data and how much variance (in percentage) it has explained.\n",
    "\n",
    "Provide your answers in the Notebook.\n",
    "\n",
    "## Answers:\n",
    "1)Accuracy: 0.6997105643994211\n",
    "\n",
    "reduced feature dimensiton: 2 pc's\n",
    "\n",
    "variance: 99.80699856872896\n",
    "\n",
    "2)Accuracy: 0.683206106870229\n",
    "\n",
    "reduced feature dimensiton: 2 pc's\n",
    "\n",
    "variance: 99.80699856872896\n",
    "\n",
    "3)variance summary: [0.09324324068491728,1.6655843234835528,0.2541600473337551,1.946447346588171,0.45227342485092964,0.07497962843616227,0.15322633420900933,0.16085830998932185,0.0776267998072253,0.4157095251221379,\n",
    "0.040620251608818554,0.742524255407614,0.09062255504001963,0.11234819972973736,0.06699993229260176,0.6819319336533649,0.19718513519745243,0.28209102715855694,3.1523315909584833,0.25986228107311266,\n",
    "1.4419442036986294,1.0521745333528625,0.12270057502622313,0.19592620955746853,2.793408623947534,0.7866897712879051,11.338654082864783,0.2900641551931054,0.3520364539372348,0.2085580407850846,\n",
    "0.16272550978520736,0.10795093873732554,0.30903281913194697,0.10853422589607174,0.28330057489392657,0.16210539211702582,0.17931106060119276,0.04868677004998913,0.18893979265377084,0.12244119517496208,\n",
    "0.13046883577294163,0.5880120513971442,0.05009171113085058,0.3868536140347565,1.023511045840697,0.8301379472609923,0.00581776436124472,0.08164428809427061,0.059278287522844746,0.07309202820436013,\n",
    "0.011967083113500844,0.6653202097612048,0.0604579635030191,0.1843346284614876,1006.7579173576536,37982.62252893982,367657.7160786403]\n",
    "\n",
    "Max variance Index:56\n",
    "\n",
    "Max variance: 367657.7160786403\n",
    "\n",
    "Percentage variance of max attripute: 90.40413231387978 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6610909090909091\n",
      "variance: 99.80699856872896\n",
      "Time: 5.729179775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.RowMatrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Matrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.SingularValueDecomposition\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.DataFrame\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{StructField, StructType, LongType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{DoubleType, IntegerType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.param.ParamMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mclassification\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SparkSession._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.SingularValueDecomposition\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, LongType}\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "object classification {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        val timing = System.nanoTime        \n",
    "        \n",
    "        // Load the data\n",
    "        val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")\n",
    "        // Separate into array\n",
    "        val data = text.map(line => line.split(',').map(_.toDouble))\n",
    "        // Organise data into feastures and labels\n",
    "        val dataLP = data.map(t => (t(57), Vectors.dense(t.take(57))))\n",
    "        \n",
    "        val mat: RowMatrix = new RowMatrix(dataLP.values)\n",
    "        val (pc, eigen) = mat.computePrincipalComponentsAndExplainedVariance(2)\n",
    "        \n",
    "        val projected: RowMatrix = mat.multiply(pc)\n",
    "        \n",
    "        val pcaRDD = projected.rows\n",
    "        val pcaRDD2 = pcaRDD.map(_.toArray).map{case Array(pca1, pca2) => (pca1, pca2)}\n",
    "        val pcaDF = sparkSession.createDataFrame(pcaRDD2).toDF(\"pca1\", \"pca2\")\n",
    "        \n",
    "        val dataLP_label = dataLP.keys\n",
    "        val DataLP_label = dataLP_label.toDF(\"label\")\n",
    "        \n",
    "        val allrows = pcaDF.rdd.zip(DataLP_label.rdd).map{\n",
    "            case (rowLeft, rowRight) => Row.fromSeq(rowLeft.toSeq ++ rowRight.toSeq)}\n",
    "        \n",
    "        val schema = StructType(pcaDF.schema.fields ++ DataLP_label.schema.fields)\n",
    "        \n",
    "        val the_DF: DataFrame = sqlContext.createDataFrame(allrows, schema)\n",
    "        \n",
    "        val DFF = the_DF.select(the_DF(\"label\").cast(DoubleType).as(\"label\"),\n",
    "                                the_DF(\"pca1\").cast(DoubleType).as(\"pca1\"),\n",
    "                                the_DF(\"pca2\").cast(DoubleType).as(\"pca2\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        // Index the labels\n",
    "        val labelIndexer = new StringIndexer()\n",
    "          .setInputCol(\"label\")\n",
    "          .setOutputCol(\"indexedLabel\")\n",
    "          .fit(DFF)\n",
    "        \n",
    "        val assembler = new VectorAssembler().setInputCols(Array(\"pca1\", \"pca2\"))\n",
    "                                            .setOutputCol(\"features\")\n",
    "        \n",
    "       \n",
    "    \n",
    "       val lr = new LogisticRegression()\n",
    "          .setMaxIter(5)\n",
    "          .setElasticNetParam(0.1)\n",
    "          .setRegParam(0.01)\n",
    "        \n",
    "        //Split the data into training and test sets (30% held out for testing).\n",
    "        val Array(trainingData, testData) = DFF.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "\n",
    "\n",
    "        // Pipeline.\n",
    "        // Set the Pipeline and the Stages\n",
    "        val pipeline = new Pipeline()\n",
    "          .setStages(Array(labelIndexer, assembler, lr))\n",
    "\n",
    "        // Train model.\n",
    "        val model = pipeline.fit(trainingData)\n",
    "\n",
    "        val predictions = model.transform(testData)\n",
    "\n",
    "        // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new MulticlassClassificationEvaluator()\n",
    "          .setLabelCol(\"label\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"accuracy\")\n",
    "        val accuracy = evaluator.evaluate(predictions)\n",
    "        println(\"Accuracy: \"  + (accuracy))\n",
    "        \n",
    "        var i=0\n",
    "        var totalVariance = 0.999999999\n",
    "        var array = eigen.toArray\n",
    "        var currentVariance = 0.0\n",
    "        while (i < array.length){\n",
    "        currentVariance += array(i)\n",
    "        i += 1}\n",
    "        println(\"variance: \" + (currentVariance / totalVariance * 100))\n",
    "        \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "classification.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6977904490377762\n",
      "Time: 4.058148963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.RowMatrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Matrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.SingularValueDecomposition\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.DataFrame\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{StructField, StructType, LongType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{DoubleType, IntegerType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.param.ParamMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mclassification2\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SparkSession._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.SingularValueDecomposition\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, LongType}\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "object classification2 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        val timing = System.nanoTime        \n",
    "        \n",
    "        // Load the data\n",
    "        val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")\n",
    "        // Separate into array\n",
    "        val data = text.map(line => line.split(',').map(_.toDouble))\n",
    "        // Organise data into feastures and labels\n",
    "        val dataLP = data.map(t => (t(57), Vectors.dense(t.take(57))))\n",
    "        \n",
    "        \n",
    "        \n",
    "        val scaler = new StandardScaler(withMean = true, withStd = false).fit(dataLP.values).transform(dataLP.values)\n",
    "        val mat2: RowMatrix = new RowMatrix(scaler)\n",
    "        val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat2.computeSVD(2, computeU = true)\n",
    "        val singular: Vector = svd.s\n",
    "        val pca: Matrix = svd.V\n",
    "        val normalise = Vectors.dense(singular.toArray.map(a => a*a))\n",
    "       \n",
    "        \n",
    "        val projected: RowMatrix = mat2.multiply(pca)\n",
    "       \n",
    "        \n",
    "        \n",
    "        val pcaRDD = projected.rows\n",
    "        val pcaRDD2 = pcaRDD.map(_.toArray).map{case Array(pca1, pca2) => (pca1, pca2)}\n",
    "\n",
    "        val pcaDF = sparkSession.createDataFrame(pcaRDD2).toDF(\"pca1\", \"pca2\")\n",
    "\n",
    "        val dataLP_label = dataLP.keys\n",
    "        val DataLP_label = dataLP_label.toDF(\"label\")\n",
    "        \n",
    "        val allrows = pcaDF.rdd.zip(DataLP_label.rdd).map{\n",
    "            case (rowLeft, rowRight) => Row.fromSeq(rowLeft.toSeq ++ rowRight.toSeq)}\n",
    "        \n",
    "        val schema = StructType(pcaDF.schema.fields ++ DataLP_label.schema.fields)\n",
    "        \n",
    "        val the_DF: DataFrame = sqlContext.createDataFrame(allrows, schema)\n",
    "        \n",
    "        val DFF = the_DF.select(the_DF(\"label\").cast(DoubleType).as(\"label\"),\n",
    "                                the_DF(\"pca1\").cast(DoubleType).as(\"pca1\"),\n",
    "                                the_DF(\"pca2\").cast(DoubleType).as(\"pca2\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        // Index the labels\n",
    "        val labelIndexer = new StringIndexer()\n",
    "          .setInputCol(\"label\")\n",
    "          .setOutputCol(\"indexedLabel\")\n",
    "          .fit(DFF)\n",
    "        \n",
    "        val assembler = new VectorAssembler().setInputCols(Array(\"pca1\", \"pca2\"))\n",
    "                                            .setOutputCol(\"features\")\n",
    "        \n",
    "       \n",
    "    \n",
    "       val lr = new LogisticRegression()\n",
    "          .setMaxIter(5)\n",
    "          .setElasticNetParam(0.1)\n",
    "          .setRegParam(0.01)\n",
    "        \n",
    "        //Split the data into training and test sets (30% held out for testing).\n",
    "        val Array(trainingData, testData) = DFF.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "\n",
    "\n",
    "        // Pipeline.\n",
    "        // Set the Pipeline and the Stages\n",
    "        val pipeline = new Pipeline()\n",
    "          .setStages(Array(labelIndexer, assembler, lr))\n",
    "\n",
    "        // Train model.\n",
    "        val model = pipeline.fit(trainingData)\n",
    "\n",
    "        val predictions = model.transform(testData)\n",
    "\n",
    "        // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new MulticlassClassificationEvaluator()\n",
    "          .setLabelCol(\"label\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"accuracy\")\n",
    "        val accuracy = evaluator.evaluate(predictions)\n",
    "        println(\"Accuracy: \"  + (accuracy))      \n",
    "        \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "classification2.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09324324068491728,1.6655843234835528,0.2541600473337551,1.946447346588171,0.45227342485092964,0.07497962843616227,0.15322633420900933,0.16085830998932185,0.0776267998072253,0.4157095251221379,0.040620251608818554,0.742524255407614,0.09062255504001963,0.11234819972973736,0.06699993229260176,0.6819319336533649,0.19718513519745243,0.28209102715855694,3.1523315909584833,0.25986228107311266,1.4419442036986294,1.0521745333528625,0.12270057502622313,0.19592620955746853,2.793408623947534,0.7866897712879051,11.338654082864783,0.2900641551931054,0.3520364539372348,0.2085580407850846,0.16272550978520736,0.10795093873732554,0.30903281913194697,0.10853422589607174,0.28330057489392657,0.16210539211702582,0.17931106060119276,0.04868677004998913,0.18893979265377084,0.12244119517496208,0.13046883577294163,0.5880120513971442,0.05009171113085058,0.3868536140347565,1.023511045840697,0.8301379472609923,0.00581776436124472,0.08164428809427061,0.059278287522844746,0.07309202820436013,0.011967083113500844,0.6653202097612048,0.0604579635030191,0.1843346284614876,1006.7579173576536,37982.62252893982,367657.7160786403]'n\n",
      "Max variance Index:56\n",
      "Max variance: 367657.7160786403\n",
      "Percentage variance of max attripute: 90.40413231387978\n",
      "Time: 0.488257458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.RowMatrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Matrix\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.SingularValueDecomposition\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.DataFrame\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{StructField, StructType, LongType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{DoubleType, IntegerType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.param.ParamMap\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mvariance\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SparkSession._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.SingularValueDecomposition\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, LongType}\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "\n",
    "object variance {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        val timing = System.nanoTime        \n",
    "        \n",
    "        // Load the data\n",
    "        val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")\n",
    "        // Separate into array\n",
    "        val data = text.map(line => line.split(',').map(_.toDouble))\n",
    "        // Organise data into feastures and labels\n",
    "        val dataLP = data.map(t => (t(57), Vectors.dense(t.take(57))))\n",
    "        \n",
    "        val dataLPsummary: MultivariateStatisticalSummary = Statistics.colStats(dataLP.values)\n",
    "        \n",
    "        val variance_Array = dataLPsummary.variance.toArray\n",
    "        \n",
    "        print(dataLPsummary.variance)\n",
    "        \n",
    "        val maximumIndex = variance_Array.zipWithIndex.maxBy(_._1)._2\n",
    "        \n",
    "        println(\"'n\")\n",
    "        println(\"Max variance Index:\" + maximumIndex)\n",
    "        \n",
    "        \n",
    "        var maximumVariance = variance_Array.reduceLeft(_ max _)\n",
    "        println(\"Max variance: \" +maximumVariance)\n",
    "        \n",
    "        var totalVar = 0.0\n",
    "        var counter = 0\n",
    "        while (counter < variance_Array.length){\n",
    "            totalVar += variance_Array(counter)\n",
    "            counter +=1\n",
    "        }\n",
    "       \n",
    "        println(\"Percentage variance of max attripute: \" + maximumVariance / totalVar *100)\n",
    "        \n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "}\n",
    "}\n",
    "variance.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
