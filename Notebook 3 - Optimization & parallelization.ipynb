{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimization / parallelization</h1>\n",
    "\n",
    "Spark consists of a *driver* program that executes *parallel* operations on a cluster. There are two main abstractions that Spark provides:\n",
    "\n",
    "- An ability to divide a dataset into partitions which can be operated on in parallel.\n",
    "- Shared variables: by default, variables used in each copy of a function are distinct. However, Spark does allow for shared variables: either **broadcast** variables, which cache a value in memory on all nodes, or **accumulators**, which are only added to.\n",
    "\n",
    "We initiate our Spark session using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Understanding closures</h2>\n",
    "\n",
    "There is a difference between running in local and cluster mode; the main difference is variable values. Note that you may get unexpected behaviour due to assumptions about variable values being updated / not being updated!\n",
    "\n",
    "<h3>Broadcast variables</h3>\n",
    "\n",
    "To avoid creating a copy of a variable for each task, an accessible (read-only!) variable can be kept on each machine - this is useful for particularly large datasets which may be needed for multiple tasks. The data broadcasted this way is cached in serialized form and deserialized before running each task.\n",
    "\n",
    "Broadcast variables are created from a variable $v$ by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around $v$, and its value can be accessed by calling the *value* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@32b719ed\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@77668bb4\n",
       "\u001b[36mbroadcastVar\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mbroadcast\u001b[0m.\u001b[32mBroadcast\u001b[0m[\u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m]] = Broadcast(2)\n",
       "\u001b[36mres10_3\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkSession = SparkSession.builder\n",
    "  .master(\"local[1]\")\n",
    "  .appName(\"Parallelization\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "val broadcastVar = sc.broadcast(Array(1, 2, 3))\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accumulators</h3>\n",
    "\n",
    "[Accumulators](https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/Accumulator.html) are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in <tt>MapReduce</tt>) or sums. \n",
    "\n",
    "You can create a numeric accumulator by calling *SparkContext.longAccumulator()* or *SparkContext.doubleAccumulator()* to accumulate either Long or Double values (it is possible for users to create their own accumulators of different type), and the accumulator is created with an initial value. Cluster tasks can then add to it using <tt>add</tt>. However, they cannot read its value - that can only be read using <tt>value</tt> by the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mAcc\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object Acc {\n",
    "   def example( ) : Unit = {\n",
    "      val accum = sc.accumulator(0)\n",
    "      sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)\n",
    "      println(accum.value)\n",
    "   }\n",
    "}\n",
    "\n",
    "Acc.example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accumulators are only guaranteed to update the value of a variable once for updates within *actions*, within lazy transforms (such as *map*) accumulator updates are not guaranteed to be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m5\u001b[0m)\n",
       "\u001b[36maccum2\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mutil\u001b[0m.\u001b[32mLongAccumulator\u001b[0m = LongAccumulator(id: 92, name: None, value: 15)\n",
       "\u001b[36mres12_2\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m5\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val accum2 = sc.longAccumulator\n",
    "data.map { x => accum2.add(x); x }\n",
    "println(accum2.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sequential vs parallel operations</h2>\n",
    "\n",
    "Some examples of operations carried out in parallel are below. Note that these are executed on small collections for illustration purposes only - in general, you will want to run these on large collections.\n",
    "\n",
    "<h3>map</h3>\n",
    "\n",
    "Using a parallel map to transform a collection of <tt>String</tt> to all-uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlastNames\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mparallel\u001b[0m.\u001b[32mimmutable\u001b[0m.\u001b[32mParSeq\u001b[0m[\u001b[32mString\u001b[0m] = ParVector(Smith, Jones, Frankenstein, Bach, Jackson, Rodin)\n",
       "\u001b[36mres13_1\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mparallel\u001b[0m.\u001b[32mimmutable\u001b[0m.\u001b[32mParSeq\u001b[0m[\u001b[32mString\u001b[0m] = ParVector(SMITH, JONES, FRANKENSTEIN, BACH, JACKSON, RODIN)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val lastNames = List(\"Smith\",\"Jones\",\"Frankenstein\",\"Bach\",\"Jackson\",\"Rodin\").par\n",
    "lastNames.map(_.toUpperCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>fold</h3>\n",
    "\n",
    "Summing via fold on a <tt>ParArray</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mparArray\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mparallel\u001b[0m.\u001b[32mmutable\u001b[0m.\u001b[32mParArray\u001b[0m[\u001b[32mInt\u001b[0m] = ParArray(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340,\u001b[33m...\u001b[0m\n",
       "\u001b[36mres14_1\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m50005000\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val parArray = (1 to 10000).toArray.par   \n",
    "parArray.fold(0)(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>filter</h3>\n",
    "\n",
    "Using a parallel filter to select the last names that come alphabetically after the letter “K”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlastNames\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mparallel\u001b[0m.\u001b[32mimmutable\u001b[0m.\u001b[32mParSeq\u001b[0m[\u001b[32mString\u001b[0m] = ParVector(Smith, Jones, Frankenstein, Bach, Jackson, Rodin)\n",
       "\u001b[36mres15_1\u001b[0m: \u001b[32mcollection\u001b[0m.\u001b[32mparallel\u001b[0m.\u001b[32mimmutable\u001b[0m.\u001b[32mParSeq\u001b[0m[\u001b[32mString\u001b[0m] = ParVector(Smith, Jones, Jackson, Rodin)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val lastNames = List(\"Smith\",\"Jones\",\"Frankenstein\",\"Bach\",\"Jackson\",\"Rodin\").par\n",
    "lastNames.filter(_.head >= 'J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you run the parallel operations above sequentially on a bigger collection (for example a list of numbers from 0 - 10000) and do timing experiments to compare speed of execution?\n",
    "\n",
    "<h2>Parallelized collections</h2>\n",
    "\n",
    "A parallelized collection is created by calling **parallelize** on an existing collection. Note that the dataset will be copied and then a distributed dataset is created that can be worked on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m5\u001b[0m)\n",
       "\u001b[36mdistData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mInt\u001b[0m] = ParallelCollectionRDD[2] at parallelize at Main.scala:30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of *partitions* can be set manually, by passing parallelize a second argument to the sparkContext\n",
    "\n",
    "    sc.parallelize(data, 10)\n",
    "    \n",
    "Spark tries to set the number of partitions automatically based on the cluster, the rule being 2-4 partitions for every CPU in the cluster.\n",
    "\n",
    "<h3>$\\pi$ Estimation</h3>\n",
    "\n",
    "Spark can also be used for compute-intensive tasks. This code estimates $\\pi$ by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be $\\pi / 4$, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNUM_SAMPLES\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m10000000\u001b[0m\n",
       "\u001b[36mcount\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m7855170\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val NUM_SAMPLES = 10000000\n",
    "\n",
    "val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>\n",
    "    val x = Math.random()\n",
    "    val y = Math.random()\n",
    "    if (x*x + y*y < 1) 1 else 0\n",
    "}.reduce(_ + _)\n",
    "\n",
    "println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The $\\pi$ estimation example is particularly interesting (and it appears in the exercises below!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "<h3>Exercise 1</h3>\n",
    "\n",
    "Create the <tt>accum2</tt> program from the <bb>accumulator</bb> section as a standalone program on the HPC to see whether / when it yields unexpected results.\n",
    "\n",
    "<h3>Exercise 2</h3>\n",
    "\n",
    "You have run the $\\pi$ estimation program above as a standalone program when learning to use the HPC and have seen it fail to scale. If we point you at the function <tt>XORShiftRandom</tt>, can you modify the program so it does scale? (Carry out timing experiments to prove that it now works as expected.)\n",
    "\n",
    "<h3>Exercise 3</h3>\n",
    "\n",
    "We introduced **transformations** and **actions** in Notebook 2 and extended our knowledge of them in this notebook. The main advantage of Spark is parallelization. In this exercise, you are to use the [bagofwords Enron .gz](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words) dataset, strip off the first 3 lines, and using the <tt>.partitions.size</tt> method - i.e. for example\n",
    "\n",
    "    distData.partitions.size\n",
    "    \n",
    "to investigate how / if the number of partitions changes after running transformations & actions. Read the file in as a <tt>.gz</tt> file, and investigate the effect of at least 10 of [Spark's most common transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations) and actions (make sure that your list includes <tt>reduceByKey</tt> and <tt>groupByKey</tt>)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
