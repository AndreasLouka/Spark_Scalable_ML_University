{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Creating standalone programs\n",
    "\n",
    "Since the amount of memory available to a Jupyter notebook in SageMathCloud is limited (which is why we have to close one to be able to open another), any larger scale programs have to be run on a larger machine and so need us to create standalone programs. We will continue to demonstrate small scale examples in the notebooks, and you can try out small bits of code in a Jupyter cell, but you should try to create the corresponding <tt>.scala</tt> programs on <tt>iceberg</tt> and elsewhere.\n",
    "\n",
    "If you are using a Windows machine to access <tt>iceberg.shef.ac.uk</tt> and <tt>sharc.shef.ac.uk</tt>, you'll need to download the portable version of [<tt>mobaXterm</tt>](http://mobaxterm.mobatek.net/download.html). This will provide you with access to your chosen HPC and a way to copy files between your Windows system and the remote HPC.\n",
    "\n",
    "Once you have logged into a node on <tt>iceberg</tt>, you may need to ask for more memory than the default <tt>qrsh</tt> gives you:\n",
    "\n",
    "    qrsh -l mem=8G -l rmem=8G\n",
    "    \n",
    "You can access <tt>iceberg</tt> from any location, but <tt>sharc</tt> must be accessed from the University network or via VPN. Both <tt>iceberg</tt> and <tt>sharc</tt> run Linux. While you can develop your initial programs on the Windows system in a text editor of your choice, and copy over to the HPC for compiling every time, you may benefit from taking a look at some Linux text editors as the programs get more complicated.\n",
    "\n",
    "## Differences between Sharc and Iceberg\n",
    "\n",
    "Iceberg and Sharc are configured slightly differently.  For example, the module command for `sbt`on Iceberg is\n",
    "\n",
    "```\n",
    "module load apps/binapps/sbt/0.13.13\n",
    "```\n",
    "\n",
    "Whereas on Sharc, you need to run two module commands \n",
    "\n",
    "```\n",
    "module load dev/sbt/0.13.13\n",
    "module load apps/java/jdk1.8.0_102/binary\n",
    "```\n",
    "\n",
    "The notes here are for Iceberg. The documentation for both HPC systems is at http://docs.hpc.shef.ac.uk/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SBT (Scala Build Tool)\n",
    "\n",
    "1. The organization of your project should be:\n",
    "```\n",
    "[user@HPC project]$ find .\n",
    ".\n",
    "./project.sbt\n",
    "./src\n",
    "./src/main\n",
    "./src/main/scala\n",
    "./src/main/scala/Project.scala\n",
    "```\n",
    "\n",
    "This means that you need to create your project using `mkdir` (which makes a directory), for example:\n",
    "```\n",
    "[user@HPC]$ mkdir Hello\n",
    "```\n",
    "\n",
    "Then screate the subdirectories\n",
    "\n",
    "```\n",
    "[user@HPC]$ mkdir Hello/src\n",
    "[user@HPC]$ mkdir Hello/src/main\n",
    "[user@HPC]$ mkdir Hello/src/main/scala\n",
    "```\n",
    "\n",
    "Both `project.sbt` and `Project.scala` are text files (so to start with can be created on your Windows machine and copied over to the HPC).\n",
    "\n",
    "2. `project.sbt` should contain any requirements of your program, such as library dependencies. For example:\n",
    "\n",
    "```\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.11.8\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-yarn\" % \"2.0.1\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"2.0.1\"\n",
    "```\n",
    "\n",
    "## To package a module using sbt on iceberg:\n",
    "\n",
    "- Start an interactive session on a worker node\n",
    "```\n",
    "qrsh\n",
    "```\n",
    "\n",
    "- Enter the relevant project\n",
    "\n",
    "```\n",
    "[user@HPC]$ cd Hello\n",
    "```\n",
    "\n",
    "- Package your module\n",
    "\n",
    "```\n",
    "[user@HPC project]$ sbt package\n",
    "```\n",
    "\n",
    "sbt should be loaded for you, but if the call fails then run\n",
    "\n",
    "```\n",
    "module load apps/binapps/sbt/0.13.13\n",
    "```\n",
    "\n",
    "## RUNNING on HPC\n",
    "\n",
    "### INTERACTIVE MODE\n",
    "\n",
    "To run in interactive mode (this DOES NOT have all the resources you would like and can end up being killed - it should only be used for `playing' with a program):\n",
    "\n",
    "- Start an interactive session on a worker node (making sure memory requirements are specified). For example:\n",
    "\n",
    "```\n",
    "qrsh -l mem=8G -l rmem=8G\n",
    "```\n",
    "\n",
    "- Load Spark\n",
    "\n",
    "```\n",
    "module load apps/gcc/4.4.7/spark/2.0\n",
    "```\n",
    "\n",
    "- Package your project as above using sbt\n",
    "\n",
    "- Execute using\n",
    "\n",
    "```\n",
    "spark-submit --master local[n] target/scala-2.11/project_2.11-1.0.jar\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "n specifies the number of threads needed\n",
    "\n",
    "So the Pi estimation program (from Notebook 3) could be run as follows\n",
    "\n",
    "```\n",
    "spark-submit --master local[1] target/scala-2.11/pi-estimation_2.11-1.0.jar 1000000 > out$CORES.1000000.txt\n",
    "```\n",
    "\n",
    "You can only use 1 thread in interactive mode.\n",
    "\n",
    "### SCHEDULER MODE\n",
    "\n",
    "To submit a job to the scheduler, create a bash job to submit containing what you need from the scheduler (i.e. the memory you want per core and the number of cores you'd like), and what your program requires. For the Pi estimation program, the bash program can be called \"pi.sh\" and can look like this (if it's being submitted from the toplevel \"project\" directory):\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "# Load spark\n",
    "module load apps/gcc/4.4.7/spark/2.0\n",
    "# Request 4 cores from the HPC scheduler\n",
    "#$ -pe openmp 4\n",
    "# Request 4Gig of virtual and real memory PER CORE\n",
    "#$ -l rmem=4G\n",
    "#$ -l mem=4G\n",
    "\n",
    "# Make sure CORES is equal to the number of openmp slots\n",
    "export CORES=4\n",
    "spark-submit --master local\\[$CORES\\] target/scala-2.11/pi-estimation_2.11-1.0.jar 1000000 > out$CORES.1000000.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
