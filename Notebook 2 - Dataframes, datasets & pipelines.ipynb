{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Library (MLlib)</h1>\n",
    "\n",
    "[MLlib](http://spark.apache.org/docs/latest/ml-guide.html) is Spark’s machine learning (ML) library. It provides:\n",
    "\n",
    "- *ML Algorithms*: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- *Featurization*: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- *Pipelines*: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- *Persistence*: saving and load algorithms, models, and Pipelines\n",
    "- *Utilities*: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "We carry out the usual settings, classpath and imports, this time including <tt>MLlib</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147 new artifacts in macro\n",
      "147 new artifacts in runtime\n",
      "147 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.classification.LogisticRegression\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{HashingTF, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.linalg.Vector\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// imports for the text document pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/03/02 09:48:23 INFO SparkContext: Running Spark version 2.0.1\n",
      "17/03/02 09:48:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/03/02 09:48:23 INFO SecurityManager: Changing view acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/02 09:48:23 INFO SecurityManager: Changing modify acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/02 09:48:23 INFO SecurityManager: Changing view acls groups to: \n",
      "17/03/02 09:48:23 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/03/02 09:48:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(b97eec96efcb40779e247b002e047f82); groups with view permissions: Set(); users  with modify permissions: Set(b97eec96efcb40779e247b002e047f82); groups with modify permissions: Set()\n",
      "17/03/02 09:48:24 INFO Utils: Successfully started service 'sparkDriver' on port 44056.\n",
      "17/03/02 09:48:24 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/03/02 09:48:24 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/03/02 09:48:24 INFO DiskBlockManager: Created local directory at /projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week2/java_temp/blockmgr-b6398131-464d-4868-b5d2-0b9758759aaf\n",
      "17/03/02 09:48:24 INFO MemoryStore: MemoryStore started with capacity 3.8 GB\n",
      "17/03/02 09:48:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/03/02 09:48:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "17/03/02 09:48:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.0.36:4040\n",
      "17/03/02 09:48:24 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/03/02 09:48:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36307.\n",
      "17/03/02 09:48:24 INFO NettyBlockTransferService: Server created on 10.240.0.36:36307\n",
      "17/03/02 09:48:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.0.36, 36307)\n",
      "17/03/02 09:48:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.240.0.36:36307 with 3.8 GB RAM, BlockManagerId(driver, 10.240.0.36, 36307)\n",
      "17/03/02 09:48:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.0.36, 36307)\n",
      "17/03/02 09:48:25 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n",
      "17/03/02 09:48:25 INFO SharedState: Warehouse path is '/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week2/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@5d265c75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Create Spark session\n",
    "val sparkSession = SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"Spark dataframes and datasets\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>MLlib</tt> allows easy combination of numerous algorithms into a single pipeline using standardized APIs for machine learning algorithms. The key concepts are:\n",
    "\n",
    "- **Dataframe**. Dataframes can hold a variety of data types.\n",
    "- **Transformer**. Transforms one dataframe into another.\n",
    "- **Estimator**. Algorithm which can be fit on a DataFrame to produce a Transformer.\n",
    "- **Pipeline**. A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "- **Parameter**. Transformers and Estimators share a common API for specifying parameters.\n",
    "\n",
    "More details on these below, and a list of some of the available ML features is available [here](http://spark.apache.org/docs/latest/ml-features.html).\n",
    "\n",
    "<h2>Datasets and Dataframes</h2>\n",
    "\n",
    "Along with the introduction of <tt>SparkSession</tt>, the <tt>resilient distributed dataset</tt> (RDD) was replaced by [dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset). Again, these are objects which can be worked on in parallel. The available operations are:\n",
    "\n",
    "- **transformations**: produce new datasets\n",
    "- **actions**: computations which return results\n",
    "\n",
    "We will start with creating dataframes and datasets, showing how we can print their contents. We create a dataframe in the cell below and print out some info (we can also modify the output before printing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeopleDF\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [age: bigint, name: string]\n",
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// create a dataframe based on the contents of a JSON file\n",
    "val peopleDF = sparkSession.read.json(\"files/people.json\")\n",
    "\n",
    "peopleDF.show()\n",
    "\n",
    "// Print the schema in a tree format\n",
    "peopleDF.printSchema()\n",
    "\n",
    "// Select only the \"name\" column\n",
    "peopleDF.select(\"name\").show()\n",
    "\n",
    "// This import is needed to use the $-notation\n",
    "import sparkSession.implicits._\n",
    "\n",
    "// Select everybody, but increment the age by 1\n",
    "peopleDF.select($\"name\", $\"age\" + 1).show()\n",
    "\n",
    "// Select people older than 21\n",
    "peopleDF.filter($\"age\" > 21).show()\n",
    "\n",
    "// Count people by age\n",
    "peopleDF.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset example is in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "| 10|\n",
      "| 15|\n",
      "| 20|\n",
      "| 25|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 95|\n",
      "| 90|\n",
      "| 85|\n",
      "| 80|\n",
      "| 75|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|                id|\n",
      "+-------+------------------+\n",
      "|  count|                19|\n",
      "|   mean|              50.0|\n",
      "| stddev|28.136571693556885|\n",
      "|    min|                 5|\n",
      "|    max|                95|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnumDS\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mjava\u001b[0m.\u001b[32mlang\u001b[0m.\u001b[32mLong\u001b[0m] = [id: bigint]\n",
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// create a dataset using sparkSession.range starting from 5 to 100, with increments of 5\n",
    "val numDS = sparkSession.range(5, 100, 5)\n",
    "\n",
    "// order by column\n",
    "numDS.orderBy(\"id\").show(5)\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "numDS.orderBy($\"id\".desc).show(5)\n",
    "\n",
    "// compute descriptive stats and display them\n",
    "numDS.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataframe example, showing access to columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|language|percent|\n",
      "+--------+-------+\n",
      "|Scala   |35     |\n",
      "|Python  |30     |\n",
      "|Java    |20     |\n",
      "|R       |15     |\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlangPercentDF\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [_1: string, _2: int]\n",
       "\u001b[36mlpDF\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [language: string, percent: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// create a DataFrame using sparkSession.createDataFrame from a List or Seq\n",
    "val langPercentDF = sparkSession.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))\n",
    "\n",
    "// rename the columns\n",
    "val lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")\n",
    "\n",
    "// order the DataFrame in descending order of percentage\n",
    "lpDF.orderBy($\"percent\".desc).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading text</h3>\n",
    "\n",
    "Aside from creating a dataset by transforming a previous one, we can also read data from a file directly into a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|  cdatetime|             address|district|      beat|grid|          crimedescr|ucr_ncic_code|   latitude|   longitude|\n",
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|1/1/06 0:00|  3108 OCCIDENTAL DR|       3|3C        |1115|10851(A)VC TAKE V...|         2404|38.55042047|-121.3914158|\n",
      "|1/1/06 0:00| 2082 EXPEDITION WAY|       5|5A        |1512|459 PC  BURGLARY ...|         2204|38.47350069|-121.4901858|\n",
      "|1/1/06 0:00|          4 PALEN CT|       2|2A        | 212|10851(A)VC TAKE V...|         2404|38.65784584|-121.4621009|\n",
      "|1/1/06 0:00|      22 BECKFORD CT|       6|6C        |1443|476 PC PASS FICTI...|         2501|38.50677377|-121.4269508|\n",
      "|1/1/06 0:00|    3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY-...|         2299| 38.6374478|-121.3846125|\n",
      "|1/1/06 0:00|  5301 BONNIEMAE WAY|       6|6B        |1084|530.5 PC USE PERS...|         2604|38.52697863|-121.4513383|\n",
      "|1/1/06 0:00|       2217 16TH AVE|       4|4A        | 957|459 PC  BURGLARY ...|         2299|  38.537173|-121.4875774|\n",
      "|1/1/06 0:00|           3547 P ST|       3|3C        | 853|484 PC   PETTY TH...|         2308|38.56433456|-121.4618826|\n",
      "|1/1/06 0:00|    3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY ...|         2203| 38.6374478|-121.3846125|\n",
      "|1/1/06 0:00|   1326 HELMSMAN WAY|       1|1B        | 444|1708 US   THEFT O...|         2310|38.60960217|-121.4918375|\n",
      "|1/1/06 0:00|  2315 STOCKTON BLVD|       6|6B        |1005|ASSAULT WITH WEAP...|         7000|38.55426406|-121.4546045|\n",
      "|1/1/06 0:00|        5112 63RD ST|       6|6B        |1088|530.5 PC USE PERS...|         2604|38.52816497|-121.4314528|\n",
      "|1/1/06 0:00|   6351 DRIFTWOOD ST|       4|4C        |1261|SUSP PERS-NO CRIM...|         7000|38.51092155|-121.5488201|\n",
      "|1/1/06 0:00|7721 COLLEGE TOWN DR|       3|3C        | 888|530.5 PC USE PERS...|         2604|38.55611545|-121.4142729|\n",
      "|1/1/06 0:00|     8460 ROVANA CIR|       6|6C        |1447|484G(B) PC ACCESS...|         2605|38.50398051|-121.3923987|\n",
      "|1/1/06 0:00|       4856 11TH AVE|       6|6B        |1054|487(A) PC GRAND T...|         2303|38.54152896|-121.4495097|\n",
      "|1/1/06 0:00|        6033 69TH ST|       6|6C        |1403|     TELEPEST -I RPT|         7000|38.51657327|-121.4234749|\n",
      "|1/1/06 0:00|            547 L ST|       3|3M        | 742|487(A) GRAND THEF...|         2308|38.58184562|-121.5011657|\n",
      "|1/1/06 0:00|        3525 42ND ST|       6|6A        |1034|530.5 PC USE PERS...|         2604|38.54270763|-121.4572067|\n",
      "|1/1/06 0:00|     5641 DORSET WAY|       4|4C        |1225|484J PC PUBLISH C...|         2605|38.52459987|-121.5203609|\n",
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfCrime\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [cdatetime: string, address: string ... 7 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Read a csv file\n",
    "val dfCrime = sparkSession.read.option(\"header\",\"true\").csv(\"files/SacramentocrimeJanuary2006.csv\")\n",
    "dfCrime.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read plain text as a dataset, we need an extra <tt>import</tt> for schema conversion. Once the text is read in, operations can be carried out to find line lengths, total length of text or anything else you may want to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|It was the best o...|\n",
      "|                    |\n",
      "|There were a king...|\n",
      "|                    |\n",
      "|It was the year o...|\n",
      "|                    |\n",
      "|France, less favo...|\n",
      "|                    |\n",
      "|In England, there...|\n",
      "|                    |\n",
      "|All these things,...|\n",
      "+--------------------+\n",
      "\n",
      "[value: int]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m\n",
       "\u001b[36mbookDS\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mString\u001b[0m] = [value: string]\n",
       "\u001b[36mlineLengths\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mInt\u001b[0m] = [value: int]\n",
       "\u001b[36mtotalLength\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m5773\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Read a plain text file\n",
    "import sparkSession.implicits._\n",
    "\n",
    "// class converts from dataframe to dataset output\n",
    "val bookDS = sparkSession.read.text(\"files/TaleOfTwoCities.txt\").as[String]\n",
    "bookDS.show()\n",
    "\n",
    "val lineLengths = bookDS.map(s => s.length)\n",
    "\n",
    "// To maintain lineLengths in memory\n",
    "//lineLengths.persist()\n",
    "\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)\n",
    "println(lineLengths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformations</h3>\n",
    "\n",
    "We create other datasets from an existing dataset using **transformations**. A list of some of the possible transformations is available [here](http://spark.apache.org/docs/latest/programming-guide.html#transformations), and some examples follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|     It|\n",
      "|    was|\n",
      "|    the|\n",
      "|   best|\n",
      "|     of|\n",
      "| times,|\n",
      "|     it|\n",
      "|    was|\n",
      "|    the|\n",
      "|  worst|\n",
      "|     of|\n",
      "| times,|\n",
      "|     it|\n",
      "|    was|\n",
      "|    the|\n",
      "|    age|\n",
      "|     of|\n",
      "|wisdom,|\n",
      "|     it|\n",
      "|    was|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwords\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[\u001b[32mString\u001b[0m] = [value: string]\n",
       "\u001b[36mgroupedWords\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mKeyValueGroupedDataset\u001b[0m[\u001b[32mString\u001b[0m, \u001b[32mString\u001b[0m] = org.apache.spark.sql.KeyValueGroupedDataset@c0b4d34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val words = bookDS.flatMap(value => value.split(\"\\\\s+\"))\n",
    "words.show()\n",
    "val groupedWords = words.groupByKey(_.toLowerCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Actions</h3>\n",
    "\n",
    "Some of the most common actions are available from [this page](http://spark.apache.org/docs/latest/programming-guide.html#actions). For example, <tt>count</tt> returns the number of elements in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|      value|count(1)|\n",
      "+-----------+--------+\n",
      "|       some|       3|\n",
      "|      those|       2|\n",
      "|   received|       1|\n",
      "|     taking|       1|\n",
      "|     worked|       1|\n",
      "|      lords|       2|\n",
      "|  countries|       1|\n",
      "|   spending|       1|\n",
      "|  character|       1|\n",
      "|    snipped|       1|\n",
      "|      dozen|       1|\n",
      "|   chickens|       1|\n",
      "|      among|       2|\n",
      "|       even|       1|\n",
      "|rest--along|       1|\n",
      "|  cautioned|       1|\n",
      "|        got|       1|\n",
      "|        did|       1|\n",
      "|   conceded|       1|\n",
      "|        two|       2|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcounts\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mDataset\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mLong\u001b[0m)] = [value: string, count(1): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val counts = groupedWords.count()\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipelines</h2>\n",
    "\n",
    "It is common that a number of algorithms need to run on some data. MLlib allows this to be encoded as a [pipeline](http://spark.apache.org/docs/latest/ml-pipeline.html), and it takes care of input / output of each phase.\n",
    "\n",
    "We demonstrate a simple pipeline using the task of stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[i, saw, the, red...|\n",
      "|[mary, had, a, li...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataSet\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [id: int, text: string]\n",
       "\u001b[36mtokenizer\u001b[0m: \u001b[32mTokenizer\u001b[0m = tok_bd1ce41acb00\n",
       "\u001b[36mwordsData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [id: int, text: string ... 1 more field]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Prepare dataset consisting of (id, text) tuples.\n",
    "val dataSet = sparkSession.createDataFrame(Seq(\n",
    "  (0, \"I saw the red baloon\"),\n",
    "  (1, \"Mary had a little lamb\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(dataSet)\n",
    "wordsData.select(\"words\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will have noticed in the previous notebook's exercises, the most common words in a text are often words such as *and*, *so* etc. These are not informative, and could be removed. Our pipeline is in two stages:\n",
    "\n",
    "1. tokenizer\n",
    "2. stop word removal\n",
    "\n",
    "These two stages are to be run in that order, and the input DataFrame will be transformed as it passes through them. Both stages are Transformer stages, and so the <tt>transform()</tt> method will be called on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|                text|               words|            filtered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|I saw the red baloon|[i, saw, the, red...|  [saw, red, baloon]|\n",
      "|  1|Mary had a little...|[mary, had, a, li...|[mary, little, lamb]|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[0m: \u001b[32mTokenizer\u001b[0m = tok_43963b0eea05\n",
       "\u001b[36mremover\u001b[0m: \u001b[32mStopWordsRemover\u001b[0m = stopWords_a619aebb3ebe\n",
       "\u001b[36mpipeline\u001b[0m: \u001b[32mPipeline\u001b[0m = pipeline_3af25d339b14\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32mPipelineModel\u001b[0m = pipeline_3af25d339b14\n",
       "\u001b[36mresult\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [id: int, text: string ... 2 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Configure an ML pipeline, which consists of two stages: tokenizer, and stopWordsRemover.\n",
    "\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(\"words\")\n",
    "    .setOutputCol(\"filtered\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer,remover))\n",
    "\n",
    "val model = pipeline.fit(dataSet)\n",
    "val result = model.transform(dataSet)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full power of pipelines, we present a second example: one which includes an estimator in the form of logistic regression. This pipeline has three steps:\n",
    "\n",
    "1. split each document's text into words (<i>tokenizer</i>)\n",
    "2. convert each document's words into a feature vector (<i>hashingTF</i>)\n",
    "3. learn a prediction model using the features vectors and labels (<i>logistic regression</i>)\n",
    "\n",
    "For Estimator stages, the <tt>fit()</tt> method is called to produce a Transformer and that Transformer’s <tt>transform()</tt> method is called on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.5406433544852302,0.45935664551476996], prediction=0.0\n",
      "(5, l m n) --> prob=[0.9334382627383524,0.06656173726164764], prediction=0.0\n",
      "(6, mapreduce spark) --> prob=[0.7799076868204318,0.22009231317956823], prediction=0.0\n",
      "(7, apache hadoop) --> prob=[0.9768636139518375,0.023136386048162483], prediction=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtraining\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [id: bigint, text: string ... 1 more field]\n",
       "\u001b[36mtokenizer\u001b[0m: \u001b[32mTokenizer\u001b[0m = tok_f559905b18b2\n",
       "\u001b[36mhashingTF\u001b[0m: \u001b[32mHashingTF\u001b[0m = hashingTF_53f2091000d0\n",
       "\u001b[36mlr\u001b[0m: \u001b[32mLogisticRegression\u001b[0m = logreg_3df5de65390e\n",
       "\u001b[36mpipeline\u001b[0m: \u001b[32mPipeline\u001b[0m = pipeline_770b677eee63\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32mPipelineModel\u001b[0m = pipeline_770b677eee63\n",
       "\u001b[36mtest\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mpackage\u001b[0m.\u001b[32mDataFrame\u001b[0m = [id: bigint, text: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Prepare training documents from a list of (id, text, label) tuples.\n",
    "val training = sparkSession.createDataFrame(Seq(\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "    .setNumFeatures(1000)\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.01)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = sparkSession.createDataFrame(Seq(\n",
    "    (4L, \"spark i j k\"),\n",
    "    (5L, \"l m n\"),\n",
    "    (6L, \"mapreduce spark\"),\n",
    "    (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test)\n",
    "    .select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "    .collect()\n",
    "    .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "        println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "<h3>Exercise 1</h3>\n",
    "\n",
    "In the CSV file above, <tt>[SacramentoCrime](http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv)</tt>, the <tt>ucr_ncic_code</tt> represents the type of crime carried out. Use any transformations / actions to output crime types in descending order of frequency. You should create this as a standalone program.\n",
    "\n",
    "<h3>Exercise 2</h3>\n",
    "\n",
    "As well as the \"[TaleOfTwoCities.txt](files/TaleOfTwoCities.txt)\", the files directory contains the file \"[GreatExpectations.txt](files/GreatExpectations.txt)\". Read in both files, and find the top 20 most frequent (overall) words that appear in both documents. (You will need to convert the documents to lower case, but you can assume that ends of line and whitespace indicate word boundaries.)\n",
    "\n",
    "<h3>Exercise 3</h3>\n",
    "\n",
    "There are a [lot of transformers and estimators](http://spark.apache.org/docs/latest/ml-features.html) implemented within Spark that can be pipelined. Create a pipeline which prints n-grams from the [TaleOfTwoCities.txt](files/TaleOfTwoCities.txt) and the [GreatExpectations.txt](files/GreatExpectations.txt) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|  cdatetime|             address|district|      beat|grid|          crimedescr|ucr_ncic_code|   latitude|   longitude|\n",
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "|1/1/06 0:00|  3108 OCCIDENTAL DR|       3|3C        |1115|10851(A)VC TAKE V...|         2404|38.55042047|-121.3914158|\n",
      "|1/1/06 0:00| 2082 EXPEDITION WAY|       5|5A        |1512|459 PC  BURGLARY ...|         2204|38.47350069|-121.4901858|\n",
      "|1/1/06 0:00|          4 PALEN CT|       2|2A        | 212|10851(A)VC TAKE V...|         2404|38.65784584|-121.4621009|\n",
      "|1/1/06 0:00|      22 BECKFORD CT|       6|6C        |1443|476 PC PASS FICTI...|         2501|38.50677377|-121.4269508|\n",
      "|1/1/06 0:00|    3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY-...|         2299| 38.6374478|-121.3846125|\n",
      "|1/1/06 0:00|  5301 BONNIEMAE WAY|       6|6B        |1084|530.5 PC USE PERS...|         2604|38.52697863|-121.4513383|\n",
      "|1/1/06 0:00|       2217 16TH AVE|       4|4A        | 957|459 PC  BURGLARY ...|         2299|  38.537173|-121.4875774|\n",
      "|1/1/06 0:00|           3547 P ST|       3|3C        | 853|484 PC   PETTY TH...|         2308|38.56433456|-121.4618826|\n",
      "|1/1/06 0:00|    3421 AUBURN BLVD|       2|2A        | 508|459 PC  BURGLARY ...|         2203| 38.6374478|-121.3846125|\n",
      "|1/1/06 0:00|   1326 HELMSMAN WAY|       1|1B        | 444|1708 US   THEFT O...|         2310|38.60960217|-121.4918375|\n",
      "|1/1/06 0:00|  2315 STOCKTON BLVD|       6|6B        |1005|ASSAULT WITH WEAP...|         7000|38.55426406|-121.4546045|\n",
      "|1/1/06 0:00|        5112 63RD ST|       6|6B        |1088|530.5 PC USE PERS...|         2604|38.52816497|-121.4314528|\n",
      "|1/1/06 0:00|   6351 DRIFTWOOD ST|       4|4C        |1261|SUSP PERS-NO CRIM...|         7000|38.51092155|-121.5488201|\n",
      "|1/1/06 0:00|7721 COLLEGE TOWN DR|       3|3C        | 888|530.5 PC USE PERS...|         2604|38.55611545|-121.4142729|\n",
      "|1/1/06 0:00|     8460 ROVANA CIR|       6|6C        |1447|484G(B) PC ACCESS...|         2605|38.50398051|-121.3923987|\n",
      "|1/1/06 0:00|       4856 11TH AVE|       6|6B        |1054|487(A) PC GRAND T...|         2303|38.54152896|-121.4495097|\n",
      "|1/1/06 0:00|        6033 69TH ST|       6|6C        |1403|     TELEPEST -I RPT|         7000|38.51657327|-121.4234749|\n",
      "|1/1/06 0:00|            547 L ST|       3|3M        | 742|487(A) GRAND THEF...|         2308|38.58184562|-121.5011657|\n",
      "|1/1/06 0:00|        3525 42ND ST|       6|6A        |1034|530.5 PC USE PERS...|         2604|38.54270763|-121.4572067|\n",
      "|1/1/06 0:00|     5641 DORSET WAY|       4|4C        |1225|484J PC PUBLISH C...|         2605|38.52459987|-121.5203609|\n",
      "+-----------+--------------------+--------+----------+----+--------------------+-------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|ucr_ncic_code|count|\n",
      "+-------------+-----+\n",
      "|         7000| 2470|\n",
      "|         2404|  881|\n",
      "|         2299|  474|\n",
      "|         5400|  357|\n",
      "|         2999|  356|\n",
      "|         2204|  356|\n",
      "|         2399|  262|\n",
      "|         1315|  225|\n",
      "|         2303|  176|\n",
      "|         1299|  146|\n",
      "|         2308|  140|\n",
      "|         2203|  135|\n",
      "|         5401|  133|\n",
      "|         3599|  129|\n",
      "|         5404|  105|\n",
      "|         1313|   99|\n",
      "|         2605|   97|\n",
      "|         2604|   94|\n",
      "|         3532|   66|\n",
      "|         3572|   58|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mcrimes_descending_freq\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Exercise 1\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "object crimes_descending_freq {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "      \n",
    "        val sparkSession = SparkSession.builder\n",
    "          .master(\"local\")\n",
    "          .appName(\"Crimes\")\n",
    "          .getOrCreate()\n",
    "        \n",
    "        \n",
    "    val dfCrime = sparkSession.read.option(\"header\",\"true\").csv(\"files/SacramentocrimeJanuary2006.csv\")\n",
    "     dfCrime.show()\n",
    "    dfCrime.groupBy(\"ucr_ncic_code\").count().orderBy($\"count\".desc).show()\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "crimes_descending_freq.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|value|count(1)|\n",
      "+-----+--------+\n",
      "|  the|      76|\n",
      "|   of|      55|\n",
      "|  and|      40|\n",
      "|   in|      26|\n",
      "|    a|      23|\n",
      "|   to|      23|\n",
      "|  was|      21|\n",
      "|   it|      14|\n",
      "| with|      14|\n",
      "| that|      12|\n",
      "| were|      11|\n",
      "|   by|      11|\n",
      "|  had|      10|\n",
      "|   on|       9|\n",
      "|  his|       9|\n",
      "|   as|       6|\n",
      "|   at|       6|\n",
      "|  for|       6|\n",
      "|  one|       5|\n",
      "|     |       5|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------+\n",
      "|value|count(1)|\n",
      "+-----+--------+\n",
      "|  the|      88|\n",
      "|  and|      85|\n",
      "|    i|      47|\n",
      "|    a|      47|\n",
      "|     |      43|\n",
      "|   to|      41|\n",
      "|   of|      38|\n",
      "| that|      30|\n",
      "|   he|      27|\n",
      "|   my|      24|\n",
      "|   me|      22|\n",
      "|  his|      21|\n",
      "|   in|      21|\n",
      "|  you|      21|\n",
      "|  was|      21|\n",
      "| with|      16|\n",
      "|   as|      16|\n",
      "|   on|      14|\n",
      "| said|      14|\n",
      "|   at|      14|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------------+\n",
      "|Words|(Counts_tale + Counts_age)|\n",
      "+-----+--------------------------+\n",
      "|  the|                       164|\n",
      "|  and|                       125|\n",
      "|   of|                        93|\n",
      "|    a|                        70|\n",
      "|   to|                        64|\n",
      "|     |                        48|\n",
      "|   in|                        47|\n",
      "| that|                        42|\n",
      "|  was|                        42|\n",
      "| with|                        30|\n",
      "|  his|                        30|\n",
      "|   he|                        29|\n",
      "| were|                        25|\n",
      "|   on|                        23|\n",
      "|   it|                        22|\n",
      "|   as|                        22|\n",
      "|   at|                        20|\n",
      "|   by|                        20|\n",
      "|  had|                        15|\n",
      "|  for|                        14|\n",
      "+-----+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mfrequent_20\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Exercise 2\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "\n",
    "object frequent_20 {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "      \n",
    "        val sparkSession = SparkSession.builder\n",
    "          .master(\"local\")\n",
    "          .appName(\"top_20\")\n",
    "          .getOrCreate()\n",
    "        \n",
    "        val tale = sparkSession.read.text(\"files/TaleOfTwoCities.txt\").as[String]\n",
    "        val great = sparkSession.read.text(\"files/GreatExpectations.txt\").as[String]\n",
    "    \n",
    "        val words_tale = tale.flatMap(value => value.split(\"\\\\s+\"))\n",
    "        val words_great = great.flatMap(value => value.split(\"\\\\s+\"))\n",
    "\n",
    "        val grouped_Words_tale = words_tale.groupByKey(_.toLowerCase)\n",
    "        val grouped_Words_great = words_great.groupByKey(_.toLowerCase)\n",
    "    \n",
    "        val counts_tale = grouped_Words_tale.count()\n",
    "        //REQUIRED TO VISUALISE COLUMN NAME: counts_tale.show()\n",
    "        counts_tale.orderBy($\"count(1)\".desc).show()\n",
    "        \n",
    "        val counts_great = grouped_Words_great.count()\n",
    "        //REQUIRED TO VISUALISE COLUMN NAME: counts_great.show()\n",
    "        counts_great.orderBy($\"count(1)\".desc).show()\n",
    "        \n",
    "        \n",
    "        val joined = counts_tale.join(counts_great, Seq(\"value\"))\n",
    "        \n",
    "        val newcolumns = Seq(\"Words\", \"Counts_tale\", \"Counts_age\")\n",
    "        val dataframe = joined.toDF(newcolumns: _*)\n",
    "        val final_df = dataframe.select($\"Words\", $\"Counts_tale\" + $\"Counts_age\")\n",
    "        final_df.orderBy($\"(Counts_tale + Counts_age)\".desc).show()\n",
    "    }\n",
    "}\n",
    "\n",
    "frequent_20.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               value|               words|             ngrams2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|It was the best o...|[it, was, the, be...|[it was, was the,...|\n",
      "|                    |                  []|                  []|\n",
      "|There were a king...|[there, were, a, ...|[there were, were...|\n",
      "|                    |                  []|                  []|\n",
      "|It was the year o...|[it, was, the, ye...|[it was, was the,...|\n",
      "|                    |                  []|                  []|\n",
      "|France, less favo...|[france,, less, f...|[france, less, le...|\n",
      "|                    |                  []|                  []|\n",
      "|In England, there...|[in, england,, th...|[in england,, eng...|\n",
      "|                    |                  []|                  []|\n",
      "|All these things,...|[all, these, thin...|[all these, these...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               value|               words|             ngrams2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|My father’s famil...|[my, father’s, fa...|[my father’s, fat...|\n",
      "|                    |                  []|                  []|\n",
      "|I give Pirrip as ...|[i, give, pirrip,...|[i give, give pir...|\n",
      "|                    |                  []|                  []|\n",
      "|Ours was the mars...|[ours, was, the, ...|[ours was, was th...|\n",
      "|                    |                  []|                  []|\n",
      "|“Hold your noise!...|[“hold, your, noi...|[“hold your, your...|\n",
      "|                    |                  []|                  []|\n",
      "|A fearful man, al...|[a, fearful, man,...|[a fearful, fearf...|\n",
      "|                    |                  []|                  []|\n",
      "|“Oh! Don’t cut my...|[“oh!, don’t, cut...|[“oh! don’t, don’...|\n",
      "|                    |                  []|                  []|\n",
      "|“Tell us your nam...|[“tell, us, your,...|[“tell us, us you...|\n",
      "|                    |                  []|                  []|\n",
      "|         “Pip, sir.”|      [“pip,, sir.”]|       [“pip, sir.”]|\n",
      "|                    |                  []|                  []|\n",
      "|“Once more,” said...|[“once, more,”, s...|[“once more,”, mo...|\n",
      "|                    |                  []|                  []|\n",
      "|    “Pip. Pip, sir.”|[“pip., pip,, sir.”]|[“pip. pip,, pip,...|\n",
      "|                    |                  []|                  []|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36msparkSession.implicits._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.NGram\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mngrams\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Exercise 3\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "import org.apache.spark.ml.feature.NGram\n",
    "\n",
    "\n",
    "object ngrams {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "      \n",
    "        val sparkSession = SparkSession.builder\n",
    "          .master(\"local\")\n",
    "          .appName(\"ngrams\")\n",
    "          .getOrCreate()\n",
    "        \n",
    "        val tale = sparkSession.read.text(\"files/TaleOfTwoCities.txt\").as[String]\n",
    "        val great = sparkSession.read.text(\"files/GreatExpectations.txt\").as[String]\n",
    "    \n",
    "        val tokenizer = new Tokenizer().setInputCol(\"value\").setOutputCol(\"words\")\n",
    "        \n",
    "        val ngram2 = new NGram().setN(2).setInputCol(tokenizer.getOutputCol).setOutputCol(\"ngrams2\")\n",
    "        \n",
    "        val pipeline = new Pipeline().setStages(Array(tokenizer,ngram2))\n",
    "    \n",
    "        val model_tale = pipeline.fit(tale)\n",
    "        val result_tale = model_tale.transform(tale)\n",
    "        result_tale.show()\n",
    "        \n",
    "        val model_great = pipeline.fit(great)\n",
    "        val result_great = model_great.transform(great)\n",
    "        result_great.show()\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "ngrams.main(Array())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
