{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision trees</h1>\n",
    "\n",
    "A [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) can be thought of as a sequence of **hierarchical if-else statements** that test feature values to predict a class.\n",
    "\n",
    "Using [MLlib](https://spark.apache.org/docs/2.0.2/mllib-decision-tree.html) to train a decision tree from data, we want to carry out the following steps:\n",
    "\n",
    "- read dataset\n",
    "- train a decision tree model\n",
    "- measure the training error of the model\n",
    "    \n",
    "Start off with the usual setting up and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147 new artifacts in macro\n",
      "147 new artifacts in runtime\n",
      "147 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.DecisionTree\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.tree.model.DecisionTreeModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.regression.LabeledPoint\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// decision tree imports\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "\n",
    "// importing CSV data into the expected format\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple decision tree example in the cell below reads a dataset, trains a decision tree model and then measures the training error of the model. We use the [Spambase](http://archive.ics.uci.edu/ml/datasets/Spambase) dataset, replicated for Jupyter at \n",
    "\n",
    "    files/spambase.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "17/03/06 16:32:51 INFO SparkContext: Running Spark version 2.0.1\n",
      "17/03/06 16:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/03/06 16:32:52 INFO SecurityManager: Changing view acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/06 16:32:52 INFO SecurityManager: Changing modify acls to: b97eec96efcb40779e247b002e047f82\n",
      "17/03/06 16:32:52 INFO SecurityManager: Changing view acls groups to: \n",
      "17/03/06 16:32:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "17/03/06 16:32:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(b97eec96efcb40779e247b002e047f82); groups with view permissions: Set(); users  with modify permissions: Set(b97eec96efcb40779e247b002e047f82); groups with modify permissions: Set()\n",
      "17/03/06 16:32:52 INFO Utils: Successfully started service 'sparkDriver' on port 39784.\n",
      "17/03/06 16:32:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "17/03/06 16:32:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "17/03/06 16:32:52 INFO DiskBlockManager: Created local directory at /projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/java_temp/blockmgr-44214c3b-a8f0-497b-81d5-716bd95508d8\n",
      "17/03/06 16:32:52 INFO MemoryStore: MemoryStore started with capacity 3.8 GB\n",
      "17/03/06 16:32:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "17/03/06 16:32:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "17/03/06 16:32:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.0.36:4040\n",
      "17/03/06 16:32:53 INFO Executor: Starting executor ID driver on host localhost\n",
      "17/03/06 16:32:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41075.\n",
      "17/03/06 16:32:53 INFO NettyBlockTransferService: Server created on 10.240.0.36:41075\n",
      "17/03/06 16:32:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.0.36, 41075)\n",
      "17/03/06 16:32:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.240.0.36:41075 with 3.8 GB RAM, BlockManagerId(driver, 10.240.0.36, 41075)\n",
      "17/03/06 16:32:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.0.36, 41075)\n",
      "17/03/06 16:32:53 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.\n",
      "17/03/06 16:32:53 INFO SharedState: Warehouse path is '/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/spark-warehouse'.\n",
      "17/03/06 16:32:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.1 KB, free 3.8 GB)\n",
      "17/03/06 16:32:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.3 KB, free 3.8 GB)\n",
      "17/03/06 16:32:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.240.0.36:41075 (size: 14.3 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:54 INFO SparkContext: Created broadcast 0 from textFile at Main.scala:34\n",
      "17/03/06 16:32:54 INFO FileInputFormat: Total input paths to process : 1\n",
      "17/03/06 16:32:54 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:112\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Got job 0 (take at DecisionTreeMetadata.scala:112) with 1 output partitions\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Final stage: ResultStage 0 (take at DecisionTreeMetadata.scala:112)\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Missing parents: List()\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at map at DecisionTreeMetadata.scala:112), which has no missing parents\n",
      "17/03/06 16:32:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.4 KB, free 3.8 GB)\n",
      "17/03/06 16:32:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 3.8 GB)\n",
      "17/03/06 16:32:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.240.0.36:41075 (size: 2.5 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at map at DecisionTreeMetadata.scala:112)\n",
      "17/03/06 16:32:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "17/03/06 16:32:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5466 bytes)\n",
      "17/03/06 16:32:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "17/03/06 16:32:55 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:32:55 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "17/03/06 16:32:55 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "17/03/06 16:32:55 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "17/03/06 16:32:55 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "17/03/06 16:32:55 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "17/03/06 16:32:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 903 bytes result sent to driver\n",
      "17/03/06 16:32:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/1)\n",
      "17/03/06 16:32:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:55 INFO DAGScheduler: ResultStage 0 (take at DecisionTreeMetadata.scala:112) finished in 0.306 s\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Job 0 finished: take at DecisionTreeMetadata.scala:112, took 0.372255 s\n",
      "17/03/06 16:32:55 INFO SparkContext: Starting job: count at DecisionTreeMetadata.scala:116\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Got job 1 (count at DecisionTreeMetadata.scala:116) with 1 output partitions\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Final stage: ResultStage 1 (count at DecisionTreeMetadata.scala:116)\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Missing parents: List()\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at retag at RandomForest.scala:103), which has no missing parents\n",
      "17/03/06 16:32:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.0 KB, free 3.8 GB)\n",
      "17/03/06 16:32:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 3.8 GB)\n",
      "17/03/06 16:32:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.240.0.36:41075 (size: 2.2 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at retag at RandomForest.scala:103)\n",
      "17/03/06 16:32:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "17/03/06 16:32:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5385 bytes)\n",
      "17/03/06 16:32:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "17/03/06 16:32:55 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:32:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1041 bytes result sent to driver\n",
      "17/03/06 16:32:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 203 ms on localhost (1/1)\n",
      "17/03/06 16:32:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:55 INFO DAGScheduler: ResultStage 1 (count at DecisionTreeMetadata.scala:116) finished in 0.204 s\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Job 1 finished: count at DecisionTreeMetadata.scala:116, took 0.303706 s\n",
      "17/03/06 16:32:55 INFO RandomForest: numFeatures: 57\n",
      "17/03/06 16:32:55 INFO RandomForest: numClasses: 2\n",
      "17/03/06 16:32:55 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:894\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Registering RDD 10 (flatMap at RandomForest.scala:887)\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Got job 2 (collectAsMap at RandomForest.scala:894) with 1 output partitions\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Final stage: ResultStage 3 (collectAsMap at RandomForest.scala:894)\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at flatMap at RandomForest.scala:887), which has no missing parents\n",
      "17/03/06 16:32:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.0 KB, free 3.8 GB)\n",
      "17/03/06 16:32:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.2 KB, free 3.8 GB)\n",
      "17/03/06 16:32:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.240.0.36:41075 (size: 4.2 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at flatMap at RandomForest.scala:887)\n",
      "17/03/06 16:32:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "17/03/06 16:32:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5573 bytes)\n",
      "17/03/06 16:32:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "17/03/06 16:32:55 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:32:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1328 bytes result sent to driver\n",
      "17/03/06 16:32:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 708 ms on localhost (1/1)\n",
      "17/03/06 16:32:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:56 INFO DAGScheduler: ShuffleMapStage 2 (flatMap at RandomForest.scala:887) finished in 0.709 s\n",
      "17/03/06 16:32:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:32:56 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:32:56 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
      "17/03/06 16:32:56 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:32:56 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[12] at map at RandomForest.scala:889), which has no missing parents\n",
      "17/03/06 16:32:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.8 KB, free 3.8 GB)\n",
      "17/03/06 16:32:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.4 KB, free 3.8 GB)\n",
      "17/03/06 16:32:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.240.0.36:41075 (size: 5.4 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at map at RandomForest.scala:889)\n",
      "17/03/06 16:32:56 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\n",
      "17/03/06 16:32:56 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:32:56 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "17/03/06 16:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "17/03/06 16:32:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 35335 bytes result sent to driver\n",
      "17/03/06 16:32:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1006 ms on localhost (1/1)\n",
      "17/03/06 16:32:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:57 INFO DAGScheduler: ResultStage 3 (collectAsMap at RandomForest.scala:894) finished in 1.007 s\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Job 2 finished: collectAsMap at RandomForest.scala:894, took 1.890031 s\n",
      "17/03/06 16:32:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 40.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 101.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.240.0.36:41075 (size: 101.0 B, free: 3.8 GB)\n",
      "17/03/06 16:32:57 INFO SparkContext: Created broadcast 5 from broadcast at RandomForest.scala:500\n",
      "17/03/06 16:32:57 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:550\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Registering RDD 15 (mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Got job 3 (collectAsMap at RandomForest.scala:550) with 1 output partitions\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Final stage: ResultStage 5 (collectAsMap at RandomForest.scala:550)\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at mapPartitions at RandomForest.scala:521), which has no missing parents\n",
      "17/03/06 16:32:57 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 54.6 KB, free 3.8 GB)\n",
      "17/03/06 16:32:57 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.5 KB, free 3.8 GB)\n",
      "17/03/06 16:32:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.240.0.36:41075 (size: 16.5 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:57 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:57 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "17/03/06 16:32:58 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5464 bytes)\n",
      "17/03/06 16:32:58 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "17/03/06 16:32:58 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.240.0.36:41075 in memory (size: 2.5 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 1012.7 KB, free 3.8 GB)\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Added rdd_14_0 in memory on 10.240.0.36:41075 (size: 1012.7 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.240.0.36:41075 in memory (size: 2.2 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO ContextCleaner: Cleaned shuffle 0\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.240.0.36:41075 in memory (size: 4.2 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.240.0.36:41075 in memory (size: 5.4 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2287 bytes result sent to driver\n",
      "17/03/06 16:32:58 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 773 ms on localhost (1/1)\n",
      "17/03/06 16:32:58 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:58 INFO DAGScheduler: ShuffleMapStage 4 (mapPartitions at RandomForest.scala:521) finished in 0.774 s\n",
      "17/03/06 16:32:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:32:58 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:32:58 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
      "17/03/06 16:32:58 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:32:58 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at map at RandomForest.scala:540), which has no missing parents\n",
      "17/03/06 16:32:58 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.3 KB, free 3.8 GB)\n",
      "17/03/06 16:32:58 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.1 KB, free 3.8 GB)\n",
      "17/03/06 16:32:58 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.240.0.36:41075 (size: 11.1 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:58 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at map at RandomForest.scala:540)\n",
      "17/03/06 16:32:58 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "17/03/06 16:32:58 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:32:58 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "17/03/06 16:32:58 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:32:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "17/03/06 16:32:58 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2386 bytes result sent to driver\n",
      "17/03/06 16:32:58 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 115 ms on localhost (1/1)\n",
      "17/03/06 16:32:58 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:58 INFO DAGScheduler: ResultStage 5 (collectAsMap at RandomForest.scala:550) finished in 0.117 s\n",
      "17/03/06 16:32:58 INFO DAGScheduler: Job 3 finished: collectAsMap at RandomForest.scala:550, took 1.208138 s\n",
      "17/03/06 16:32:58 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 40.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 101.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.240.0.36:41075 (size: 101.0 B, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 8 from broadcast at RandomForest.scala:500\n",
      "17/03/06 16:32:59 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:550\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Registering RDD 18 (mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Got job 4 (collectAsMap at RandomForest.scala:550) with 1 output partitions\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Final stage: ResultStage 7 (collectAsMap at RandomForest.scala:550)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[18] at mapPartitions at RandomForest.scala:521), which has no missing parents\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 55.3 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 16.8 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.240.0.36:41075 (size: 16.8 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[18] at mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5464 bytes)\n",
      "17/03/06 16:32:59 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "17/03/06 16:32:59 INFO BlockManager: Found block rdd_14_0 locally\n",
      "17/03/06 16:32:59 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1486 bytes result sent to driver\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 59 ms on localhost (1/1)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:59 INFO DAGScheduler: ShuffleMapStage 6 (mapPartitions at RandomForest.scala:521) finished in 0.060 s\n",
      "17/03/06 16:32:59 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:32:59 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:32:59 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at map at RandomForest.scala:540), which has no missing parents\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 36.8 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 11.4 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.240.0.36:41075 (size: 11.4 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at map at RandomForest.scala:540)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:32:59 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "17/03/06 16:32:59 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:32:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "17/03/06 16:32:59 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2458 bytes result sent to driver\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 14 ms on localhost (1/1)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:59 INFO DAGScheduler: ResultStage 7 (collectAsMap at RandomForest.scala:550) finished in 0.015 s\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Job 4 finished: collectAsMap at RandomForest.scala:550, took 0.433545 s\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 40.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 101.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.240.0.36:41075 (size: 101.0 B, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 11 from broadcast at RandomForest.scala:500\n",
      "17/03/06 16:32:59 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:550\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Registering RDD 21 (mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Got job 5 (collectAsMap at RandomForest.scala:550) with 1 output partitions\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsMap at RandomForest.scala:550)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[21] at mapPartitions at RandomForest.scala:521), which has no missing parents\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 55.8 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 17.0 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.240.0.36:41075 (size: 17.0 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[21] at mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5464 bytes)\n",
      "17/03/06 16:32:59 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "17/03/06 16:32:59 INFO BlockManager: Found block rdd_14_0 locally\n",
      "17/03/06 16:32:59 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1486 bytes result sent to driver\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 13 ms on localhost (1/1)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:59 INFO DAGScheduler: ShuffleMapStage 8 (mapPartitions at RandomForest.scala:521) finished in 0.015 s\n",
      "17/03/06 16:32:59 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:32:59 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:32:59 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[23] at map at RandomForest.scala:540), which has no missing parents\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 37.0 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 11.4 KB, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.240.0.36:41075 (size: 11.4 KB, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[23] at map at RandomForest.scala:540)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:32:59 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "17/03/06 16:32:59 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:32:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "17/03/06 16:32:59 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2778 bytes result sent to driver\n",
      "17/03/06 16:32:59 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 109 ms on localhost (1/1)\n",
      "17/03/06 16:32:59 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:32:59 INFO DAGScheduler: ResultStage 9 (collectAsMap at RandomForest.scala:550) finished in 0.111 s\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Job 5 finished: collectAsMap at RandomForest.scala:550, took 0.399318 s\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 101.0 B, free 3.8 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.240.0.36:41075 (size: 101.0 B, free: 3.8 GB)\n",
      "17/03/06 16:32:59 INFO SparkContext: Created broadcast 14 from broadcast at RandomForest.scala:500\n",
      "17/03/06 16:32:59 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:550\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Registering RDD 24 (mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Got job 6 (collectAsMap at RandomForest.scala:550) with 1 output partitions\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Final stage: ResultStage 11 (collectAsMap at RandomForest.scala:550)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
      "17/03/06 16:32:59 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[24] at mapPartitions at RandomForest.scala:521), which has no missing parents\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 56.7 KB, free 3.7 GB)\n",
      "17/03/06 16:32:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 17.4 KB, free 3.7 GB)\n",
      "17/03/06 16:32:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.240.0.36:41075 (size: 17.4 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[24] at mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, PROCESS_LOCAL, 5464 bytes)\n",
      "17/03/06 16:33:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "17/03/06 16:33:00 INFO BlockManager: Found block rdd_14_0 locally\n",
      "17/03/06 16:33:00 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1486 bytes result sent to driver\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 13 ms on localhost (1/1)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:00 INFO DAGScheduler: ShuffleMapStage 10 (mapPartitions at RandomForest.scala:521) finished in 0.014 s\n",
      "17/03/06 16:33:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:33:00 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:33:00 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[26] at map at RandomForest.scala:540), which has no missing parents\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 37.2 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.5 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.240.0.36:41075 (size: 11.5 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:00 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at map at RandomForest.scala:540)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:33:00 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "17/03/06 16:33:00 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:33:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "17/03/06 16:33:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3258 bytes result sent to driver\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 114 ms on localhost (1/1)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:00 INFO DAGScheduler: ResultStage 11 (collectAsMap at RandomForest.scala:550) finished in 0.115 s\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Job 6 finished: collectAsMap at RandomForest.scala:550, took 0.324097 s\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 40.0 B, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 101.0 B, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.240.0.36:41075 (size: 101.0 B, free: 3.8 GB)\n",
      "17/03/06 16:33:00 INFO SparkContext: Created broadcast 17 from broadcast at RandomForest.scala:500\n",
      "17/03/06 16:33:00 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:550\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Registering RDD 27 (mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Got job 7 (collectAsMap at RandomForest.scala:550) with 1 output partitions\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Final stage: ResultStage 13 (collectAsMap at RandomForest.scala:550)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[27] at mapPartitions at RandomForest.scala:521), which has no missing parents\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 58.3 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.9 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.240.0.36:41075 (size: 17.9 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:00 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[27] at mapPartitions at RandomForest.scala:521)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, partition 0, PROCESS_LOCAL, 5464 bytes)\n",
      "17/03/06 16:33:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
      "17/03/06 16:33:00 INFO BlockManager: Found block rdd_14_0 locally\n",
      "17/03/06 16:33:00 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1486 bytes result sent to driver\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 13 ms on localhost (1/1)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: ShuffleMapStage 12 (mapPartitions at RandomForest.scala:521) finished in 0.014 s\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "17/03/06 16:33:00 INFO DAGScheduler: running: Set()\n",
      "17/03/06 16:33:00 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "17/03/06 16:33:00 INFO DAGScheduler: failed: Set()\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[29] at map at RandomForest.scala:540), which has no missing parents\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 37.5 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 11.6 KB, free 3.7 GB)\n",
      "17/03/06 16:33:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.240.0.36:41075 (size: 11.6 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:00 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[29] at map at RandomForest.scala:540)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, partition 0, ANY, 5188 bytes)\n",
      "17/03/06 16:33:00 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
      "17/03/06 16:33:00 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "17/03/06 16:33:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "17/03/06 16:33:00 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 3737 bytes result sent to driver\n",
      "17/03/06 16:33:00 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 50 ms on localhost (1/1)\n",
      "17/03/06 16:33:00 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:00 INFO DAGScheduler: ResultStage 13 (collectAsMap at RandomForest.scala:550) finished in 0.051 s\n",
      "17/03/06 16:33:00 INFO DAGScheduler: Job 7 finished: collectAsMap at RandomForest.scala:550, took 0.417094 s\n",
      "17/03/06 16:33:00 INFO MapPartitionsRDD: Removing RDD 14 from persistence list\n",
      "17/03/06 16:33:00 INFO BlockManager: Removing RDD 14\n",
      "17/03/06 16:33:00 INFO RandomForest: Internal timing for DecisionTree:\n",
      "17/03/06 16:33:00 INFO RandomForest:   init: 3.07133853\n",
      "  total: 6.272185944\n",
      "  findSplits: 2.087478783\n",
      "  findBestSplits: 3.186335317\n",
      "  chooseSplits: 3.178897021\n",
      "17/03/06 16:33:01 INFO SparkContext: Starting job: count at Main.scala:68\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Got job 8 (count at Main.scala:68) with 1 output partitions\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Final stage: ResultStage 14 (count at Main.scala:68)\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Missing parents: List()\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[31] at filter at Main.scala:68), which has no missing parents\n",
      "17/03/06 16:33:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 12.2 KB, free 3.8 GB)\n",
      "17/03/06 16:33:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.1 KB, free 3.8 GB)\n",
      "17/03/06 16:33:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.240.0.36:41075 (size: 6.1 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:01 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[31] at filter at Main.scala:68)\n",
      "17/03/06 16:33:01 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks\n",
      "17/03/06 16:33:01 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, PROCESS_LOCAL, 5385 bytes)\n",
      "17/03/06 16:33:01 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "17/03/06 16:33:01 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:33:01 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 954 bytes result sent to driver\n",
      "17/03/06 16:33:01 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 39 ms on localhost (1/1)\n",
      "17/03/06 16:33:01 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:01 INFO DAGScheduler: ResultStage 14 (count at Main.scala:68) finished in 0.040 s\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Job 8 finished: count at Main.scala:68, took 0.142565 s\n",
      "17/03/06 16:33:01 INFO SparkContext: Starting job: count at Main.scala:68\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Got job 9 (count at Main.scala:68) with 1 output partitions\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Final stage: ResultStage 15 (count at Main.scala:68)\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Missing parents: List()\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[5] at randomSplit at Main.scala:40), which has no missing parents\n",
      "17/03/06 16:33:01 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 3.5 KB, free 3.8 GB)\n",
      "17/03/06 16:33:01 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.1 KB, free 3.8 GB)\n",
      "17/03/06 16:33:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.240.0.36:41075 (size: 2.1 KB, free: 3.8 GB)\n",
      "17/03/06 16:33:01 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1012\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[5] at randomSplit at Main.scala:40)\n",
      "17/03/06 16:33:01 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.08931185944363104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/03/06 16:33:01 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, partition 0, PROCESS_LOCAL, 5385 bytes)\n",
      "17/03/06 16:33:01 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
      "17/03/06 16:33:01 INFO HadoopRDD: Input split: file:/projects/b97eec96-efcb-4077-9e24-7b002e047f82/Scalable-ML/week4/files/spambase.data:0+698341\n",
      "17/03/06 16:33:01 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 954 bytes result sent to driver\n",
      "17/03/06 16:33:01 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 33 ms on localhost (1/1)\n",
      "17/03/06 16:33:01 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "17/03/06 16:33:01 INFO DAGScheduler: ResultStage 15 (count at Main.scala:68) finished in 0.033 s\n",
      "17/03/06 16:33:01 INFO DAGScheduler: Job 9 finished: count at Main.scala:68, took 0.194443 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 5 with 49 nodes\n",
      "  If (feature 51 <= 0.057)\n",
      "   If (feature 6 <= 0.04)\n",
      "    If (feature 23 <= 0.0)\n",
      "     If (feature 15 <= 0.1)\n",
      "      If (feature 52 <= 0.182)\n",
      "       Predict: 0.0\n",
      "      Else (feature 52 > 0.182)\n",
      "       Predict: 1.0\n",
      "     Else (feature 15 > 0.1)\n",
      "      If (feature 4 <= 1.07)\n",
      "       Predict: 0.0\n",
      "      Else (feature 4 > 1.07)\n",
      "       Predict: 1.0\n",
      "    Else (feature 23 > 0.0)\n",
      "     If (feature 24 <= 0.05)\n",
      "      If (feature 55 <= 9.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 55 > 9.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 24 > 0.05)\n",
      "      Predict: 0.0\n",
      "   Else (feature 6 > 0.04)\n",
      "    If (feature 26 <= 0.0)\n",
      "     If (feature 24 <= 0.26)\n",
      "      If (feature 49 <= 0.375)\n",
      "       Predict: 1.0\n",
      "      Else (feature 49 > 0.375)\n",
      "       Predict: 0.0\n",
      "     Else (feature 24 > 0.26)\n",
      "      If (feature 25 <= 0.29)\n",
      "       Predict: 0.0\n",
      "      Else (feature 25 > 0.29)\n",
      "       Predict: 1.0\n",
      "    Else (feature 26 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Else (feature 51 > 0.057)\n",
      "   If (feature 52 <= 0.0)\n",
      "    If (feature 6 <= 0.0)\n",
      "     If (feature 15 <= 0.17)\n",
      "      If (feature 54 <= 3.272)\n",
      "       Predict: 0.0\n",
      "      Else (feature 54 > 3.272)\n",
      "       Predict: 1.0\n",
      "     Else (feature 15 > 0.17)\n",
      "      If (feature 51 <= 0.49)\n",
      "       Predict: 1.0\n",
      "      Else (feature 51 > 0.49)\n",
      "       Predict: 1.0\n",
      "    Else (feature 6 > 0.0)\n",
      "     If (feature 45 <= 0.0)\n",
      "      If (feature 24 <= 0.09)\n",
      "       Predict: 1.0\n",
      "      Else (feature 24 > 0.09)\n",
      "       Predict: 0.0\n",
      "     Else (feature 45 > 0.0)\n",
      "      Predict: 0.0\n",
      "   Else (feature 52 > 0.0)\n",
      "    If (feature 24 <= 0.61)\n",
      "     If (feature 45 <= 0.33)\n",
      "      If (feature 26 <= 0.2)\n",
      "       Predict: 1.0\n",
      "      Else (feature 26 > 0.2)\n",
      "       Predict: 0.0\n",
      "     Else (feature 45 > 0.33)\n",
      "      If (feature 8 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 8 > 0.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 24 > 0.61)\n",
      "     If (feature 6 <= 0.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 6 > 0.0)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[0m: \u001b[32mSparkSession\u001b[0m = org.apache.spark.sql.SparkSession@626e290b\n",
       "\u001b[36mtext\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = files/spambase.data MapPartitionsRDD[1] at textFile at Main.scala:34\n",
       "\u001b[36mdata\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[3] at map at Main.scala:37\n",
       "\u001b[36msplits\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m]] = \u001b[33mArray\u001b[0m(\n",
       "  MapPartitionsRDD[4] at randomSplit at Main.scala:40,\n",
       "  MapPartitionsRDD[5] at randomSplit at Main.scala:40\n",
       ")\n",
       "\u001b[36mtrainingData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[4] at randomSplit at Main.scala:40\n",
       "\u001b[36mtestData\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mLabeledPoint\u001b[0m] = MapPartitionsRDD[5] at randomSplit at Main.scala:40\n",
       "\u001b[36mnumClasses\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m2\u001b[0m\n",
       "\u001b[36mcategoricalFeaturesInfo\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m()\n",
       "\u001b[36mimpurity\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"gini\"\u001b[0m\n",
       "\u001b[36mmaxDepth\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m5\u001b[0m\n",
       "\u001b[36mmaxBins\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m32\u001b[0m\n",
       "\u001b[36mmodel\u001b[0m: \u001b[32mDecisionTreeModel\u001b[0m = DecisionTreeModel classifier of depth 5 with 49 nodes\n",
       "\u001b[36mlabelAndPreds\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mDouble\u001b[0m, \u001b[32mDouble\u001b[0m)] = MapPartitionsRDD[30] at map at Main.scala:62\n",
       "\u001b[36mtestErr\u001b[0m: \u001b[32mDouble\u001b[0m = \u001b[32m0.08931185944363104\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Create Spark session\n",
    "val sparkSession = SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"Decision Tree example\")\n",
    "    .getOrCreate()\n",
    "\n",
    "// Load the data\n",
    "val text = sparkSession.sparkContext.textFile(\"files/spambase.data\")\n",
    "\n",
    "// Separate into array\n",
    "val data = text.map(line => line.split(',').map(_.toDouble)).map(t => LabeledPoint(t(57), Vectors.dense(t.take(57))))\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "\n",
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification tree model:\\n\" + model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of information is contained in the output: in this case, the model is a classifier of depth 1 with 3 nodes. The structure of the decision tree is also printed\n",
    "\n",
    "    If (feature 434 <= 0.0)\n",
    "        Predict: 0.0\n",
    "    Else (feature 434 > 0.0)\n",
    "        Predict: 1.0\n",
    "        \n",
    "The example above contains the variable <tt>impurity</tt>. The node [impurity](https://spark.apache.org/docs/2.0.2/mllib-decision-tree.html#node-impurity-and-information-gain) is the measure of homogeneity of the labels at the node. The current implmentation includes two impurity measures for classification: Gini impurity and entropy, invoked by passing the relevant value (<tt>gini</tt> or <tt>entropy</tt>) to the classifier.\n",
    "\n",
    "<h1>Exercises</h1>\n",
    "\n",
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Make the decision tree code a standalone program to run on HPC. Make <tt>impurity</tt> value an argument to the program (with the values <tt>gini</tt> or <tt>entropy</tt>). Run this on the [default of credit cards](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) datase. Note that this dataset has a different format to the Spambase dataset above - you will need to convert from XLS format to, say, CSV, before using the data. You can use any available tool for this: for example, Excell has an export option, or there is a command line tool <tt>xls2csv</tt> available on Linux.\n",
    "\n",
    "<h2>Exercise 2</h2>\n",
    "\n",
    "Modify your program to run the decision tree as part of a pipeline (see Notebook 3 for a refresher on pipelines). The pipeline model can be used to find the best set of parameters using cross validation. An example of a cross-validator can be found [here](http://spark.apache.org/docs/2.1.0/ml-tuning.html#cross-validation). In your case, make <tt>paramGrid</tt> contain different values for <tt>maxDepth</tt>, <tt>maxBins</tt> and <tt>impurity</tt> and find the best parameters, and associated test error, for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
