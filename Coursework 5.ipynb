{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# COM6012 - 2017: Coursework 5\n",
    "\n",
    "Deadline: 11:59PM on Thursday 11 May 2017\n",
    "\n",
    "Submission: via SageMatchCloud (We will collect from you automatically). \n",
    "Please upload everything in a zip file to SageMathCloud as instructed in Coursework 1 **EXCEPT the downloaded data**. <br />\n",
    "Check the filesize of the zip file uploaded to make sure it is not too big.<br />\n",
    "\n",
    "\n",
    "## Exercise 1 (7 Marks)\n",
    "\n",
    "We use the [KDDCUP1999](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+Data) data with 4M points to study K-means. Note that the data are labelled so you need to ignore the labels when doing K-means. \n",
    "\n",
    "Step 1: Use K-means to cluster all data points in the KDDCUP1999 data with K=500 and K=1000. Answer the followign questions for both K values.\n",
    "\n",
    "Step 2: What are the sizes (number of points) and centroids of the 3 largest clusters?\n",
    "\n",
    "Step 3: What are the sizes (number of points) and centroids of the 3 smallest clusters?\n",
    "\n",
    "Step 4. What are the respective SSEs for the two K values?\n",
    "\n",
    "Step 5. Find the majority labels for the 3 largest clusters and the 3 smallest clusters (one majority label for each cluster). \n",
    "\n",
    "Step 6: Briefly describe 3-5 interesting findings.\n",
    "\n",
    "\n",
    "## Exercise 2 (6 Marks)\n",
    "\n",
    "We use the [MovieLens 10M Dataset](http://grouplens.org/datasets/movielens/).\n",
    "\n",
    "Step 1: Run ./split_ratings.sh to get the five splits (r1 to r5) for five-fold cross-validation. See [ReadMe](file:///home/haiping/Downloads/ml-10M100K/README.html).\n",
    "\n",
    "Step 2: Run ALS with default settings on the five splits and report the **mean** and **variance** of the MSE from five-fold corss-validation.\n",
    "\n",
    "Step 3: Run **nonnegative** ALS on the five splits and report the **mean** and **variance** of the MSE from five-fold corss-validation.\n",
    "\n",
    "Step 4: Use (Reuse above) K-means to cluster the movie factors (itemFactors) learned with default settings (Step 2) for each of the five splits and find the majority movie Genres for the 3 largest clusters. Explain your choice of K in K-means and report the **majority genre** and **the number of movies of this genre** for each of the 3 largest clusters in each split (referring to movies.dat).\n",
    "\n",
    "Step 5: Briefly describe 3-5 interesting findings.\n",
    "\n",
    "\n",
    "\n",
    "# Answers:\n",
    "## Exercise 1 Answers:\n",
    "### FILES:\n",
    "|      File     |    Ouput File for K = 500    |   Output File for K = 1000   |\n",
    "|---------------|------------------------------|------------------------------|\n",
    "|exercise1.scala|acp16al_Coursework4.sh.o405587|acp16al_Coursework4.sh.o405588|\n",
    "\n",
    "#### STEP 2: Sizes (number of points) and centroids of the 3 largest clusters\n",
    "> ##### For K = 500:\n",
    "\n",
    "> \n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|         0|2896623|\n",
    "|        28|1629247|\n",
    "|       420| 206719|\n",
    "\n",
    ">  Centroid for 1st largest cluster (cluster 0) = \n",
    "[0.030703710710670625,0.10297149611888194,1.235936902314033E-4,0.06981317329216305,957.8892887079558,9.18419\n",
    "3955094752,0.0,8.941554684338954E-5,0.0,5.523740345537577E-5,0.0,0.02969873520155438,0.0,0.0,2.0714026295765917E-6,7.836806615231437E-5,4.639\n",
    "941890251565E-4,1.1634378102788522E-4,3.4972181062684787E-4,0.0,0.0,0.0,491.5614857894875,491.57665743281405,2.270257282015946E-5,2.908939759\n",
    "4687244E-5,4.139352921437222E-6,6.635393090077014E-6,0.9994347660074543,9.626118730036884E-4,0.012411999911619898,250.9146257596869,251.01594\n",
    "462174117,0.9880440228296189,0.0021887165175035537,0.9713275205172428,3.9303138727333716E-4,2.2649061585560225E-4,7.317229788979105E-5,2.5756\n",
    "85599746893E-4,3.773750357317037E-5]\n",
    " \n",
    "> Centroid for 2nd largest cluster (cluster 28) = \n",
    "[1.103258883348336,2.7543279265932634,0.8949180894822432,139.80376182545544,57.4655078108712,88.654433641320\n",
    "48,1.7169445255223805E-5,0.0017899146678570816,1.8395834202025505E-6,0.006999001719397303,7.174375338789946E-5,0.15242297664219612,6.68381976\n",
    "0069267E-5,1.2263889468017003E-6,7.971528154211052E-6,0.01488774861969924,1.2202570020676918E-4,7.971528154211052E-6,1.8027917517984994E-4,0.\n",
    "0,0.0,0.0012901611720353887,131.09460916210654,11.157475699103019,0.5340713966853143,0.5341724817942842,0.17281237352864154,0.172516930299416\n",
    "67,0.37014218140243305,0.06156671801170769,0.04043081204117576,224.6520360509295,68.38353658686145,0.2891237818889998,0.08735456866710667,0.0\n",
    "7678718595224444,0.013588812634774648,0.5340835440679954,0.5338884439823554,0.17212930554505212,0.17101623493693857]\n",
    " \n",
    "> Centroid for 3rd largest cluster (cluster 420) = \n",
    "[1.103258883348336,2.7543279265932634,0.8949180894822432,139.80376182545544,57.4655078108712,88.654433641320\n",
    "48,1.7169445255223805E-5,0.0017899146678570816,1.8395834202025505E-6,0.006999001719397303,7.174375338789946E-5,0.15242297664219612,6.68381976\n",
    "0069267E-5,1.2263889468017003E-6,7.971528154211052E-6,0.01488774861969924,1.2202570020676918E-4,7.971528154211052E-6,1.8027917517984994E-4,0.\n",
    "0,0.0,0.0012901611720353887,131.09460916210654,11.157475699103019,0.5340713966853143,0.5341724817942842,0.17281237352864154,0.172516930299416\n",
    "67,0.37014218140243305,0.06156671801170769,0.04043081204117576,224.6520360509295,68.38353658686145,0.2891237818889998,0.08735456866710667,0.0\n",
    "7678718595224444,0.013588812634774648,0.5340835440679954,0.5338884439823554,0.17212930554505212,0.17101623493693857]\n",
    "\n",
    "> ##### For K = 1000:\n",
    "\n",
    "> \n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|         0|2895975|\n",
    "|        29|1582684|\n",
    "|        30| 202257|\n",
    "\n",
    "> Centroid for 1st largest cluster (cluster 0) = \n",
    "[0.03048013428552387,0.1015394422288745,1.1533471758015331E-4,0.06996190155871072,956.5039602281694,9.197095\n",
    "636998135,0.0,8.943620315347218E-5,0.0,5.559547763594217E-5,0.0,0.029535356479238543,0.0,0.0,2.0718811541344907E-6,7.873148385711064E-5,4.672\n",
    "0920025732765E-4,1.1637065815722056E-4,3.5014791504872893E-4,0.0,0.0,0.0,491.6708754630223,491.6860595893739,2.2217472242835522E-5,2.82949902\n",
    "94963336E-5,4.312965935856631E-6,6.809582726588692E-6,0.99944067496363,9.575475004653125E-4,0.012422319132544548,250.93890678572157,251.05908\n",
    "03817372,0.9881884886973704,0.0021756996656337887,0.9714448366822411,3.8846045072369527E-4,2.2554152930380597E-4,7.203930772925447E-5,2.55987\n",
    "82286381837E-4,3.77669403046157E-5]\n",
    " \n",
    "> Centroid for 2nd largest cluster (cluster 29) = \n",
    "[1.100513096193548,2.75745036873268,0.9191299322097207,62.842397696240994,52.19266345634518,67.8895265785342\n",
    "6,1.7666863526003097E-5,0.001841770522585823,1.8928782349289035E-6,0.0030286051758862454,7.066745410401239E-5,0.13415585202235109,6.688169763\n",
    "415459E-5,1.2619188232859356E-6,8.202472351358582E-6,0.015315277798809757,1.1862036938887795E-4,8.202472351358582E-6,1.7729959467167395E-4,0.\n",
    "0,0.0,6.038281569423202E-4,134.6538253807209,11.181855121623736,0.5495202941785146,0.5496161242939829,0.17724971733018519,0.1769087784121073,\n",
    "0.35250492148342105,0.06271723932565788,0.03953897688669311,226.35950111301239,64.87788979410531,0.27543869976915,0.08532846485075382,0.07212\n",
    "689477098759,0.013536716790104781,0.549532932295687,0.5493367859433477,0.17650243424146345,0.1752527055540126]\n",
    " \n",
    "> Centroid for 3rd largest cluster (cluster 30) = \n",
    "[1.100513096193548,2.75745036873268,0.9191299322097207,62.842397696240994,52.19266345634518,67.8895265785342\n",
    "6,1.7666863526003097E-5,0.001841770522585823,1.8928782349289035E-6,0.0030286051758862454,7.066745410401239E-5,0.13415585202235109,6.688169763\n",
    "415459E-5,1.2619188232859356E-6,8.202472351358582E-6,0.015315277798809757,1.1862036938887795E-4,8.202472351358582E-6,1.7729959467167395E-4,0.\n",
    "0,0.0,6.038281569423202E-4,134.6538253807209,11.181855121623736,0.5495202941785146,0.5496161242939829,0.17724971733018519,0.1769087784121073,\n",
    "0.35250492148342105,0.06271723932565788,0.03953897688669311,226.35950111301239,64.87788979410531,0.27543869976915,0.08532846485075382,0.07212\n",
    "689477098759,0.013536716790104781,0.549532932295687,0.5493367859433477,0.17650243424146345,0.1752527055540126]\n",
    "\n",
    "#### STEP 3: Sizes (number of points) and centroids of the 3 smallest clusters\n",
    "> ##### For K=500:\n",
    "\n",
    "\n",
    "> \n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       243|     1 |\n",
    "|       148|     1 |\n",
    "|       392|     1 |\n",
    " \n",
    "> Centroid for 1st smallest cluster (cluster 243) =  [1.0,11.0,0.0,43.0,614.0,1511099.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,201.0,33.0,0.16,0.02,0.0,0.06,0.0,0.0,0.0,0.0]\n",
    " \n",
    "> Centroid for 2nd smallest cluster (cluster 148) =  [1.0,6.0,0.0,2.0,2194619.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,126.0,43.0,0.12,0.06,0.12,0.05,0.0,0.0,0.0,0.0]\n",
    " \n",
    "> Centroid for 3rd smallest cluster (cluster 392) =  [1.0,6.0,0.0,2.0,2194619.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,207.0,67.0,0.32,0.02,0.32,0.0,0.0,0.0,0.05,0.0]\n",
    "\n",
    "> ##### For K=1000:\n",
    "\n",
    "> \n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       148|     1 |\n",
    "|       463|     1 |\n",
    "|       471|     1 |\n",
    "\n",
    " \n",
    "> Centroid for 1st smallest cluster (cluster 148) =  [1.0,11.0,0.0,44.0,858.0,1408161.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,80.0,16.0,0.2,0.06,0.01,0.0,0.0,0.0,0.0,0.0]\n",
    " \n",
    "> Centroid for 2nd smallest cluster (cluster 463) = [1.0,6.0,0.0,3.0,2194619.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,191.0,72.0,0.38,0.02,0.38,0.0,0.0,0.0,0.0,0.0]\n",
    " \n",
    "> Centroid for 3rd smallest cluster (cluster 471) =  [1.0,6.0,0.0,2.0,2194619.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,115.0,104.0,0.36,0.05,0.36,0.02,0.0,0.0,0.0,0.0]\n",
    "\n",
    "#### STEP 4: SSE\n",
    "> ##### For K=500:\n",
    "> SSE = 5.198580227960735E12\n",
    "\n",
    "> ##### For K=1000:\n",
    "> SSE = 1.9529989472017983E12\n",
    "\n",
    "#### STEP 5.1: Majority labels for the 3 largest clusters\n",
    "> ##### For K=500:\n",
    ">\n",
    "| cluster  | label |\n",
    "|----------|-------|\n",
    "|         0| smurf |\n",
    "|        28|neptune|\n",
    "|       420| normal|\n",
    "\n",
    "> ##### For K=1000:\n",
    ">\n",
    "| cluster  | label |\n",
    "|----------|-------|\n",
    "|         0| smurf |\n",
    "|        29|neptune|\n",
    "|        30| normal|\n",
    "\n",
    "#### STEP 5.2: Majority labels for the 3 smallest clusters\n",
    "> ##### For K=500:\n",
    ">\n",
    "| cluster  | label |\n",
    "|----------|-------|\n",
    "|       243| normal|\n",
    "|       148| normal|\n",
    "|       392| normal|\n",
    "\n",
    "> ##### For K=1000:\n",
    ">\n",
    "| cluster  | label |\n",
    "|----------|-------|\n",
    "|       148| normal|\n",
    "|       463| normal|\n",
    "|       471| normal|\n",
    "\n",
    "#### STEP 6: Interesting Things\n",
    "\n",
    "> For both K=500 and K=1000, the biggest cluster is the same (cluster id 0)\n",
    "\n",
    "> For both K=500 and K=1000, the labels for the 3 largest clusters are the same.\n",
    "\n",
    "> For both K=500 and K=1000, the labels for the 3 smallest clusters are the same.\n",
    "\n",
    "> SSE for K=1000 is less than half of SSE for K=500.\n",
    "\n",
    "> Both for K=500 and for K=1000, not only 3 clusters have size 1 (smallest clusters).\n",
    "\n",
    "> Execution time for K=500 is 11648.952313849 seconds. Execution time for K=1000 is 12048.876130864 seconds. Therefore number of K does not affect execution time a lot (since execution time is almost the same for both values for K)\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 2 Answers:\n",
    "### FILES for Steps 2 & 3:\n",
    "|      File     | Ouput File for Default ALS   |Output File for **nonnegative** ALS|\n",
    "|---------------|------------------------------|-----------------------------------|\n",
    "|exercise2.scala|acp16al_Coursework4.sh.o405668|  acp16al_Coursework4.sh.o405684   |\n",
    "\n",
    "### FILE for Steps 4:\n",
    "|       File     |           Ouput File         |\n",
    "|----------------|------------------------------|\n",
    "|exercise2_.scala|acp16al_Coursework4.sh.o407274|\n",
    "\n",
    "#### STEP 2: ALS with default settings on the five splits (**mean** and **variance** of the MSE from five-fold corss-validation) \n",
    "\n",
    ">Root-mean-square error 1 = 0.9375114980813093\n",
    "\n",
    ">Root-mean-square error 2 = 0.7405533570017504\n",
    "\n",
    ">Root-mean-square error 3 = 0.6444566219233624\n",
    "\n",
    ">Root-mean-square error 4 = 0.7680171368711345\n",
    "\n",
    ">Root-mean-square error 5 = 0.5040866562245446\n",
    "\n",
    "> **Mean** = 0.7189250540204202\n",
    "\n",
    "> **Variance** = 0.02047178683243034\n",
    "\n",
    "#### STEP 3: **nonnegative** ALS on the five splits (**mean** and **variance** of the MSE from five-fold corss-validation)\n",
    "\n",
    ">Root-mean-square error 1 = 0.7397005896807222\n",
    "\n",
    ">Root-mean-square error 2 = 0.5966543015628872\n",
    "\n",
    ">Root-mean-square error 3 = 0.6788821179197473\n",
    "\n",
    ">Root-mean-square error 4 = 0.9083173653159354\n",
    "\n",
    ">Root-mean-square error 5 = 0.5304578087192152\n",
    "\n",
    "> **Mean** = 0.6908024366397015\n",
    "\n",
    "> **Variance** = 0.01688402772152725\n",
    "\n",
    "#### STEP 4:\n",
    "\n",
    "** The value of K was chosen to be 72. According to wikipedia (https://en.wikipedia.org/wiki/List_of_genres) the number of gernes for movies is 24. Since in the movies.dat there are combinations of gernes like Comedy-Drama, the number 24 was multiplied by 3 to compensate with the combinations. Therefore, 24x3 = 72.** \n",
    "> ##### r1:\n",
    "> ###### Number of movies (size) of 3 largest clusters:\n",
    ">\n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       14 |   514 |\n",
    "|       34 |   455 |\n",
    "|       10 |   453 |\n",
    "\n",
    "> ###### Majority Labels of 3 largest clusters:\n",
    ">\n",
    "| cluster  | gerne         | label count|\n",
    "|----------|---------------|------------|\n",
    "|       14 | Drama         | 81|\n",
    "|       34 | Drama         | 131|\n",
    "|       10 |Comedy-Romance | 45|\n",
    "\n",
    "\n",
    "\n",
    "> ##### r2:\n",
    "> ###### Number of movies (size) of 3 largest clusters:\n",
    ">\n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       60 |   616 |\n",
    "|       49 |   535 |\n",
    "|       4  |   521 |\n",
    "\n",
    "> ###### Majority Labels of 3 largest clusters:\n",
    ">\n",
    "| cluster  | gerne | label count|\n",
    "|----------|-------|------------|\n",
    "|       60 | Drama | 124|\n",
    "|       49 | Drama | 97|\n",
    "|       4  | Drama | 79|\n",
    "\n",
    "\n",
    "\n",
    "> ##### r3:\n",
    "> ###### Number of movies (size) of 3 largest clusters:\n",
    ">\n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       3  |   503 |\n",
    "|       14 |   420 |\n",
    "|       48 |   404 |\n",
    "\n",
    "> ###### Majority Labels of 3 largest clusters:\n",
    ">\n",
    "| cluster  | gerne           | label count|\n",
    "|----------|-----------------|------------|\n",
    "|       3  | Drama           | 78|\n",
    "|       14 |Comedy - Romance | 34|\n",
    "|       48 | Drama           | 59|\n",
    "\n",
    "\n",
    "> ##### r4:\n",
    "> ###### Number of movies (size) of 3 largest clusters:\n",
    ">\n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       15 |   575 |\n",
    "|       10 |   543 |\n",
    "|       67 |   438 |\n",
    "\n",
    "> ###### Majority Labels of 3 largest clusters:\n",
    ">\n",
    "| cluster  | gerne | label count|\n",
    "|----------|-------|------------|\n",
    "|       15 | Drama | 104|\n",
    "|       10 | Drama | 114|\n",
    "|       67 | Drama | 96|\n",
    "\n",
    "\n",
    "> ##### r5:\n",
    "> ###### Number of movies (size) of 3 largest clusters:\n",
    ">\n",
    "| cluster  |  size |\n",
    "|----------|-------|\n",
    "|       14 |   514 |\n",
    "|       34 |   455 |\n",
    "|       10 |   453 |\n",
    "\n",
    "> ###### Majority Labels of 3 largest clusters:\n",
    ">\n",
    "| cluster  | gerne         | label count|\n",
    "|----------|---------------|------------|\n",
    "|       14 | Drama         | 81|\n",
    "|       34 | Drama         | 131|\n",
    "|       10 |Comedy-Romance | 45|\n",
    "\n",
    "\n",
    "#### STEP 5: Interesting Things\n",
    "> The execution time for default settings ALS and **nonnegative** ALS is almost the same (9.973E-5 seconds and 7.7645E-5 seconds).\n",
    "\n",
    "> Changing the number of K to any value from 30-100 does not affect results very much. \n",
    "\n",
    "> Most common gerne for all is the Drama. \n",
    "\n",
    "> Execution time for step 4 is 302.497521091 seconds. \n",
    "\n",
    "> Default settings ALS and **nonnegative** ALS provide almost the same mean and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.0.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Sizes of 3 largest clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|        24| 7208|\n",
      "|       113| 5832|\n",
      "|       162| 5313|\n",
      "+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Centroid for 1st largest cluster[0.25069367369589346,0.7523584905660378,0.8765260821309656,0.0,1.3251942286348501,0.18645948945615984,0.0,0.0,0.0,0.0,0.0,0.0012486126526082132,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.099889012208657,4.597669256381798,0.0015496670366259712,0.0010377358490566038,0.8683435072142063,0.8454300776914535,1.0,0.0,0.31862097669256423,15.487930077691454,246.93521087680355,0.9998099334073253,1.512208657047725E-4,0.39437014428413447,0.19680632630410752,0.0015274694783573807,1.1237513873473925E-4,0.8446698113207537,0.8182866259711263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Centroid for 2nd largest cluster[1.9999999999999998,3.9999999999999996,0.0,0.0,1032.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,510.99999999999994,510.99999999999994,0.0,0.0,0.0,0.0,0.9999999999999999,0.0,0.0,254.99999999999997,249.44701646090533,0.9782235939643347,0.001687242798353902,0.9782235939643347,0.0,0.0013477366255143963,0.0,0.0,0.0]\n",
      " \n",
      " Centroid for 3rd largest cluster[1.9999999999999998,3.9999999999999996,0.0,0.0,1032.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,510.99999999999994,510.99999999999994,0.0,0.0,0.0,0.0,0.9999999999999999,0.0,0.0,254.99999999999997,249.44701646090533,0.9782235939643347,0.001687242798353902,0.9782235939643347,0.0,0.0013477366255143963,0.0,0.0,0.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Sizes of 3 smallest clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|        53|    1|\n",
      "|        65|    1|\n",
      "|       137|    1|\n",
      "+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Centroid for 1st smallest cluster[1.0,3.0,0.0,35942.0,81.0,44.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,53.0,53.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Centroid for 2nd smallest cluster[0.0,10.0,0.0,19.0,291.0,619694.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,126.0,19.0,0.14,0.03,0.01,0.11,0.0,0.0,0.0,0.0]\n",
      " \n",
      " Centroid for 3rd smallest cluster[0.0,10.0,0.0,25.0,289.0,1421071.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,255.0,64.0,0.25,0.02,0.0,0.0,0.0,0.0,0.0,0.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " SSE 3.7369216226237655E9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 1st largest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   _c41|count|\n",
      "+-------+-----+\n",
      "|normal.| 6321|\n",
      "+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 2nd largest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  _c41|count|\n",
      "+------+-----+\n",
      "|smurf.| 5832|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 3rd largest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   _c41|count|\n",
      "+-------+-----+\n",
      "|normal.| 4086|\n",
      "+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 1st smallest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   _c41|count|\n",
      "+-------+-----+\n",
      "|normal.|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 2nd smallest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   _c41|count|\n",
      "+-------+-----+\n",
      "|normal.|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Label for 3rd smallest cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   _c41|count|\n",
      "+-------+-----+\n",
      "|normal.|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 248.548729153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SparkSession\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.Row\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.DataFrame\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StringIndexer\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.clustering.{KMeans,KMeansModel, KMeansSummary}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.DoubleType\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.VectorAssembler\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mExercise_1\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 1:\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.clustering.{KMeans,KMeansModel, KMeansSummary}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "object Exercise_1 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E1\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        \n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime \n",
    "        \n",
    "        ///*\n",
    "        val df = sparkSession.read.format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\", \"false\") //reading the headers\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .load(\"files/kddcup_.data.gz\").toDF\n",
    "            \n",
    "        //PREPROCESSING:\n",
    "        val DtoDouble = udf[Double, Double]( _.toDouble)\n",
    "              \n",
    "        val dfnew = df\n",
    "        .withColumn(\"_c0\", DtoDouble(df(\"_c0\")))\n",
    "        .withColumn(\"_c4\", DtoDouble(df(\"_c4\")))\n",
    "        .withColumn(\"_c5\", DtoDouble(df(\"_c5\")))\n",
    "        .withColumn(\"_C6\", DtoDouble(df(\"_c6\")))\n",
    "        .withColumn(\"_c7\", DtoDouble(df(\"_c7\")))\n",
    "        .withColumn(\"_c8\", DtoDouble(df(\"_c8\")))\n",
    "        .withColumn(\"_c9\", DtoDouble(df(\"_c9\")))\n",
    "        .withColumn(\"_c10\", DtoDouble(df(\"_c10\")))\n",
    "        .withColumn(\"_c11\", DtoDouble(df(\"_c11\")))\n",
    "        .withColumn(\"_c12\", DtoDouble(df(\"_c12\")))\n",
    "        .withColumn(\"_c13\", DtoDouble(df(\"_c13\")))\n",
    "        .withColumn(\"_c14\", DtoDouble(df(\"_c14\")))\n",
    "        .withColumn(\"_c15\", DtoDouble(df(\"_c15\")))\n",
    "        .withColumn(\"_c16\", DtoDouble(df(\"_c16\")))\n",
    "        .withColumn(\"_c17\", DtoDouble(df(\"_c17\")))\n",
    "        .withColumn(\"_c18\", DtoDouble(df(\"_c18\")))\n",
    "        .withColumn(\"_c19\", DtoDouble(df(\"_c19\")))\n",
    "        .withColumn(\"_c20\", DtoDouble(df(\"_c20\")))\n",
    "        .withColumn(\"_c21\", DtoDouble(df(\"_c21\")))\n",
    "        .withColumn(\"_c22\", DtoDouble(df(\"_c22\")))\n",
    "        .withColumn(\"_c23\", DtoDouble(df(\"_c23\")))\n",
    "        .withColumn(\"_c24\", DtoDouble(df(\"_c24\")))\n",
    "        .withColumn(\"_c25\", DtoDouble(df(\"_c25\")))\n",
    "        .withColumn(\"_c26\", DtoDouble(df(\"_c26\")))\n",
    "        .withColumn(\"_c27\", DtoDouble(df(\"_c27\")))\n",
    "        .withColumn(\"_c28\", DtoDouble(df(\"_c28\")))\n",
    "        .withColumn(\"_c29\", DtoDouble(df(\"_c29\")))\n",
    "        .withColumn(\"_c30\", DtoDouble(df(\"_c30\")))\n",
    "        .withColumn(\"_c31\", DtoDouble(df(\"_c31\")))\n",
    "        .withColumn(\"_c32\", DtoDouble(df(\"_c32\")))\n",
    "        .withColumn(\"_c33\", DtoDouble(df(\"_c33\")))\n",
    "        .withColumn(\"_c34\", DtoDouble(df(\"_c34\")))\n",
    "        .withColumn(\"_c35\", DtoDouble(df(\"_c35\")))\n",
    "        .withColumn(\"_c36\", DtoDouble(df(\"_c36\")))\n",
    "        .withColumn(\"_c37\", DtoDouble(df(\"_c37\")))\n",
    "        .withColumn(\"_c38\", DtoDouble(df(\"_c38\")))\n",
    "        .withColumn(\"_c39\", DtoDouble(df(\"_c39\")))\n",
    "        .withColumn(\"_c40\", DtoDouble(df(\"_c40\")))\n",
    "        .select(\"_c0\", \"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\", \"_C6\", \"_c7\",\"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\", \"_c20\",                     \"_c21\", \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\",                         \"_c40\", \"_c41\")\n",
    "                       \n",
    "        //VectorAssembler for non-strings\n",
    "        val assembler_Doubles = new VectorAssembler()\n",
    "          .setInputCols(Array(\"_c0\", \"_c4\", \"_c5\", \"_C6\", \"_c7\",\"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\", \"_c20\", \"_c21\",                                 \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \"_c35\", \"_c36\", \"_c37\", \"_c38\",                                        \"_c39\", \"_c40\"))\n",
    "          .setOutputCol(\"features_D\")\n",
    "        \n",
    "        val dfnewD = assembler_Doubles.transform(dfnew)\n",
    "\n",
    "        //Use StringIndexer for all string columns:\n",
    "        val indexer1 = new StringIndexer()\n",
    "          .setInputCol(\"_c1\")\n",
    "          .setOutputCol(\"_c1_\")\n",
    "          .fit(dfnewD)\n",
    "          .transform(dfnewD)\n",
    "\n",
    "        val indexer2 = new StringIndexer()\n",
    "          .setInputCol(\"_c2\")\n",
    "          .setOutputCol(\"_c2_\")\n",
    "          .fit(indexer1)\n",
    "          .transform(indexer1)\n",
    "\n",
    "        val indexer3 = new StringIndexer()\n",
    "          .setInputCol(\"_c3\")\n",
    "          .setOutputCol(\"_c3_\")\n",
    "          .fit(indexer2)\n",
    "          .transform(indexer2)\n",
    "         \n",
    "        //VectorAssembler for all strings.\n",
    "        val assembler_Strings = new VectorAssembler()\n",
    "          .setInputCols(Array(\"_c1_\", \"_c2_\", \"_c3_\"))\n",
    "          .setOutputCol(\"features_S\")\n",
    "        val dfnewS = assembler_Strings.transform(indexer3)\n",
    "    \n",
    "        //Final VectorAssembler to combine non-strings and strings features:\n",
    "        val assembler = new VectorAssembler()\n",
    "            .setInputCols(Array(\"features_S\", \"features_D\"))\n",
    "            .setOutputCol(\"features\")\n",
    "        val dfNew = assembler.transform(dfnewS).cache()\n",
    "        \n",
    "         //Dataframe for kmeans:\n",
    "        val Array(dataset) = dfNew.select(\"features\", \"_c41\").randomSplit(Array(1.0))\n",
    "\n",
    "        //Step1:\n",
    "        val kmeans = new KMeans().setK(500).setSeed(1L).setPredictionCol(\"prediction\")\n",
    "        val model = kmeans.fit(dataset)\n",
    "        val cluster_centers = model.clusterCenters\n",
    "        val predictions = model.transform(dataset)\n",
    "        \n",
    "        //Step2:\n",
    "        val count_largest = predictions.groupBy(\"prediction\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n Sizes of 3 largest clusters\")\n",
    "        count_largest.show(3)\n",
    "\n",
    "        val largest = count_largest.select(\"prediction\").take(3)\n",
    "        val largest_1_toInt = largest(0).getInt(0)\n",
    "        val largest_2_toInt = largest(1).getInt(0)\n",
    "        val largest_3_toInt = largest(2).getInt(0)\n",
    "\n",
    "        print(\"\\n \\n Centroid for 1st largest cluster\" + cluster_centers(largest_1_toInt))\n",
    "        print(\"\\n \\n Centroid for 2nd largest cluster\" + cluster_centers(largest_2_toInt))\n",
    "        print(\"\\n \\n Centroid for 3rd largest cluster\" + cluster_centers(largest_2_toInt))\n",
    "        \n",
    "        //Step3:\n",
    "        val count_smallest = predictions.groupBy(\"prediction\").count().orderBy((\"count\"))\n",
    "        println(\"\\n \\n Sizes of 3 smallest clusters\")\n",
    "        count_smallest.show(3)\n",
    "\n",
    "        val smallest = count_smallest.select(\"prediction\").take(3)\n",
    "        val smallest_1_toInt = smallest(0).getInt(0)\n",
    "        val smallest_2_toInt = smallest(1).getInt(0)\n",
    "        val smallest_3_toInt = smallest(2).getInt(0)\n",
    "\n",
    "        print(\"\\n \\n Centroid for 1st smallest cluster\" + cluster_centers(smallest_1_toInt))\n",
    "        print(\"\\n \\n Centroid for 2nd smallest cluster\" + cluster_centers(smallest_2_toInt))\n",
    "        print(\"\\n \\n Centroid for 3rd smallest cluster\" + cluster_centers(smallest_3_toInt))\n",
    "\n",
    "        //Step4:\n",
    "        val SSE = model.computeCost(predictions)\n",
    "        println(\"\\n\\n SSE \" + SSE)\n",
    "        \n",
    "        //Step5:\n",
    "        val label_largest_1 = predictions.filter(predictions(\"prediction\") === largest_1_toInt).groupBy(\"_c41\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n Label for 1st largest cluster\")\n",
    "        label_largest_1.show(1)\n",
    "        val label_largest_2 = predictions.filter(predictions(\"prediction\") === largest_2_toInt).groupBy(\"_c41\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n Label for 2nd largest cluster\")\n",
    "        label_largest_2.show(1)\n",
    "        val label_largest_3 = predictions.filter(predictions(\"prediction\") === largest_3_toInt).groupBy(\"_c41\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n Label for 3rd largest cluster\")\n",
    "        label_largest_3.show(1)\n",
    "        \n",
    "        val label_smallest_1 = predictions.filter(predictions(\"prediction\") === smallest_1_toInt).groupBy(\"_c41\").count().orderBy((\"count\"))\n",
    "        println(\"\\n Label for 1st smallest cluster\")\n",
    "        label_smallest_1.show(1)\n",
    "        val label_smallest_2 = predictions.filter(predictions(\"prediction\") === smallest_2_toInt).groupBy(\"_c41\").count().orderBy((\"count\"))\n",
    "        println(\"\\n Label for 2nd smallest cluster\")\n",
    "        label_smallest_2.show(1)\n",
    "        val label_smallest_3 = predictions.filter(predictions(\"prediction\") === smallest_3_toInt).groupBy(\"_c41\").count().orderBy((\"count\"))\n",
    "        println(\"\\n Label for 3rd smallest cluster\")\n",
    "        label_smallest_3.show(1)\n",
    "        //*/\n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "\n",
    "}\n",
    "}\n",
    "Exercise_1.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "// Exercise 2:\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "object Exercise_2 {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E2\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "        \n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime \n",
    "        \n",
    "        val r1_train = sparkSession.read.textFile(\"files/ml-10M100K/r1.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r1_test = sparkSession.read.textFile(\"files/ml-10M100K/r1.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r2_train = sparkSession.read.textFile(\"files/ml-10M100K/r2.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r2_test = sparkSession.read.textFile(\"files/ml-10M100K/r2.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r3_train = sparkSession.read.textFile(\"files/ml-10M100K/r3.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r3_test = sparkSession.read.textFile(\"files/ml-10M100K/r3.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r4_train = sparkSession.read.textFile(\"files/ml-10M100K/r4.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r4_test = sparkSession.read.textFile(\"files/ml-10M100K/r4.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r5_train = sparkSession.read.textFile(\"files/ml-10M100K/r5.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r5_test = sparkSession.read.textFile(\"files/ml-10M100K/r5.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        // Build the recommendation model using ALS on the training data\n",
    "        val als = new ALS()\n",
    "          .setMaxIter(5)\n",
    "          .setRegParam(0.01)\n",
    "          .setUserCol(\"userId\")\n",
    "          .setItemCol(\"movieId\")\n",
    "          .setRatingCol(\"rating\")\n",
    "          //.setNonnegative(true)\n",
    "\n",
    "\n",
    "        val model_r1 = als.fit(r1_train)\n",
    "        val model_r2 = als.fit(r2_train)\n",
    "        val model_r3 = als.fit(r3_train)\n",
    "        val model_r4 = als.fit(r4_train)\n",
    "        val model_r5 = als.fit(r5_train)\n",
    "\n",
    "        // Evaluate the model by computing the RMSE on the test data\n",
    "        \n",
    "        val predictions_1 = model_r1.transform(r1_test)\n",
    "        val predictions_r1 = predictions_1.filter(!isnan($\"prediction\"))\n",
    "\n",
    "        val predictions_2 = model_r2.transform(r2_test)\n",
    "        val predictions_r2 = predictions_2.filter(!isnan($\"prediction\"))\n",
    "\n",
    "        val predictions_3 = model_r3.transform(r3_test)\n",
    "        val predictions_r3 = predictions_3.filter(!isnan($\"prediction\"))\n",
    "        \n",
    "        val predictions_4 = model_r4.transform(r4_test)\n",
    "        val predictions_r4 = predictions_4.filter(!isnan($\"prediction\"))\n",
    "\n",
    "        val predictions_5 = model_r5.transform(r5_test)\n",
    "        val predictions_r5 = predictions_5.filter(!isnan($\"prediction\"))\n",
    "       \n",
    "        \n",
    "      \n",
    "\n",
    "        val evaluator = new RegressionEvaluator()\n",
    "          .setMetricName(\"rmse\")\n",
    "          .setLabelCol(\"rating\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "\n",
    "\n",
    "        val rmse_r1 = evaluator.evaluate(predictions_r1)\n",
    "        println(s\"Root-mean-square error 1 = $rmse_r1\")\n",
    "        \n",
    "        val rmse_r2 = evaluator.evaluate(predictions_r2)\n",
    "        println(s\"Root-mean-square error 2 = $rmse_r2\")\n",
    "\n",
    "        val rmse_r3 = evaluator.evaluate(predictions_r3)\n",
    "        println(s\"Root-mean-square error 3 = $rmse_r3\")\n",
    "\n",
    "        val rmse_r4 = evaluator.evaluate(predictions_r4)\n",
    "        println(s\"Root-mean-square error 4 = $rmse_r4\")\n",
    "\n",
    "        val rmse_r5 = evaluator.evaluate(predictions_r5)\n",
    "        println(s\"Root-mean-square error 5 = $rmse_r5\")\n",
    "\n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime \n",
    "        \n",
    "     \n",
    "        //Mean:\n",
    "        val mean = (rmse_r1 + rmse_r2 + rmse_r3 + rmse_r4 + rmse_r5) / 5\n",
    "        println(s\"Mean = $mean\")\n",
    "\n",
    "        //Variance:\n",
    "        val variance = ((rmse_r1*rmse_r1 + rmse_r2*rmse_r2 + rmse_r3*rmse_r3 + rmse_r4*rmse_r4 + rmse_r5*rmse_r5) / 5) - (mean*mean)\n",
    "        println(s\"Variance = $variance\")\n",
    "\n",
    "\n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "\n",
    "}\n",
    "\n",
    "        case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n",
    "\n",
    "        def parseRating(str: String): Rating = {\n",
    "          val fields = str.split(\"::\")\n",
    "          assert(fields.size == 4)\n",
    "          Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n",
    "        }\n",
    "\n",
    "}\n",
    "Exercise_2.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "// Exercise 2_:\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.clustering.{KMeans,KMeansModel, KMeansSummary}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg._\n",
    "import  scala.collection.mutable.WrappedArray\n",
    "\n",
    "object Exercise_2_ {\n",
    "    \n",
    "    def main(args: Array[String]) {\n",
    "        \n",
    "        val sparkSession = SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"E2\")\n",
    "            .getOrCreate()\n",
    "        \n",
    "        val sc = sparkSession.sparkContext\n",
    "        \n",
    "        import sparkSession.implicits._\n",
    "\n",
    "        //Measure execution time:\n",
    "        val timing = System.nanoTime \n",
    "\n",
    "        //Read r files:\n",
    "        val r1_train = sparkSession.read.textFile(\"files/ml-10M100K/r1.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r1_test = sparkSession.read.textFile(\"files/ml-10M100K/r1.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r2_train = sparkSession.read.textFile(\"files/ml-10M100K/r2.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r2_test = sparkSession.read.textFile(\"files/ml-10M100K/r2.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r3_train = sparkSession.read.textFile(\"files/ml-10M100K/r3.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r3_test = sparkSession.read.textFile(\"files/ml-10M100K/r3.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r4_train = sparkSession.read.textFile(\"files/ml-10M100K/r4.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r4_test = sparkSession.read.textFile(\"files/ml-10M100K/r4.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        val r5_train = sparkSession.read.textFile(\"files/ml-10M100K/r5.train\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "        val r5_test = sparkSession.read.textFile(\"files/ml-10M100K/r5.test\")\n",
    "          .map(parseRating)\n",
    "          .toDF()\n",
    "\n",
    "        //Read movies.dat:\n",
    "        val moviesdat = sparkSession.read.textFile(\"files/ml-10M100K/movies.dat\")\n",
    "          .map(parseMovies)\n",
    "          .toDF()\n",
    "\n",
    "        // Build the recommendation model using ALS on the training data\n",
    "        val als = new ALS()\n",
    "          .setMaxIter(5)\n",
    "          .setRegParam(0.01)\n",
    "          .setUserCol(\"userId\")\n",
    "          .setItemCol(\"movieId\")\n",
    "          .setRatingCol(\"rating\")\n",
    "\n",
    "        //Models:\n",
    "        val model_r1 = als.fit(r1_train)\n",
    "        val model_r2 = als.fit(r2_train)\n",
    "        val model_r3 = als.fit(r3_train)\n",
    "        val model_r4 = als.fit(r4_train)\n",
    "        val model_r5 = als.fit(r5_train)\n",
    "\n",
    "        //Itemfactors:\n",
    "        val itemfactors1 = model_r1.itemFactors\n",
    "        val itemfactors2 = model_r2.itemFactors\n",
    "        val itemfactors3 = model_r3.itemFactors\n",
    "        val itemfactors4 = model_r4.itemFactors\n",
    "        val itemfactors5 = model_r5.itemFactors\n",
    "       \n",
    "\n",
    "        //Convert itemfactors to appropriate format for kmeans:\n",
    "        itemfactors1.printSchema()\n",
    "        val convertudf = udf ((array: WrappedArray[Float]) => {\n",
    "          Vectors.dense(array.map(_.toDouble).array)\n",
    "          })\n",
    "\n",
    "        val df1 = itemfactors1.withColumn(\"features\", convertudf('features))\n",
    "        val df2 = itemfactors2.withColumn(\"features\", convertudf('features))\n",
    "        val df3 = itemfactors3.withColumn(\"features\", convertudf('features))\n",
    "        val df4 = itemfactors4.withColumn(\"features\", convertudf('features))\n",
    "        val df5 = itemfactors1.withColumn(\"features\", convertudf('features))\n",
    "\n",
    "\n",
    "        //Kmeans:\n",
    "              //r1:\n",
    "        val kmeans1 = new KMeans().setK(72).setSeed(1L).setPredictionCol(\"prediction1\")\n",
    "        val model1 = kmeans1.fit(df1)\n",
    "        val cluster_centers1 = model1.clusterCenters\n",
    "        val predictions1 = model1.transform(df1)\n",
    "              //r2:\n",
    "        val kmeans2 = new KMeans().setK(72).setSeed(1L).setPredictionCol(\"prediction2\")\n",
    "        val model2 = kmeans2.fit(df2)\n",
    "        val cluster_centers2 = model2.clusterCenters\n",
    "        val predictions2 = model2.transform(df2)\n",
    "              //r3:\n",
    "        val kmeans3 = new KMeans().setK(72).setSeed(1L).setPredictionCol(\"prediction3\")\n",
    "        val model3 = kmeans3.fit(df3)\n",
    "        val cluster_centers3 = model3.clusterCenters\n",
    "        val predictions3 = model3.transform(df3)\n",
    "              //r4:\n",
    "        val kmeans4 = new KMeans().setK(72).setSeed(1L).setPredictionCol(\"prediction4\")\n",
    "        val model4 = kmeans4.fit(df4)\n",
    "        val cluster_centers4 = model4.clusterCenters\n",
    "        val predictions4 = model4.transform(df4)\n",
    "              //r5:\n",
    "        val kmeans5 = new KMeans().setK(72).setSeed(1L).setPredictionCol(\"prediction5\")\n",
    "        val model5 = kmeans5.fit(df5)\n",
    "        val cluster_centers5 = model5.clusterCenters\n",
    "        val predictions5 = model5.transform(df5)\n",
    "        \n",
    "       \n",
    "\n",
    "        //Majority Labels:\n",
    "              // r1:\n",
    "        val count_largest1 = predictions1.groupBy(\"prediction1\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n r1) Sizes of 3 largest clusters\")\n",
    "        count_largest1.show(3)\n",
    "        val largest1 = count_largest1.select(\"prediction1\").take(3)\n",
    "        val largest_1_toInt1 = largest1(0).getInt(0)\n",
    "        val largest_2_toInt1 = largest1(1).getInt(0)\n",
    "        val largest_3_toInt1 = largest1(2).getInt(0)\n",
    "\n",
    "\n",
    "        val label_largest_1_1 = predictions1.filter(predictions1(\"prediction1\") === largest_1_toInt1)\n",
    "        val r1_labels_1_joined = moviesdat.join(label_largest_1_1, moviesdat(\"movieId\") === label_largest_1_1(\"id\"))\n",
    "        val r1_labels_1 = r1_labels_1_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r1) Label for 1st largest cluster\")\n",
    "        r1_labels_1.show(1)\n",
    "       \n",
    "        val label_largest_2_1 = predictions1.filter(predictions1(\"prediction1\") === largest_2_toInt1)\n",
    "        val r1_labels_2_joined = moviesdat.join(label_largest_2_1, moviesdat(\"movieId\") === label_largest_2_1(\"id\"))\n",
    "        val r1_labels_2 = r1_labels_2_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r1) Label for 2nd largest cluster\")\n",
    "        r1_labels_2.show(1)\n",
    "\n",
    "        val label_largest_3_1 = predictions1.filter(predictions1(\"prediction1\") === largest_3_toInt1)\n",
    "        val r1_labels_3_joined = moviesdat.join(label_largest_3_1, moviesdat(\"movieId\") === label_largest_3_1(\"id\"))\n",
    "        val r1_labels_3 = r1_labels_3_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r1) Label for 3rd largest cluster\")\n",
    "        r1_labels_3.show(1)\n",
    "\n",
    "\n",
    "              // r2:\n",
    "        val count_largest2 = predictions2.groupBy(\"prediction2\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n r2) Sizes of 3 largest clusters\")\n",
    "        count_largest2.show(3)\n",
    "        val largest2 = count_largest2.select(\"prediction2\").take(3)\n",
    "        val largest_1_toInt2 = largest2(0).getInt(0)\n",
    "        val largest_2_toInt2 = largest2(1).getInt(0)\n",
    "        val largest_3_toInt2 = largest2(2).getInt(0)\n",
    "\n",
    "\n",
    "        val label_largest_1_2 = predictions2.filter(predictions2(\"prediction2\") === largest_1_toInt2)\n",
    "        val r2_labels_1_joined = moviesdat.join(label_largest_1_2, moviesdat(\"movieId\") === label_largest_1_2(\"id\"))\n",
    "        val r2_labels_1 = r2_labels_1_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r2) Label for 1st largest cluster\")\n",
    "        r2_labels_1.show(1)\n",
    "       \n",
    "        val label_largest_2_2 = predictions2.filter(predictions2(\"prediction2\") === largest_2_toInt2)\n",
    "        val r2_labels_2_joined = moviesdat.join(label_largest_2_2, moviesdat(\"movieId\") === label_largest_2_2(\"id\"))\n",
    "        val r2_labels_2 = r2_labels_2_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r2) Label for 2nd largest cluster\")\n",
    "        r2_labels_2.show(1)\n",
    "\n",
    "        val label_largest_3_2 = predictions2.filter(predictions2(\"prediction2\") === largest_3_toInt2)\n",
    "        val r2_labels_3_joined = moviesdat.join(label_largest_3_2, moviesdat(\"movieId\") === label_largest_3_2(\"id\"))\n",
    "        val r2_labels_3 = r2_labels_3_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r2) Label for 3rd largest cluster\")\n",
    "        r2_labels_3.show(1)\n",
    "\n",
    "\n",
    "              // r3:\n",
    "        val count_largest3 = predictions3.groupBy(\"prediction3\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n r3) Sizes of 3 largest clusters\")\n",
    "        count_largest3.show(3)\n",
    "        val largest3 = count_largest3.select(\"prediction3\").take(3)\n",
    "        val largest_1_toInt3 = largest3(0).getInt(0)\n",
    "        val largest_2_toInt3 = largest3(1).getInt(0)\n",
    "        val largest_3_toInt3 = largest3(2).getInt(0)\n",
    "\n",
    "\n",
    "        val label_largest_1_3 = predictions3.filter(predictions3(\"prediction3\") === largest_1_toInt3)\n",
    "        val r3_labels_1_joined = moviesdat.join(label_largest_1_3, moviesdat(\"movieId\") === label_largest_1_3(\"id\"))\n",
    "        val r3_labels_1 = r3_labels_1_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r3) Label for 1st largest cluster\")\n",
    "        r3_labels_1.show(1)\n",
    "       \n",
    "        val label_largest_2_3 = predictions3.filter(predictions3(\"prediction3\") === largest_2_toInt3)\n",
    "        val r3_labels_2_joined = moviesdat.join(label_largest_2_3, moviesdat(\"movieId\") === label_largest_2_3(\"id\"))\n",
    "        val r3_labels_2 = r3_labels_2_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r3) Label for 2nd largest cluster\")\n",
    "        r3_labels_2.show(1)\n",
    "\n",
    "        val label_largest_3_3 = predictions3.filter(predictions3(\"prediction3\") === largest_3_toInt3)\n",
    "        val r3_labels_3_joined = moviesdat.join(label_largest_3_3, moviesdat(\"movieId\") === label_largest_3_3(\"id\"))\n",
    "        val r3_labels_3 = r3_labels_3_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r3) Label for 3rd largest cluster\")\n",
    "        r3_labels_3.show(1)\n",
    "\n",
    "\n",
    "              // r4:\n",
    "        val count_largest4 = predictions4.groupBy(\"prediction4\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n r4) Sizes of 3 largest clusters\")\n",
    "        count_largest4.show(3)\n",
    "        val largest4 = count_largest4.select(\"prediction4\").take(3)\n",
    "        val largest_1_toInt4 = largest4(0).getInt(0)\n",
    "        val largest_2_toInt4 = largest4(1).getInt(0)\n",
    "        val largest_3_toInt4 = largest4(2).getInt(0)\n",
    "\n",
    "\n",
    "        val label_largest_1_4 = predictions4.filter(predictions4(\"prediction4\") === largest_1_toInt4)\n",
    "        val r4_labels_1_joined = moviesdat.join(label_largest_1_4, moviesdat(\"movieId\") === label_largest_1_4(\"id\"))\n",
    "        val r4_labels_1 = r4_labels_1_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r4) Label for 1st largest cluster\")\n",
    "        r4_labels_1.show(1)\n",
    "       \n",
    "        val label_largest_2_4 = predictions4.filter(predictions4(\"prediction4\") === largest_2_toInt4)\n",
    "        val r4_labels_2_joined = moviesdat.join(label_largest_2_4, moviesdat(\"movieId\") === label_largest_2_4(\"id\"))\n",
    "        val r4_labels_2 = r4_labels_2_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r4) Label for 2nd largest cluster\")\n",
    "        r4_labels_2.show(1)\n",
    "\n",
    "        val label_largest_3_4 = predictions4.filter(predictions4(\"prediction4\") === largest_3_toInt4)\n",
    "        val r4_labels_3_joined = moviesdat.join(label_largest_3_4, moviesdat(\"movieId\") === label_largest_3_4(\"id\"))\n",
    "        val r4_labels_3 = r4_labels_3_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r4) Label for 3rd largest cluster\")\n",
    "        r4_labels_3.show(1)\n",
    "\n",
    "              // r5:\n",
    "        val count_largest5 = predictions5.groupBy(\"prediction5\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n \\n r5) Sizes of 3 largest clusters\")\n",
    "        count_largest5.show(3)\n",
    "        val largest5 = count_largest5.select(\"prediction5\").take(3)\n",
    "        val largest_1_toInt5 = largest5(0).getInt(0)\n",
    "        val largest_2_toInt5 = largest5(1).getInt(0)\n",
    "        val largest_3_toInt5 = largest5(2).getInt(0)\n",
    "\n",
    "\n",
    "        val label_largest_1_5 = predictions5.filter(predictions5(\"prediction5\") === largest_1_toInt5)\n",
    "        val r5_labels_1_joined = moviesdat.join(label_largest_1_5, moviesdat(\"movieId\") === label_largest_1_5(\"id\"))\n",
    "        val r5_labels_1 = r5_labels_1_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r5) Label for 1st largest cluster\")\n",
    "        r5_labels_1.show(1)\n",
    "       \n",
    "        val label_largest_2_5 = predictions5.filter(predictions5(\"prediction5\") === largest_2_toInt5)\n",
    "        val r5_labels_2_joined = moviesdat.join(label_largest_2_5, moviesdat(\"movieId\") === label_largest_2_5(\"id\"))\n",
    "        val r5_labels_2 = r5_labels_2_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r5) Label for 2nd largest cluster\")\n",
    "        r5_labels_2.show(1)\n",
    "\n",
    "        val label_largest_3_5 = predictions5.filter(predictions5(\"prediction5\") === largest_3_toInt5)\n",
    "        val r5_labels_3_joined = moviesdat.join(label_largest_3_5, moviesdat(\"movieId\") === label_largest_3_5(\"id\"))\n",
    "        val r5_labels_3 = r5_labels_3_joined.groupBy(\"gerne\").count().orderBy(desc(\"count\"))\n",
    "        println(\"\\n r5) Label for 3rd largest cluster\")\n",
    "        r5_labels_3.show(1)\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        println(\"Time: \" + (System.nanoTime-timing) / 1e9d)\n",
    "\n",
    "}\n",
    "        case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n",
    "        def parseRating(str: String): Rating = {\n",
    "          val fields = str.split(\"::\")\n",
    "          assert(fields.size == 4)\n",
    "          Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n",
    "        }\n",
    "        \n",
    "        case class Movies(movieId: Int, title: String, gerne: String)\n",
    "        def parseMovies(str: String): Movies = {\n",
    "          val fields = str.split(\"::\")\n",
    "          assert(fields.size == 3)\n",
    "          Movies(fields(0).toInt, fields(1).toString, fields(2).toString)\n",
    "        }\n",
    "\n",
    "}\n",
    "Exercise_2_.main(Array())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
  },
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}